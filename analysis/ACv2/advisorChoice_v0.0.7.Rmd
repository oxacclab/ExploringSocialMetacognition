---
title: "Advisor choice (dates)"
author: "[Matt Jaquiery](https://github.com/mjaquiery) ([matt.jaquiery@psy.ox.ac.uk](mailto:matt.jaquiery@psy.ox.ac.uk))"
output:
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '3'
    css: ../src/writeUp.css
    includes:
      after_body: ../src/toc_menu.html
  html_notebook:
    toc: yes
    toc_depth: 3
    css: ../src/writeUp.css
    includes:
      after_body: ../src/toc_menu.html
editor_options:
  chunk_output_type: inline
---

May 2020

[Script run `r Sys.time()`]


```{r prematter, include = F}

library(tidyverse)
library(scales)
library(broom)
library(BayesFactor)
library(prettyMD)
library(knitr)
library(parallel)
library(ez)

opts_chunk$set('echo' = F)

set.seed(20200504)

# Plot setup
theme_set(theme_light() + 
            theme(panel.grid = element_blank(),
                  legend.position = 'top'))

```

```{r loadData, include = F}

# rDir <- "http://localhost/ExploringSocialMetacognition/data/public/"
testData <- F

studyVersion <- "0-0-7"
studyName <- "advisorChoice"

overrideMarkerList <- c(7, 13, 21)

exclude <- list(
  maxTrialRT = 60000,    # trials take < 1 minute
  minTrials = 11,        # at least 11 trials completed
  minChangeRate = .1,    # some advice taken on 10%+ of trials
  badMarker = T,        # markers not in markerList
  custom = list(
      adviceColumnsNumeric = 
        # Each participant should have only numeric values in the advice details
        function(x) {
          x %>% 
            transmute(advisor0adviceCentre = 
                        as.numeric(as.character(advisor0adviceCentre))) %>%
            summarise_all(~any(is.na(.))) %>% 
            pull()
        },
      eightKeyTrials = 
        # Each participant should have at least eight choice trials
        function(x) {
          x %>% 
            filter(advisorOptions != "") %>%
            nrow() < 8
        }
    )
  ) 

skipLoadData <- F

source("src/02_Exclusions.R")

if (testData) {
  # Construct some models of participant behaviour on the ‘prefer worst known to
  # unknown’, ‘treat unknown as worst known’, and ‘weight advice weights
  # according to how likely advice of a given confidence was to come from each
  # advisor’
  source("src/sim-confidence-estimation.R")
  model <- simulateCK(
    AdvisedTrial,
    agentInsensitivitySD = 8,
    agentConfidence = 2,
    agentConfidenceSD = 4,
    agentEgoBias = .7, 
    agentEgoBiasSD = .2,
    agentEffectSize = .1,
    agentEffectSizeSD = .1,
    strategy = 'PreferWorst'
    )
  AdvisedTrial <- model$trials
  decisions <- byDecision(AdvisedTrial)
}

```

# Introduction

We have [previously observed](https://oxacclab.github.io/ExploringSocialMetacognition/analysis/ACv2/writeUp_v1.0.1.html) that people are more influenced by accurate over agreeing advisors when given feedback, and by agreeing over accurate advisors when denied feedback. Here we seek to extend that finding to the domain of source selection by offering participants a choice of advisor.

## Preregistration

This is a preregistered replication of a [previous study](https://oxacclab.github.io/ExploringSocialMetacognition/analysis/ACv2/advisorChoice_v0.0.6.html) which demonstrated that people would preferentially pick the Accurate advisor when provided with feedback. When feedback was withheld, the evidence suggested the rates of picking were equivalent to chance (although very noisy). The preregistration is done [via the OSF](https://osf.io/nwmx5), and, because the OSF is having some issues at the moment, via AsPredicted.org.

## Bayes stats

The Bayesian stats presented below use default priors. These are no more wildly unsuitable than frequentist t-test assumptions (also provided), but the competing models of selection can be specified more precisely and compared more directly. This analysis will be added at a later date.

# Method

Participants are trained on an estimation decision task (dates task) and familiarised with two advisors within a judge-advisor system, one of who offers advisory estimates close to the participant's initial estimate ('agreeing advisor') and one who offers advisory estimates close to the true answer ('accurate advisor'). In the test phase, participants are allowed to choose which of the two advisors will give them advice on each trial.

The experimental code is available on [GitHub](https://github.com/oxacclab/ExploringSocialMetacognition), and the experiment can be performed by visiting [https://acclab.psy.ox.ac.uk/~mj221/ESM/ACv2/ac.html](https://acclab.psy.ox.ac.uk/~mj221/ESM/ACv2/ac.html?PROLIFIC_PID=WriteUp). 

# Results

## Exclusions

```{r exclusions}

if (testData) {
  print("NOTE: Testing (simulated) data!!!!")
}

tmp <- suppressWarnings(left_join(exclusions, okayIds, by = "pid"))
tmp <- tmp %>% mutate(pid = as.character(pid))

tmp$condition <- factor(tmp$condition, labels = c("Feedback.AgreeFirst",
                                                  "Feedback.AccurateFirst",
                                                  "Nofeedback.AgreeFirst",
                                                  "Nofeedback.AccurateFirst"))

table(tmp$excluded, tmp$condition)

AdvisedTrial <- AdvisedTrial %>%
  mutate(
    condition = unlist(sapply(pid, function(p) tmp$condition[tmp$pid == p][1])),
    FeedbackCondition = if_else(str_detect(condition, "^Feedback"), 
                                "Feedback condition", "No feedback condition")
  )

```

```{r data visualisation variables}
# Add some variables which we can use for plotting prettily later

AdvisedTrial <- AdvisedTrial %>%
  mutate(Advisor = capitalize(as.character(advisor0idDescription)),
         Feedback = if_else(feedback, "Feedback", "No feedback"),
         Phase = if_else(advisorOptions == "", "Familiarization", "Test"))

decisions <- byDecision(AdvisedTrial) %>% 
  mutate(Decision = capitalize(decision))
```

Our final participant list consists of `r length(unique(AdvisedTrial$pid))` participants who completed an average of `r num2str(mean(aggregate(advisor0 ~ pid, AdvisedTrial, length)$advisor0))` trials each. There were `r AdvisedTrial %>% select(pid, FeedbackCondition) %>% unique() %>% filter(FeedbackCondition == "Feedback condition") %>% nrow()` participants in the Feedback condition, and `r AdvisedTrial %>% select(pid, FeedbackCondition) %>% unique() %>% filter(FeedbackCondition == "No feedback condition") %>% nrow()` in the No feedback condition.

## Task performance

First we offer a characterisation of the task, to provide the reader with a sense of how the participants performed. 

### Decisions

Participants offered estimates of whether various events took place before or after a given year. The correct answers were always between 1900 and 2000. Participants answered selecting a marker from a pool of available markers and dragging it onto a timeline to cover a range of dates within which they thought the event took place.

#### Correctness

Correct answers are those in which the chosen marker covers the year in which the event took place.

```{r accuracy}

decisions %>% filter(Phase == "Familiarization") %>% peek(responseCorrect, decision) %>%
  num2str.tibble(isProportion = T, precision = 3) %>%
  cbind(Phase = unique(decisions %>% filter(Phase == "Familiarization") %>% pull(Phase)))

```

```{r accuracyGraph}

tmp <- decisions %>%
  filter(Phase == "Familiarization") %>% 
  group_by(pid, Decision, FeedbackCondition, Phase) %>%
  do(correct = mean_cl_normal(.$responseCorrect)) %>%
  unnest(correct) %>%
  transmute(pid, Decision, FeedbackCondition, Phase,
            pCorrect = y, 
            ciLow = ymin,
            ciHigh = ymax)

ggplot(tmp, 
       aes(x = Decision, y = pCorrect)) +
  geom_hline(yintercept = max(overrideMarkerList) / 110, linetype = 'dashed') +
  geom_violin(alpha = .15, colour = NA, aes(fill = Decision)) +
  geom_boxplot(fill = "white", outlier.color = NA, aes(colour = Decision),
               width = .25, size = 1.25) +
  geom_line(alpha = .25, aes(group = pid)) + 
  scale_y_continuous(limits = c(0, 1)) +
  labs(y = "P(Response correct)") +
  facet_grid(~Phase + FeedbackCondition)

# caption = "Probability of correct responses on initial estimates and final decisions. Lines show individual participant means, while violins and boxplots give distributions. The dashed line shows chance performance when placing the widest marker randomly on the timeline. The panels are facetted according to the participants' condition."

```

### Error 

Error is the distance in years between the centre of the marker and the correct year. Note that when using smaller markers it is possible for answers to be incorrect (the marker doesn't cover the correct year) while still having relatively low error (the centre of the marker is close to the correct year).

```{r}

decisions %>% filter(Phase == "Familiarization") %>% peek(responseError, decision) %>%
  num2str.tibble(isProportion = T, precision = 1) %>%
  cbind(Phase = unique(decisions %>% filter(Phase == "Familiarization") %>% pull(Phase)))

tmp <- decisions %>%
  filter(Phase == "Familiarization") %>% 
  group_by(pid, Decision, FeedbackCondition, Phase) %>%
  do(error = mean_cl_normal(.$responseError)) %>%
  unnest(error) %>%
  transmute(pid, Decision, FeedbackCondition, Phase,
            error = y, 
            ciLow = ymin,
            ciHigh = ymax)

# Estimate error from random placement
guessingError <- tibble(
  x = sample(1890:(2010 - max(overrideMarkerList)) + max(overrideMarkerList/2), 10000, T), 
  y = sample(AdvisedTrial$correctAnswer, 10000, T),
  error = abs(x - y)
  ) %>% pull(error) %>% mean()


ggplot(tmp, aes(x = Decision, y = error)) +
  geom_hline(yintercept = guessingError, linetype = 'dashed') +
  geom_violin(alpha = .15, colour = NA, aes(fill = Decision)) +
  geom_boxplot(fill = "white", outlier.color = NA, aes(colour = Decision),
               width = .25, size = 1.25) +
  geom_line(alpha = .25, aes(group = pid)) + 
  scale_y_continuous(limits = c(0, 100)) +
  labs(y = "Mean error (years)") +
  facet_grid(~Phase + FeedbackCondition)

# caption = "Response error for initial estimates and final decisions. Lines show individual participant means, while violins and boxplots give distributions. The panels are facetted according to the participants' condition. Dashed line shows the (empirically estimated) average error from placing the widest marker randomly on the timeline."

```

### Timing

We can look at the response time - the difference between the time the response is opened and the time the response is received.  

```{r time}

decisions$rt <- decisions$responseTimeEstimate - decisions$timeResponseOpen

decisions %>% filter(Phase == "Familiarization") %>% peek(rt, decision) %>% 
  mutate_if(is.numeric, round) %>%
  cbind(Phase = unique(decisions %>% filter(Phase == "Familiarization") %>% pull(Phase)))

```

```{r timeGraph}

tmp <- decisions %>% 
  filter(Phase == "Familiarization") %>% 
  group_by(pid, Decision, Phase, FeedbackCondition) %>% 
  summarise(rt = mean(rt))

tmp %>%
  ggplot(aes(x = Decision, y = rt)) +
  geom_violin(alpha = .15, colour = NA, aes(fill = Decision)) +
  geom_boxplot(fill = "white", outlier.color = NA, aes(colour = Decision),
               width = .25, size = 1.25) +
  geom_line(alpha = .25, aes(group = pid)) + 
  scale_y_continuous(limits = c(0, NA)) +
  labs(y = "Response time (ms)") +
  facet_grid(~ Phase + FeedbackCondition)

# caption = "Response times for initial estimates and final decisions. Lines show individual participant means, while violins and boxplots give distributions."

```

### Summary {.summary}

Participants performed the task as expected: responses were better than chance, and almost all participants increased their response accuracy from initial estimates to final decisions. Likewise, error was lower than would be expected with random marker placement, and errros decreased from initial estimate to final decision. Participants took several seconds to make each decision, around 10s for initial estimates and 5s for final decisions. *NB:* they had seen the advice for 2s by the time the final decision window started.

## Metacognitive performance

### Marker usage

Participants could select one of three markers, each spanning a different number of years. They were awarded more points for selecting slimmer markers.

```{r marker usage}

PP <- decisions %>% 
  filter(Phase == "Familiarization") %>% 
  participantSummary()

tmp <- markerBreakdown(proportion, PP, hideMarkerTotal = T)

# Proportions within a row should sum to 1
for (x in tmp)
  expect_equal(apply(x[, 3:5], 1, sum), rep(1, nrow(x)))

num2str.tibble(tmp$first, isProportion = T, precision = 3)
num2str.tibble(tmp$last, isProportion = T, precision = 3)

```

```{r marker usage graph}

PP %>% 
  left_join(AdvisedTrial %>% select(pid, FeedbackCondition) %>% unique(), by = "pid") %>%
  filter(!is.na(responseMarker)) %>% 
  mutate(Decision = capitalize(decision)) %>%
  ggplot(aes(x = responseMarker, y = proportion)) +
  geom_hline(yintercept = 1/length(overrideMarkerList), linetype = 'dashed') +
  geom_violin(alpha = .15, aes(fill = Decision), colour = NA) +
  geom_boxplot(fill = "white", aes(colour = Decision), outlier.color = NA, width = .25, size = 1.25) +
  geom_line(alpha = .25, aes(group = pid)) + 
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  facet_grid(Decision ~ FeedbackCondition) +
  scale_linetype_manual(values = c("dashed")) + 
  labs(x = "response marker width (years)", 
       y = "p(marker used)")

# caption = "Proportion of marker selection by participant condition and decision time. Faint lines show individual participant scores, heavy dashed lines show participant means, and boxplots and violins give distributions. Light dashed line shows the probability of selecting markers at random"

```

#### Feedback condition by advisor

```{r}

decisions %>% 
  filter(FeedbackCondition == "Feedback condition",
         Phase == "Familiarization") %>% 
  nest(data = c(-advisor0idDescription, -FeedbackCondition, -Phase)) %>% 
  mutate(data = map(data, ~ participantSummary(.))) %>%
  unnest_legacy() %>%
  mutate(Decision = capitalize(decision)) %>%
  ggplot(aes(x = responseMarker, y = proportion)) +
  geom_hline(yintercept = 1/length(overrideMarkerList), linetype = 'dashed') +
  geom_violin(alpha = .15, aes(fill = Decision), colour = NA) +
  geom_boxplot(fill = "white", aes(colour = Decision), outlier.color = NA, width = .25, size = 1.25) +
  geom_line(alpha = .25, aes(group = pid)) + 
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  facet_grid(Decision ~ FeedbackCondition + advisor0idDescription) +
  scale_linetype_manual(values = c("dashed")) + 
  labs(x = "response marker width (years)", 
       y = "p(marker used)")

# caption = "Proportion of marker selection by participants in the Feedback condition by Advisor and decision time. Faint lines show individual participant scores, heavy dashed lines show participant means, and boxplots and violins give distributions. Light dashed line shows the probability of selecting markers at random"

```

### Relationship between marker width and error

Participants indicated the confidence in their decisions simultaneously with the decisions themselves through their choice of which marker to use. We can thus explore how well a participant's confidence signals their accuracy. We expect that slimmer markers will be associated with less error.

```{r}

tmp <- decisions %>% 
  filter(Phase == "Familiarization") %>%
  markerBreakdown(responseError, .)
num2str.tibble(tmp$first, precision = 1)
num2str.tibble(tmp$last, precision = 1)

```

```{r}

byChance <- tibble(
  responseMarker = sample(overrideMarkerList, 10000 * length(overrideMarkerList), T),
  x = map_dbl(responseMarker, ~ sample(1890:(2010 - .) + ./2, 1)),
  c = sample(AdvisedTrial$correctAnswer, length(responseMarker), T),
  error = abs(x - c)
) %>% group_by(responseMarker) %>% 
  summarise(chanceLevel = mean(error)) %>%
  mutate(responseMarker = factor(responseMarker))

decisions %>% 
  filter(Phase == "Familiarization") %>%
  group_by(Decision, FeedbackCondition, pid, responseMarker) %>%
  summarise(responseError = mean(responseError)) %>%
  ggplot(aes(x = responseMarker, y = responseError)) +
  geom_blank() +
  geom_segment(aes(x = as.numeric(responseMarker) - .33, 
                   xend = as.numeric(responseMarker) + .33, 
                   y = chanceLevel, yend = chanceLevel), 
               linetype = "dashed", data = byChance) +
  geom_violin(alpha = .25, colour = NA, aes(fill = Decision)) +
  geom_boxplot(fill = "white", outlier.color = NA, aes(colour = Decision), width = .25, size = 1.25) +
  geom_line(alpha = .25, aes(group = pid)) + 
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  facet_grid(Decision ~ FeedbackCondition) +
  scale_linetype_manual(values = c("dashed")) + 
  scale_y_continuous(limits = c(0, 100)) +
  labs(x = "response marker width (years)", 
       y = "|target - response marker centre| (years)")

# caption = "Error values for each marker, by participant condition and decision. Faint solid lines show individual participant data, heavy dashed lines show means for all participants, and violins and box plots show distributions. The light dashed lines show (empirically estimated) chance level error for each marker if it were placed randomly on the scale."

```

### Summary {.summary}

Participants in the Feedback condition favoured the middle-width marker at the expense of the narrowest marker for both initial and final decisions. Participants in the No feedback condition preferred the widest marker at the expense of the narrowest for initial decisions, but were roughly equally likely to use any marker width for final decisions. Participants in the Feedback condition did not seem to alter their marker choice as a function of the advisor providing them with advice. There was a good deal of interpersonal variation in marker usage in all groups, although it is probable that individual participants were reasonably consistent with their marker choices.

Participants had lower error on average for thinner markers, indicating appropriate usage of markers according to the accuracy of the participant's knowledge of the answer. 

## Advisor performance

Participants in should get an appropriate experience of each advisor during familiarisation. The accurate advisor should have low error, while the agreeing advisor should have low distance (a measure of the distance between the centre of the advice and the participant's initial estimate).

### Error

```{r adviceAccuracy}

tmp <- AdvisedTrial %>%
  group_by(pid, advisor0idDescription, FeedbackCondition) %>% 
  mutate(advisor0adviceCentre = as.numeric(as.character(advisor0adviceCentre))) %>%
  summarise(error = mean(abs(advisor0adviceCentre - correctAnswer)),
            distance = mean(abs(advisor0adviceCentre - 
                                  (responseEstimateLeft + responseMarkerWidth / 2))))

tmp %>% 
  group_by(advisor0idDescription, FeedbackCondition) %>%
  summarise(adviceError = mean(error),
            adviceDistance = mean(distance)) %>%
  num2str.tibble(isProportion = T, precision = 3)

```

```{r adviceAccuracyGraph}
guessingError <- tibble(
  x = sample(1890:(2010 - unique(AdvisedTrial$advisor0adviceWidth)) + unique(AdvisedTrial$advisor0adviceWidth/2), 10000, T), 
  y = sample(AdvisedTrial$correctAnswer, 10000, T),
  error = abs(x - y)
  ) %>% pull(error) %>% mean()

tmp %>% 
  gather("var", "value", c(error, distance)) %>%
  mutate(var = capitalize(var)) %>%
  ggplot(aes(x = advisor0idDescription, y = value)) +
  geom_hline(yintercept = guessingError, linetype = 'dashed') +
  geom_violin(alpha = .15, colour = NA, aes(fill = advisor0idDescription)) +
  geom_boxplot(fill = "white", outlier.color = NA, aes(colour = advisor0idDescription),
               width = .25, size = 1.25) +
  geom_line(alpha = .25, aes(group = pid)) + 
  labs(y = "Mean advice error", x = "Advisor") +
  scale_y_continuous(limits = c(0, 100)) + 
  scale_colour_discrete(h.start = 45) + 
  scale_fill_discrete(h.start = 45) +
  facet_grid(var~FeedbackCondition)
  
# caption = "Mean advice distance and error for advice from the two advisors. Lines show individual participant means, while violins and boxplots give distributions. The dashed line shows the (empirically estimated) error by placing an advisor's marker randomly on the timeline."
  
```

### Summary {.summary}

As planned, the advisors' advice differed. The Accurate advisor had lower error than the Agreeing advisor, and the distance between the participant's initial estimate and the advisor's marker was lower for the Agreeing advisor than the Accurate advisor. This need not by the case by definition (they will be equivalent if the participant is perfectly accurate), but are expected for any reasonable level of participant (in)accuracy.

## Advice during Familiarization

The participants had a familiarization block with each of the advisors individually. While these blocks are relatively short, learning may be rapid enough to demonstrate differences in influence. 

```{r}

optimalIsh <- tibble(
  advisor0idDescription = unique(AdvisedTrial$advisor0idDescription),
  woa = if_else(advisor0idDescription == "Agreeing", 0, NA_real_)
  ) %>%
  filter(advisor0idDescription == "Agreeing")

AdvisedTrial %>% 
  filter(Phase == "Familiarization") %>%
  group_by(pid, advisor0idDescription, FeedbackCondition, Phase) %>%
  summarise(woa = mean(advisor0woa)) %>%
  ggplot(aes(x = advisor0idDescription, y = woa)) +
  geom_blank() +
  geom_segment(aes(x = as.numeric(advisor0idDescription) - .33, 
                   xend = as.numeric(advisor0idDescription) + .33, 
                   y = woa, yend = woa), 
               linetype = "dashed", data = optimalIsh) +
  geom_violin(alpha = .15, colour = NA, aes(fill = advisor0idDescription)) +
  geom_boxplot(fill = "white", outlier.color = NA, 
               aes(colour = advisor0idDescription),
               width = .25, size = 1.25) +
  geom_line(alpha = .25, aes(group = pid)) + 
  scale_y_continuous(limits = c(0, 1)) +
  scale_colour_discrete(name = "Advisor", h.start = 45) + 
  scale_fill_discrete(name = "Advisor", h.start = 45) +
  labs(x = "Advisor", y = "Weight on Advice") +
  facet_grid(~ Phase + FeedbackCondition)
  
# caption = "Influence of the advice during the familiarization phase. Faint lines show individual participant means, while box plots and violins show the distributions. Dashed lines show the theoretically optimal level of advice-taking. The optimal level of advice-taking for the Accurate advisor depends upon the ability of the participant, but is likely to be very high (>.75)."

```
We included a couple of trials in each familiarization block where the advisors gave matched advice. We can compare influence just on those trials.

```{r}

# Bayes factors for comparisons within facets
tmp <- AdvisedTrial %>% 
  filter(Phase == "Familiarization",
         advisor0actualType == "disagreeReflected") %>%
  select(pid, advisor = advisor0idDescription, feedback = FeedbackCondition, firstAdvisor, woa = advisor0woa) %>%
  mutate(feedback = factor(feedback), woa = as.numeric(woa)) %>%
  arrange(pid, advisor, feedback, firstAdvisor)

# Drop participants with only observations for a single advisor
drop_ids <- tmp %>% 
  group_by(pid, advisor) %>% 
  summarise(woa = mean(woa)) %>%
  group_by(pid) %>% 
  summarise(n = n()) %>%
  filter(n < length(unique(AdvisedTrial$advisor0idDescription))) %>%
  pull(pid)

if (length(drop_ids)) {
  warning(paste0('Dropping ', length(drop_ids), 
                 ' pids without observations for all advisors. Pids: [', 
                 paste0(drop_ids, collapse = ','), ']'))
  tmp <- tmp %>% filter(!(pid %in% drop_ids))
}

comp.bf <- tmp %>% 
  group_by(pid, advisor, feedback) %>%
  summarise(woa = mean(woa)) %>%
  nest(d = -feedback) %>%
  mutate(
    bf = map_dbl(d, ~ ttestBF(
      .$woa[.$advisor == "Accurate"], 
      .$woa[.$advisor != "Accurate"],
      paired = T
    )@bayesFactor$bf %>% exp()),
    maxwoa = map_dbl(d, ~ max(.$woa)),
    acc = "Accurate",
    agr = "Agreeing"
  ) %>%
  mutate(FeedbackCondition = feedback)

optimalIsh <- tibble(
  advisor0idDescription = unique(AdvisedTrial$advisor0idDescription),
  woa = if_else(advisor0idDescription == "Agreeing", 0, NA_real_)
  ) %>%
  filter(advisor0idDescription == "Agreeing")

AdvisedTrial %>% 
  filter(Phase == "Familiarization",
         advisor0actualType == "disagreeReflected") %>%
  group_by(pid, advisor0idDescription, FeedbackCondition, Phase, advisor0actualType) %>%
  summarise(woa = mean(advisor0woa)) %>%
  ggplot(aes(x = advisor0idDescription, y = woa)) +
  geom_blank() +
  geom_segment(aes(x = as.numeric(advisor0idDescription) - .33, 
                   xend = as.numeric(advisor0idDescription) + .33, 
                   y = woa, yend = woa), 
               linetype = "dashed", data = optimalIsh) +
  geom_violin(alpha = .15, colour = NA, aes(fill = advisor0idDescription)) +
  geom_boxplot(fill = "white", outlier.color = NA, 
               aes(colour = advisor0idDescription),
               width = .25, size = 1.25) +
  geom_line(alpha = .25, aes(group = pid)) + 
  scale_y_continuous(limits = c(0, 1.15), breaks = seq(0, 1, .25)) +
  scale_colour_discrete(name = "Advisor", h.start = 45) + 
  scale_fill_discrete(name = "Advisor", h.start = 45) +
  labs(x = "Advisor", y = "Weight on Advice") +
  facet_grid(~ advisor0actualType + Phase + FeedbackCondition) +
  geom_segment(aes(x = acc, xend = agr, y = 1.1, yend = 1.1), 
               data = comp.bf) +
  geom_label(aes(x = 1.5, y = 1.1, label = paste0('BF ', num2str(bf))),
             data = comp.bf)
  
# caption = "Influence of the advice during the familiarization phase. Faint lines show individual participant means, while box plots and violins show the distributions. Dashed lines show the theoretically optimal level of advice-taking. The optimal level of advice-taking for the Accurate advisor depends upon the ability of the participant, but is likely to be very high (>.75)."

```

### Summary {.summary}

Participants' weight on advice for the disagreeReflected (off brand) trials formed the primary outcome measure of [a previous study](https://oxacclab.github.io/ExploringSocialMetacognition/analysis/ACv2/writeUp_v1.0.1.html). Here we do not observe as striking a pattern of greater influence of Accurate advisors in the Feedback condition and of Agreeing advisors in the No feedback condition. We do, however, observe a greater influence of Accurate advisors in the Feedback condition, and it is plausible there is slightly greater influence for the Agreeing advisor in the No feedback condition. 

## Hypothesis testing

The hypotheses being tested here are:  

1. Participants' pick rates for the two advisors will differ by condition.  
2. Participants in the Feedback condition will pick the Accurate advisor more than the Agreeing advisor.  
3. Participants in the No feedback condition will pick the Agreeing advisor more than the Accurate advisor.  

```{r h1}

tmp <- AdvisedTrial %>%
  filter(Phase == "Test") %>%
  group_by(pid, FeedbackCondition, firstAdvisor) %>%
  summarise(`p(chooseAccurate)` = mean(advisor0idDescription == "Accurate"))

ggplot(tmp, aes(x = FeedbackCondition, y = `p(chooseAccurate)`)) +
  geom_hline(yintercept = .5, linetype = 'dashed') +
  geom_violin(fill = "grey85", colour = NA) +
  geom_boxplot(width = .25, size = 1.25, fill = "white", outlier.color = NA) +
  geom_point(position = position_jitter(.05), shape = 4, size = 3, alpha = .5) +
  theme(text = element_text(size = 16))

# caption = "Distribution of participant advisor selection. Crosses show participants' proportion of choice trials where the Accurate advisor was chosen over the Agreeing advisor. Violins and box plots show distributions, while the dashed line indicates chance level choosing."

ttestBF(formula = `p(chooseAccurate)` ~ FeedbackCondition, data = tmp)
```

```{r h2}
ttestBF(tmp$`p(chooseAccurate)`[tmp$FeedbackCondition == "Feedback condition"], mu = .5)
```

```{r h3}
ttestBF(tmp$`p(chooseAccurate)`[tmp$FeedbackCondition != "Feedback condition"], mu = .5)
```

### Summary {.summary}

The participants' advisor pick rate differed according to whether the participants were in the Feedback or No feedback condition. Those in the Feedback condition showed a strong preference for the Accurate advisor. Those in the No feedback condition showed no systematic preference.

## Exploration

### Influence during Familiarization x Pick preference

For influence we use the disagreeReflected trials only.

```{r}

tmp <- AdvisedTrial %>%
  filter(advisor0actualType == "disagreeReflected" | Phase == "Test") %>%
  group_by(pid, advisor0idDescription, FeedbackCondition, Phase) %>%
  summarise(woa = mean(advisor0woa), n = n()) 
tmp <- tmp %>%
  pivot_wider(names_from = advisor0idDescription, values_from = c(woa, n)) %>%
  ungroup() %>%
  mutate_if(is.numeric, ~ if_else(is.na(.), 0, as.numeric(.))) %>%
  mutate(woa = woa_Accurate / (woa_Accurate + woa_Agreeing),
         n = n_Accurate / (n_Accurate + n_Agreeing)) %>%
  pivot_wider(names_from = Phase, values_from = c(woa, n)) %>%
  group_by(pid, FeedbackCondition) %>%
  summarise(woa = mean(woa_Familiarization, na.rm = T), n = mean(n_Test, na.rm = T))

adviceTaking <- tmp

print('>> Feedback condition (disagreeReflected trials only)')
cor.test(formula = ~ n + woa, data = tmp[tmp$FeedbackCondition == "Feedback condition", ])
print('>> No feedback condition (disagreeReflected trials only)')
cor.test(formula = ~ n + woa, data = tmp[tmp$FeedbackCondition != "Feedback condition", ])

ggplot(tmp, aes(y = woa, x = n, colour = FeedbackCondition)) + 
  geom_smooth(method = 'lm', se = F, fullrange = T) +
  geom_point() +
  coord_fixed() +
  scale_x_continuous(limits = c(0, 1)) +
  scale_y_continuous(limits = c(0, 1)) + 
  labs(y = "WoA_Acc / (WoA_Acc + WoA_Agr) during Familiarization", 
       x = "N_Acc / (N_Acc + N_Agr) during Test")

```

### Advisor ratings

We can look at the questionnaire items for the advisors, too.

```{r}

tmp <- left_join(debrief.advisors, adviceTaking, by = "pid") %>%
  left_join(advisors %>% select(pid, id, idDescription), by = c("pid", "advisorId" = "id")) %>%
  select(pid, knowledge, helpfulness, likability, comment, advisorId, 
         FeedbackCondition, woa, n, idDescription) %>%
  mutate_if(is.numeric, as.numeric) %>%
  filter(!is.na(FeedbackCondition)) 

tmp

tmp %>%
  gather(key = "var", value = "value", knowledge:likability) %>%
  ggplot(aes(x = idDescription, y = value,
                  fill = idDescription, colour = idDescription)) +
  geom_violin(colour = NA, alpha = .4) +
  geom_boxplot(fill = "white", width = .2, size = 1.25) +
  geom_line(colour = "black", alpha = .1, aes(group = pid)) +
  scale_colour_discrete(h.start = 45, name = "Advisor") +
  scale_fill_discrete(h.start = 45, name = "Advisor") +
  scale_y_continuous(limits = c(0, 100)) +
  facet_grid(FeedbackCondition ~ var) +
  labs(x = "Advisor", y = "Questionnaire rating")

# Arrange QQ into difference scores
diff <- tmp %>% 
  mutate(Advisor = as.character(idDescription)) %>%
  pivot_wider(id_cols = c(pid, woa, n, FeedbackCondition), 
              names_from = Advisor, 
              values_from = knowledge:likability) %>%
  mutate(knowledge = knowledge_Accurate - knowledge_Agreeing,
         helpfulness = helpfulness_Accurate - helpfulness_Agreeing,
         likability = likability_Accurate - likability_Agreeing) %>%
  select(pid, woa, n, FeedbackCondition, knowledge, helpfulness, likability) %>%
  pivot_longer(knowledge:likability)

cors <- NULL
for (v in unique(diff$name)) {
  for (fb in c(unique(diff$FeedbackCondition), 'all')) {
    for (stat in c('n', 'woa')) {
      x <- diff %>% filter(name == v)
      x$stat <- pull(x, stat)
      if (fb != 'all')
        x <- x %>% filter(FeedbackCondition == fb)
      c.fr <- cor.test(x$stat, x$value)
      c.bf <- correlationBF(x$stat, x$value)
      
      cors <- rbind(
        cors, 
        tibble(
          name = v,
          FeedbackCondition = fb,
          stat = stat,
          r = c.fr$estimate,
          p = c.fr$p.value,
          bf = exp(c.bf@bayesFactor$bf),
          s = paste0('r', lteq(prop2str(r, minPrefix = NA), sep = ""), 
                     ' p', lteq(prop2str(p, minPrefix = NA), sep = ""), 
                     '\nbf(rho!=0)=', num2str(bf, 1))
        )
      )
    }
  }
}

cors %>% filter(FeedbackCondition == 'all') %>% arrange(stat, name)

ggplot(diff, aes(x = n, y = value)) +
  geom_smooth(method = 'lm', alpha = .25, fill = "lightblue", fullrange = T) +
  geom_point(alpha = .5) +
  geom_text(
    x = .5, y = -100, hjust = .5, vjust = 0, aes(label = s), 
    data = cors[cors$stat == 'n' & cors$FeedbackCondition != 'all', ]
    ) +
  scale_x_continuous(limits = c(0, 1)) +
  scale_y_continuous(limits = c(-100, 100)) +
  labs(x = "Pick proportion (Accurate)",
       y = "Questionnaire score (Accurate - Agreeing)") +
  facet_grid(FeedbackCondition ~ name) 

ggplot(diff, aes(x = woa, y = value)) +
  geom_smooth(method = 'lm', alpha = .25, fill = "lightblue", fullrange = T) +
  geom_point(alpha = .5) +
  geom_text(
    x = .5, y = -100, hjust = .5, vjust = 0, aes(label = s), 
    data = cors[cors$stat == 'woa' & cors$FeedbackCondition != 'all', ]
    ) +
  scale_x_continuous(limits = c(0, 1)) +
  scale_y_continuous(limits = c(-100, 100)) +
  labs(x = "Weight on Advice (Accurate - Agreeing)",
       y = "Questionnaire score (Accurate - Agreeing)") +
  facet_grid(FeedbackCondition ~ name) 

```

#### Summary {.summary}

Participants in the Feedback condition showed a tendency to give higher ratings to the Accurate advisor, especially regarding helpfulness and knowledge. Those in the No feedback condition did not demonstrate especially systematic preferences, but it is probable that preferences were reasonably stable within individual participants across measures.

Overall, differences between advisors in participants' ratings for all three questionnaire items were correlated with the extent to which they picked the higher rated advisor. This pattern was not shown significantly for influence, although the evidence was equivocal rather than indicative of equivalence. The pattern was shown most strongly for picking by participants in the Feedback condition.

### Debrief comments

```{r}

AdvisedTrial %>% 
  nest(d = c(-pid, -condition)) %>% 
  mutate(acc = map_chr(d, ~ pull(., Accurate.name) %>% .[complete.cases(.)] %>% unique()),
         agr = map_chr(d, ~ pull(., Agreeing.name) %>% .[complete.cases(.)] %>% unique())) %>%
  select(pid, condition, acc, agr) %>% 
  left_join(debrief.form, by = "pid") %>%
  select(pid, condition, acc, agr, comment) %>%
  kable()

```

### Picking likelihood analysis

We can define some pretty good, clearly distinct models of how participants' pick proportions will look under a null model vs a preference model. The null model quite simply states that any preference in any direction of any strength is equally probable. Participants are known to vary in both their strength and direction of preference, so this model captures that intuition quite well. The preference model is agnostic as to strength of the preference, but places 95% of its area over the prefer Accurate side (allowing for ~5% of participants to make weird choices). 

```{r}

ac <- AdvisedTrial %>%
  filter(Phase == "Test") %>%
  group_by(pid, FeedbackCondition, firstAdvisor) %>%
  summarise(pAccurate = mean(advisor0idDescription == "Accurate"))

# The null model offers a flat prior over the whole preference space: 
# Some participants have strong preferences, others weak preferences, and they go both ways.
h0 <- function(x) case_when(
  x < 0 | x > 1 ~ 0,
  TRUE ~ 1/length(x)
)

# The preference model has two flat blocks, the preferred side gets 95% of the mass
prefAcc <- function(x) case_when(
  x < 0 | x > 1 ~ 0,
  x <= .5 ~ .05 / length(x),
  x > .5 ~ (2 - .05) / length(x)
)

prefAgr <- function(x) case_when(
  x < 0 | x > 1 ~ 0,
  x <= .5 ~ (2 - .05) / length(x),
  x > .5 ~ .05 / length(x)
)

ggplot(tibble(x = seq(0, 1, length.out = 10000)), aes(x = x)) +
  geom_hline(yintercept = 0, colour = "grey75") +
  stat_function(fun = h0, colour = NA, alpha = .25, aes(fill = "Null model"), size = 1.25, geom = 'area') +
  stat_function(fun = prefAcc, colour = NA, alpha = .25, aes(fill = 'Prefer Accurate model'), size = 1.25, geom = 'area') +
  # geom_rug(aes(x = pAccurate, colour = condition), data = ac, alpha = .25, size = 1.25) +
  labs(x = "Mean Accurate advisor pick proportion", y = "Likelihood") +
  scale_fill_discrete(name = "") +
  theme(text = element_text(size = 16),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank())  +
  geom_rug(aes(x = pAccurate, colour = FeedbackCondition), data = ac, alpha = .25, size = 1.25) +
  scale_colour_discrete(name = "Data")

# Likelihood of the models is given by taking the height of the curve at each of the observed data points
e <- 1e-10  # tiny nudge to stop getting log(0)

ac <- ac %>% 
  nest(d = -FeedbackCondition) %>% 
  mutate(
    LLpAcc = map(d, ~ log(prefAcc(.$pAccurate) + e)),
    LLpAgr = map(d, ~ log(prefAgr(.$pAccurate) + e)),
    LLh0 = map(d, ~ log(h0(.$pAccurate) + e))
  ) %>%
  unnest(cols = -FeedbackCondition)
  
ac %>% 
  group_by(FeedbackCondition) %>%
  summarise(
    LRatio_pAcc_h0 = exp(sum(LLpAcc) - sum(LLh0)),
    LRatio_pAgr_h0 = exp(sum(LLpAgr) - sum(LLh0)),
    LRatio_pAcc_pAgr = exp(sum(LLpAcc) - sum(LLpAgr))
  )

ac %>% 
  group_by(FeedbackCondition) %>%
  select(-pAccurate) %>%
  summarise_if(is.numeric, sum) %>%
  pivot_longer(cols = -FeedbackCondition) %>%
  ggplot(aes(x = name, y = value, fill = FeedbackCondition)) +
  geom_col(colour = NA, position = 'dodge') +
  labs(x = 'Model', y = 'Sum(LogLikelihood)') +
  theme(text = element_text(size = 16),
        axis.title.x = element_blank())

```
#### Summary {.summary}

The likelihood analysis shows that the Accuracy Preference model is the best fit for the picking data from participants in the Feedback condition, being nearly three times as likely as the Null model and much, much more likely than the opposite preference model. The No feedback condition picking data is best accommodated by the Null model, with the two Preference models being equally likely, and both being over 12.5 million times less likely than the Null model.

### ANOVA for Weight on Advice

```{r}

tmp <- AdvisedTrial %>% 
  filter(Phase == "Familiarization",
         advisor0actualType == "disagreeReflected") %>%
  select(pid, advisor = advisor0idDescription, feedback = FeedbackCondition, firstAdvisor, woa = advisor0woa) %>%
  mutate(feedback = factor(feedback), woa = as.numeric(woa)) %>%
  arrange(pid, advisor, feedback, firstAdvisor)

tmp %>% group_by(pid, advisor, feedback, firstAdvisor) %>% summarise(woa = mean(woa))

# Drop participants with only observations for a single advisor
drop_ids <- tmp %>% 
  group_by(pid, advisor) %>% 
  summarise(woa = mean(woa)) %>%
  group_by(pid) %>% 
  summarise(n = n()) %>%
  filter(n < length(unique(AdvisedTrial$advisor0idDescription))) %>%
  pull(pid)

if (length(drop_ids)) {
  warning(paste0('Dropping ', length(drop_ids), 
                 ' pids without observations for all advisors. Pids: [', 
                 paste0(drop_ids, collapse = ','), ']'))
  tmp <- tmp %>% filter(!(pid %in% drop_ids))
}

ezANOVA(
  data = tmp,
  dv = woa,
  wid = pid,
  within = advisor,
  between = feedback:firstAdvisor
)

a.bf <- anovaBF(woa ~ feedback + firstAdvisor + advisor + pid, 
                data = tmp, whichRandom = 'pid', progress = F)

marginalBF(a.bf, list(c(
  'advisor + feedback + advisor:feedback + firstAdvisor + pid',
  'advisor + feedback + firstAdvisor + pid'
)))

# Comparison of influence for the advisors by feedback
tt <- tmp %>% 
  group_by(pid, advisor, feedback) %>%
  summarise(woa = mean(woa)) %>%
  nest(d = -feedback) %>%
  mutate(str = map_chr(d, ~ md.ttest(
    .$woa[.$advisor == "Accurate"], .$woa[.$advisor != "Accurate"],
    labels = c('*M*|Acc', '*M*|Agr'),
    paired = T
  )))

for (r in 1:nrow(tt)) {
  cat('\n')
  print(as.character(tt$feedback[r]))
  cat(tt$str[r])
  cat('\n')
}

```

# Conclusions {.summary}

As with the previous experiment, a preference was clear in the picking data for the participants in the Feedback condition: they appeared to learn about the differences in the advisors and to use that to identify and then select the Accurate advisor. Participants in the No feedback condition showed no systematic preference, and their data are consistent with a model wherein individual preferences are not systematically altered by experience. Participants in the No feedback condition did show a systematic relationship between how much advice they took from an advisor and the extent to which they picked that advisor, although it is not known whether the relationship is learned or based on an initial preference which is not updated.

# Credits 

## Acknowledgements

Thanks as always to Nick Yeung and the other folks at the [ACC Lab](https://www.psy.ox.ac.uk/research/attention-cognitive-control-lab).

## R Packages

```{r results = 'asis'}
# list packages
packageNames <- (.packages())
# don't include very core package
packageNames <- packageNames[!(packageNames %in% 
                                 rownames(installed.packages(
                                   priority = "base")))]
# but do include the base package
packageNames <- c("base", packageNames)
out <- NULL
for (p in packageNames) {
  out <- rbind(out, data.frame('Package' = p, 
                               'Citations' = paste(format(citation(p), 
                                                          style = 'textVersion'), 
                                                   collapse = '<br/><br/>')))
}

as_tibble(out)
```

## Funding

Matt Jaquiery is funded by a studentship from the [Medical Research Council](https://mrc.ukri.org/) (reference 1943590) and the University of Oxford [Department of Experimental Psychology](https://www.psy.ox.ac.uk/) (reference 17/18_MSD_661552).

## Technical details  

```{r results = 'hold'}
cat(paste('Time stamp:', Sys.time(), '\n\n'))
cat('Runtime \n')
proc.time()
cat('\n')
sessionInfo()
```