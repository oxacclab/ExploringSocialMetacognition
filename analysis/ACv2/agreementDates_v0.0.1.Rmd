---
title: "Agreement (Dates)"
author: "[Matt Jaquiery](https://github.com/mjaquiery) ([matt.jaquiery@psy.ox.ac.uk](mailto:matt.jaquiery@psy.ox.ac.uk))"
output:
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '3'
    css: ../src/writeUp.css
    includes:
      after_body: ../src/toc_menu.html
  html_notebook:
    toc: yes
    toc_depth: 3
    css: ../src/writeUp.css
    includes:
      after_body: ../src/toc_menu.html
editor_options:
  chunk_output_type: inline
---

June 2020

[Script run `r Sys.time()`]


```{r prematter, include = F}

library(tidyverse)
library(ggridges)
library(scales)
library(broom)
library(BayesFactor)
library(prettyMD)
library(knitr)
library(parallel)
library(sjlabelled)

opts_chunk$set('echo' = F)

set.seed(20191216)

# Plot setup
theme_set(theme_light() + 
            theme(panel.grid = element_blank()))

```

```{r loadData, include = F}
# rDir <- "http://localhost/ExploringSocialMetacognition/data/public/"
testData <- F

# Some of the data from earlier versions were corrupted due to mismatched columns, so we can load that data from the raw files
studyVersion <- "0-0-3"
studyName <- "agreementDates"

useRawTrialData <- T

source("src/01_Load-data.R")
preserveWorkspaceVars <- T

# The rest of the data should be okay to load directly
studyVersion <- "0-0-4"
useRawTrialData <- F

exclude <- list(
  maxTrialRT = 60000,    # trials take < 1 minute
  minTrials = 11,        # at least 11 trials completed
  minChangeRate = .1,    # some advice taken on 10%+ of trials
  custom = list(
      tenKeyTrials = 
        # Each participant should have all ten choice trials
        function(x) {
          x %>% 
            filter(advisorChoice == 1) %>%
            nrow() != 10
        }
    )
  ) 

skipLoadData <- F

source("src/02_Exclusions.R")

if (testData) {
  # randomise the confidence of test data
  source("src/sim-confidence-estimation.R")
  model <- simulateCE(
    AdvisedTrial,
    agentInsensitivitySD = 8,
    agentConfidence = 2,
    agentConfidenceSD = 4,
    agentEgoBias = .7, 
    agentEgoBiasSD = .2,
    agentEffectSize = .1,
    agentEffectSizeSD = .1
    )
  AdvisedTrial <- model$trials
  decisions <- byDecision(AdvisedTrial)
}

```

# Introduction

The results from the Dots task have been hard to reconcile with the results of the Dates task. We therefore attempt to replicate some basic results from the Dots task using the Dates task. In this case, we attempt to demonstrate that people prefer to hear advice from an advisor who agrees with them more often, even when both advisors are balanced for objective accuracy in the advice they provide.

## Preregistration 

This study was preregistered [on the OSF](https://osf.io/8d7vg).

# Method

The experimental code is available on [GitHub](https://github.com/oxacclab/ExploringSocialMetacognition), and the experiment can be performed by visiting [https://acclab.psy.ox.ac.uk/~mj221/ESM/ACBin/agr.html?PROLIFIC_PID=WriteUp](https://acclab.psy.ox.ac.uk/~mj221/ESM/ACBin/agr.html?PROLIFIC_PID=WriteUp). 

# Results

## Exclusions

```{r exclusions}

if (testData) {
  print("NOTE: Testing (simulated) data!!!!")
}

tmp <- suppressWarnings(left_join(exclusions, okayIds, by = "pid"))

tmp$condition <- factor(tmp$condition, labels = c("¬FbLowFirst",
                                                  "¬FbHighFirst",
                                                  "FbLowFirst",
                                                  "FbHighFirst"))

table(tmp$excluded, tmp$condition)

# Check conditions
tmp <- left_join(AdvisedTrial, tmp) %>% filter(block == 2)

table(tmp$condition, tmp$advisor0idDescription, tmp$feedback)

tmp <- unique(tmp %>% select(pid, condition)) %>%
  mutate(FeedbackCondition = if_else(str_starts(condition, 'Fb'),
                                     "Feedback", "No Feedback"))

f <- function(x) case_when(x == 1 ~ "Choice Phase", 
                           T ~ "Familiarization")

AdvisedTrial <- left_join(AdvisedTrial, tmp, by = "pid") %>%
  mutate(Phase = f(advisorChoice))

decisions <- byDecision(AdvisedTrial)

```

Our final participant list consists of `r length(unique(AdvisedTrial$pid))` participants who completed an average of `r num2str(mean(aggregate(advisor0 ~ pid, AdvisedTrial, length)$advisor0))` trials each, of which an average of `r num2str(mean(aggregate(advisor0 ~ pid, AdvisedTrial[!AdvisedTrial$feedback, ], length)$advisor0))` had no feedback.

## Task performance

First we offer a characterisation of the task, to provide the reader with a sense of how the participants performed. 

### Decisions

Participants offered estimates of whether various events took place before or after a given year. The correct answers were always between 1900 and 2000. Participants answered by selecting a point on one of two confidence bars, with the choice of bar indicating the chosen answer. 

#### Correctness

```{r accuracy}

decisions %>%
  filter(Phase == "Familiarization") %>%
  peek(responseCorrect, decision) %>%
  num2str.tibble(isProportion = T, precision = 3)

decisions %>% 
  filter(Phase == "Familiarization",
         FeedbackCondition == "Feedback") %>% 
  peek(responseCorrect, decision) %>% 
  num2str.tibble(isProportion = T, precision = 3) %>%
  cbind(FeedbackCondition = "Feedback") %>%
  bind_rows(
    decisions %>% 
      filter(Phase == "Familiarization",
             FeedbackCondition == "No Feedback") %>% 
      peek(responseCorrect, decision) %>% 
      num2str.tibble(isProportion = T, precision = 3) %>%
      cbind(FeedbackCondition = "No Feedback")
  )

```

```{r accuracyGraph}

tmp <- decisions %>% 
  filter(Phase == "Familiarization") %>%
  group_by(pid, decision, Phase, FeedbackCondition) %>%
  do(correct = mean_cl_normal(.$responseCorrect)) %>%
  unnest(correct) %>%
  transmute(pid, decision, Phase, FeedbackCondition,
            pCorrect = y, 
            ciLow = ymin,
            ciHigh = ymax)

ggplot(tmp, 
       aes(x = decision, y = pCorrect)) +
  geom_violin(alpha = .15, colour = NA, aes(fill = decision)) +
  geom_boxplot(fill = "white", outlier.color = NA, aes(colour = decision),
               width = .25, size = 1.25) +
  geom_line(alpha = .25, aes(group = pid)) + 
  scale_y_continuous(limits = c(0, 1)) +
  labs(x = "decision", 
       y = "p(response correct)") +
  facet_grid(~FeedbackCondition + Phase)

```

### Timing

We can look at the response time - the difference between the time the response is opened and the time the response is received.  

```{r time}

decisions$rt <- decisions$responseTimeEstimate - decisions$timeResponseOpen

decisions %>% peek(rt, decision) %>% 
  mutate_if(is.numeric, round)

```

```{r timeGraph}

tmp <- decisions %>% 
  filter(Phase == "Familiarization") %>% 
  group_by(pid, decision, Phase) %>% 
  summarise(rt = mean(rt))

ggplot(tmp, 
       aes(x = decision, y = rt)) +
  geom_violin(alpha = .15, colour = NA, aes(fill = decision)) +
  geom_boxplot(fill = "white", outlier.color = NA, aes(colour = decision),
               width = .25, size = 1.25) +
  geom_line(alpha = .25, aes(group = pid)) + 
  labs(x = "decision", 
       y = "response time (ms)")  +
  facet_wrap(~Phase)

```

### Summary {.summary}

Accuracy was slightly higher for final decisions compared to initial estimates for participants in both Feedback and No feedback conditions, although these differences appeared to me more pronounced in the No feedback condition, which was unexpected. It is possible that participants consider these questions difficult, and that feedback raises their confidence that they can perform well on the task. This confidence boost may result in less advice-taking and, because the advice is better on average than most participants, consequently reduce the benefit participants receive from advice.

All participants tend to take less time to consider advice than to make the initial decision, as expected given that the advice is presented in an animation during which the participant can be integrating the advice with the initial decision.

## Metacognitive performance

### Confidence

Each answer bar allowed the participant to express their confidence in that answer by selecting a higher point on the bar for a higher confidence, in a range of 0-100% for each decision.

```{r confidence}

decisions %>% 
  filter(Phase == "Familiarization") %>%
  peek(responseConfidence, decision) %>%
  num2str.tibble(isProportion = T, precision = 1)

decisions %>% 
  filter(Phase == "Familiarization",
         FeedbackCondition == "Feedback") %>% 
  peek(responseConfidence, decision) %>% 
  num2str.tibble(isProportion = T, precision = 3) %>%
  cbind(FeedbackCondition = "Feedback") %>%
  bind_rows(
    decisions %>% 
      filter(Phase == "Familiarization",
             FeedbackCondition == "No Feedback") %>% 
      peek(responseConfidence, decision) %>% 
      num2str.tibble(isProportion = T, precision = 3) %>%
      cbind(FeedbackCondition = "No Feedback")
  )

```

```{r confidence graph}

tmp <- decisions %>% 
  filter(Phase == "Familiarization") %>%
  group_by(pid, decision, FeedbackCondition, Phase) %>%
  do(confidence = mean_cl_normal(.$responseConfidence)) %>%
  unnest(confidence) %>%
  transmute(pid, decision, FeedbackCondition, Phase,
            confidence = y, 
            ciLow = ymin,
            ciHigh = ymax)

ggplot(tmp, 
       aes(x = decision, y = confidence)) +
  geom_violin(alpha = .15, colour = NA, aes(fill = decision)) +
  geom_boxplot(fill = "white", outlier.color = NA, aes(colour = decision),
               width = .25, size = 1.25) +
  geom_line(alpha = .25, aes(group = pid)) + 
  scale_y_continuous(limits = c(0, 100)) +
  labs(x = "decision", 
       y = "confidence") +
  facet_grid(~FeedbackCondition + Phase)

```

### Relationship between confidence and p(correct)

Participants indicated the confidence in their decisions simultaneously with the decisions themselves. 
We can thus explore how well a participant's confidence signals their accuracy. 
We provide only a brief examination here where normalised responses are split into high and low confidence for each participant, and the accuracies of the two categories are compared. 
This is done separately for first and last decisions.

```{r confidence normalisation}

decisions <- decisions %>% 
  group_by(pid, decision) %>% 
  dplyr::mutate(zConf = scale(responseConfidence)) %>%
  ungroup() 

```

```{r confidence binary split}

tmp <- decisions %>% 
  mutate(highConf = zConf > 0) %>%
  filter(Phase == "Familiarization") %>%
  group_by(pid, decision, highConf, Phase, FeedbackCondition) %>%
  summarise(pCorrect = mean(responseCorrect)) %>%
  group_by(pid, decision) %>%
  spread(highConf, pCorrect) %>%
  mutate(pCorrectDiff = `TRUE` - `FALSE`)

tmp %>% 
  peek(pCorrectDiff, decision) %>% 
  num2str.tibble(isProportion = T, precision = 3)

print("p(correct|high confidence) - p(correct|low confidence) for initial estimates")
ttestBF(tmp$pCorrectDiff[tmp$decision == "first"])
print("p(correct|high confidence) - p(correct|low confidence) for final decisions")
ttestBF(tmp$pCorrectDiff[tmp$decision == "last"])

```

```{r confidence binary split plot}

ggplot(tmp, aes(x = decision, y = pCorrectDiff)) +
  geom_violin(alpha = .15, colour = NA, aes(fill = decision)) +
  geom_boxplot(fill = "white", outlier.color = NA, aes(colour = decision),
               width = .25, size = 1.25) +
  geom_line(alpha = .25, aes(group = pid)) + 
  labs(x = "decision", 
       y = "p(Correct | sure) - p(Correct | unsure)") +
  scale_y_continuous(limits = c(-1, 1)) +
  facet_grid(~Phase + FeedbackCondition)
 
```

### Summary {.summary}

As mentioned above, participants in the Feedback condition seem to have their confidence increased by feedback relative to those in the No feedback condition. 
It may be more accurate to suggest that the effect of feedback is to increase the variance of the confidence: some participants in the feedback condition get less confident, while others get more confidence, but the medians for the conditions are remarkably similar for both initial and final decisions.

For both conditions, participants demonstrated a slightly higher probability of being correct for more compared to less confident trials.

## Advisor performance

The high and low agreement advisors should agree at different rates but be approximately equivalently accurate.

### Accuracy

```{r adviceAccuracy}

AdvisedTrial$advisor0accurate <- ifelse(
  is.na(AdvisedTrial[, paste0(advisorNames[1], ".accurate")]),
  pull(AdvisedTrial[, paste0(advisorNames[2], ".accurate")]),
  pull(AdvisedTrial[, paste0(advisorNames[1], ".accurate")])
)

AdvisedTrial$advisor0agree <- ifelse(
  is.na(AdvisedTrial[, paste0(advisorNames[1], ".agree")]),
  pull(AdvisedTrial[, paste0(advisorNames[2], ".agree")]),
  pull(AdvisedTrial[, paste0(advisorNames[1], ".agree")])
)

tmp <- AdvisedTrial %>% 
  filter(Phase == "Familiarization") %>%
  group_by(pid, advisor0idDescription, FeedbackCondition, Phase) %>%
  summarise(advisorAccuracy = mean(advisor0accurate),
            advisorAgree = mean(advisor0agree)) 

tmp %>% 
  group_by(advisor0idDescription, FeedbackCondition) %>%
  summarise(advisorAccuracy = mean(advisorAccuracy),
            advisorAgree = mean(advisorAgree)) %>%
  num2str.tibble(isProportion = T, precision = 3)

```

```{r adviceAccuracyGraph}

tmp %>% 
  dplyr::rename(advisor = advisor0idDescription,
                `p(correct)` = advisorAccuracy,
                `p(agree)` = advisorAgree) %>%
  gather("var", "value", `p(correct)`:`p(agree)`) %>%
  ggplot(aes(x = advisor, y = value)) +
  geom_violin(alpha = .15, colour = NA, aes(fill = advisor)) +
  geom_boxplot(fill = "white", outlier.color = NA, aes(colour = advisor),
               width = .25, size = 1.25) +
  geom_line(alpha = .25, aes(group = pid)) + 
  labs(x = "advisor", 
       y = "mean") +
  scale_y_continuous(limits = c(0, 1)) +
  scale_fill_discrete(h.start = 45) + 
  scale_colour_discrete(h.start = 45) +
  facet_grid(var ~ Phase + FeedbackCondition)
  
  
```

### Summary {.summary}

The High and Low agreement advisors differed as expected, with the High agreement advisor being more likely to agree with the participant. 
There did, however, also appear to be some differences in advisor accuracy, with the Low agreement advisor being more likely to be correct in both conditions. 
This was especially true in the No feedback condition where participant accuracy was slightly lower than expected.

### Influence

The measure of influence is the extent to which a participant's confidence moves in the expected direction as a function of the advice (increasing when agreed with, decreasing when disagreed with).

Influence is the primary outcome measure, and is thus expected to differ between advisors and feedback conditions.

```{r influence}

AdvisedTrial$advisor0influence <- 
  unlist(sapply(1:nrow(AdvisedTrial),
                function(i) 
                  AdvisedTrial[i, 
                               paste0(AdvisedTrial$advisor0idDescription[i], 
                                      ".influence")]))

tmp <- AdvisedTrial %>% 
  filter(Phase == "Familiarization") %>%
  group_by(pid, advisor0idDescription, FeedbackCondition, Phase) %>%
  summarise(advisorInfluence = mean(advisor0influence)) 

tmp %>% 
  group_by(advisor0idDescription, FeedbackCondition) %>%
  summarise(advisorInfluence = mean(advisorInfluence)) %>%
  num2str.tibble(precision = 1)

```

```{r influenceGraph}

tmp %>% 
  rename(advisor = advisor0idDescription) %>%
  ggplot(aes(x = advisor, y = advisorInfluence)) +
  geom_violin(alpha = .15, colour = NA, aes(fill = advisor)) +
  geom_boxplot(fill = "white", outlier.color = NA, aes(colour = advisor),
               width = .25, size = 1.25) +
  geom_line(alpha = .25, aes(group = pid)) + 
  scale_y_continuous(limits = c(-100, 100)) +
  scale_fill_discrete(h.start = 45) + 
  scale_colour_discrete(h.start = 45) +
  facet_grid(~Phase + FeedbackCondition)

```

```{r}

library(ez)

AdvisedTrial %>% 
  dplyr::filter(Phase == "Familiarization") %>%
  group_by(pid, advisor0idDescription, FeedbackCondition) %>%
  dplyr::summarize(influence = mean(advisor0influence, na.rm = T)) %>%
  ezANOVA(
    dv = influence,
    wid = pid,
    within = advisor0idDescription,
    between = FeedbackCondition
  )

```

#### Influence distribution

It's good to keep a general eye on the distribution of weight-on-advice on a trial-by-trial basis. 

```{r influenceDistribution}

ggplot(AdvisedTrial, aes(advisor0influence)) + 
  geom_histogram(aes(fill = advisor0idDescription)) +
  scale_fill_discrete(h.start = 45) + 
  facet_grid(FeedbackCondition ~ advisor0idDescription)

```

#### Is initial accuracy or initial confidence a better predictor of influence?

```{r influence correlations}

tmp <- AdvisedTrial %>% 
  filter(Phase == "Familiarization") %>%
  select(pid, responseCorrect, responseConfidence, advisor0influence) %>%
  nest(data = -pid) %>%
  mutate(
    accBF = map(data, ~ correlationBF(.x$responseCorrect, 
                                    .x$advisor0influence)),
    acc.BF = map(accBF, ~ exp(.@bayesFactor$bf)),
    conBF = map(data, ~ correlationBF(.x$responseConfidence, 
                                    .x$advisor0influence)),
    con.BF = map(conBF, ~ exp(.@bayesFactor$bf)),
    acc = map(data, ~ cor.test(as.numeric(.x$responseCorrect), 
                               .x$advisor0influence)), 
    acc = map(acc, tidy),
    con = map(data, ~ cor.test(.x$responseConfidence, .x$advisor0influence)), 
    con = map(con, tidy)
  ) %>% 
  unnest(acc, names_sep = ".") %>%
  unnest(con, names_sep = ".") %>%
  unnest(c(acc.BF, con.BF))

tmp <- bind_rows(
  tmp %>% 
    select_at(vars(starts_with('acc'))) %>%
    rename_all(~ str_replace(., 'acc\\.(.+)', '\\1')) %>%
    mutate(property = 'accuracy'),
  tmp %>% 
    select_at(vars(starts_with('con'))) %>%
    rename_all(~ str_replace(., 'con\\.(.+)', '\\1')) %>%
    mutate(property = 'confidence')
)

tmp %>% 
  group_by(property) %>%
  summarise_if(is.numeric, mean, na.rm = T)

```

### Summary {.summary}

There were differences in influence both between feedback conditions and between advisors, but no evidence of interaction.
Participants in the No feedback condition were more influenced by both advisors, and all participants were more influenced by Low rather than High agreement advisors. 
Due to the asymmetric nature of the scale, it is likely that the difference in influence between the advisors is driven by the difference in agreement rate: there is more potential for revising an estimate downwards than upwards, and downwards revision is overwhelmingly more likely for disagreeing advice than for agreeing advice.

Influence was lower for accurate rather than inaccurate initial estimates, and lower the more confident the initial estimate was.

## Hypothesis testing

The hypotheses being tested here are:  

1. Participants will select the high agreement advisor at a rate greater than chance when given feedback.

```{r h1, results='asis'}

tmp <- AdvisedTrial %>% 
  dplyr::filter(Phase == "Choice Phase") %>%
  nest(d = c(-pid, -advisor0idDescription)) %>%
  mutate(n = map_int(d, nrow)) %>% 
  mutate(
    d = NULL,
    advisor0idDescription = str_extract(advisor0idDescription, "high|low")
    ) %>%
  spread(advisor0idDescription, n) %>%
  mutate_if(is.numeric, ~ ifelse(is.na(.), 0, .)) %>%
  mutate(pPickHigh = high / (high + low)) %>%
  left_join(AdvisedTrial %>% select(pid, FeedbackCondition) %>% unique(), by = "pid")

tmp %>% 
  filter(FeedbackCondition == "Feedback") %>%
  pull(pPickHigh) %>%
  md.ttest(.5, labels = c("*M*|Feedback")) %>%
  cat()

cat('\n\n')

tmp %>% 
  filter(FeedbackCondition != "Feedback") %>%
  pull(pPickHigh) %>%
  md.ttest(.5, labels = c("*M*|¬Feedback")) %>%
  cat()

ggplot(tmp, aes(y = FeedbackCondition, x = pPickHigh)) +
  geom_density_ridges(aes(fill = FeedbackCondition)) +
  geom_point(position = position_jitter(0, .05), alpha = .25)

pickRates <- tmp %>% select(pid, pPickHigh, FeedbackCondition)

```

### Summary {.summary}

In the Feedback condition there was no average preference for the high or low agreement advisor.
In the No feedback condition there was an average preference for the high over the low agreement advisor.
Thus participants preferred to hear advice from agreeing advisors only when they did not receive feedback to indicate to them that there was no objective difference in the accuracy of the advisors.

## Exploration

### Does Confidence-contingent agreement predict influence?

Niccolo's project was looking at whether agreement based on confidence could allow the discovery of objective accuracy differences where agreement differences did not provide a valid proxy measurement. 
Agreement was controlled in his studies, and in this one they point the opposite way for many participants (High agreement advisor is less objectively accurate). 

```{r}

# categorise participants based on which advisor was most accurate
tmp <- AdvisedTrial %>% 
  filter(Phase == 'Familiarization') %>% 
  group_by(pid) %>% 
  summarise(
    h = mean(highAgreement.accurate, na.rm = T),
    l = mean(lowAgreement.accurate, na.rm = T)
  ) %>%
  transmute(
    pid,
    mostAccurateAdvisor = if_else(h > l, "High agreement", "Low agreement"),
    accuracyDifference = h - l
  ) %>% left_join(AdvisedTrial, by = 'pid')

tmp <- tmp %>% 
  remove_all_labels() %>% 
  nest(d = -pid) %>%
  mutate(
    d = map(d, ~ arrange(., responseConfidence) %>% 
              mutate(
                influenceQuantile = 
                  sapply(responseConfidence, 
                         function(x) 
                           which(rle(.$responseConfidence)$values == x)
                         ),
                confCat = case_when(
                  influenceQuantile > max(influenceQuantile) * .7 ~ 'high',
                  influenceQuantile < max(influenceQuantile) * .3 ~ 'low',
                  T ~ 'medium'
                )
              ))
    ) %>%
  unnest(d) %>% 
  select(pid, influenceQuantile, confCat, accuracyDifference, mostAccurateAdvisor, 
         ends_with('influence'), ends_with('agreement'), FeedbackCondition,
         advisor0idDescription, advisor0agree, advisor0accurate) 

for (f in unique(tmp$FeedbackCondition)) {
  print(
    tmp %>% 
      filter(FeedbackCondition == f) %>%
      nest(d = -pid) %>%
      slice(sample(1:nrow(.), 20)) %>%
      unnest_legacy(d) %>%
      ggplot(aes(x = advisor0influence, y = pid, fill = confCat, colour = confCat)) +
      geom_vline(xintercept = 0, linetype = 'dashed') +
      geom_density_ridges(alpha = 1/3, colour = 'black') +
      geom_point(position = position_jitter(0, .1), alpha = .75) +
      facet_grid(advisor0agree ~ advisor0idDescription + FeedbackCondition,
                 labeller = label_both)
  )
}


```

### Confidence change

```{r}

AdvisedTrial %>%
  filter(Phase == 'Familiarization') %>% 
  mutate(confDiff = case_when(
    responseAnswerSide == responseAnswerSideFinal ~ 
      responseConfidenceFinal - responseConfidence,
    T ~ 
      -(responseConfidenceFinal + responseConfidence)
    )) %>%
  ggplot(aes(x = confDiff, y = pid, fill = advisor0agree)) +
  geom_vline(xintercept = 0, linetype = 'dashed') +
  geom_density_ridges(alpha = .5) + 
  facet_grid(FeedbackCondition ~ Phase + advisor0idDescription)

```

### Influence x Pick rate

We want to look at the relationship between the influence difference of advisors in the Familiarization phase and the pick rates in the Choice phase. 

```{r}

tmp <- AdvisedTrial %>% 
  filter(Phase == "Familiarization") %>%
  group_by(pid, advisor0idDescription) %>%
  summarise(influence = mean(advisor0influence, na.rm = T))

tmp <- tmp %>% left_join(pickRates, by = 'pid') %>%
  pivot_wider(names_from = advisor0idDescription, values_from = influence, names_prefix = "influence.")

tmp <- tmp %>% 
  mutate(influenceDifference = influence.highAgreement - influence.lowAgreement) %>%
  select(pid, pPickHigh, FeedbackCondition, influenceDifference)

r <- tmp %>% 
  nest(d = -FeedbackCondition) %>%
  mutate(
    r = map(d, ~ cor.test(~ pPickHigh + influenceDifference, data = .)),
    r = map(r, tidy)
  ) %>% 
  unnest(r) %>%
  select(-d) %>%
  mutate(
    text = paste0('r ', lteq(prop2str(estimate, minPrefix = '')),
                  ' [95%CI ', prop2str(conf.low, minPrefix = ''), 
                  ', ', prop2str(conf.high, minPrefix = ''), 
                  ']\np ', lteq(prop2str(p.value, minPrefix = ''))) 
  )

ggplot(tmp, aes(x = pPickHigh, y = influenceDifference, 
                fill = FeedbackCondition, colour = FeedbackCondition)) +
  geom_hline(yintercept = 0, linetype = 'dashed') +
  geom_vline(xintercept = .5, linetype = 'dashed') +
  geom_smooth(method = 'lm') + 
  geom_point(alpha = .5, colour = 'black') +
  scale_x_continuous(limits = c(0, 1)) +
  geom_label(aes(x = .5, y = max(tmp$influenceDifference) * 1.4, label = text),
            data = r, fill = "white", colour = "black") +
  facet_wrap(~ FeedbackCondition)

```

### Questionnaires

#### Advisor questionnaires

Somehow I removed the individual advisor questionnaires during development and never put them back. I do not know why I did this stupid thing, but we don't have individual advisor questionnaire data to look at for these studies.

#### General feedback

```{r}

debrief.form %>% 
  arrange(desc(nchar(comment))) %>%
  select(pid, comment)

```

# Conclusions {.summary}

!TODO[Conclusions]

# Credits 

## Acknowledgements

Thanks as always to Nick Yeung and the other folks at the [ACC Lab](https://www.psy.ox.ac.uk/research/attention-cognitive-control-lab).

## R Packages

```{r results = 'asis'}
# list packages
packageNames <- (.packages())
# don't include very core package
packageNames <- packageNames[!(packageNames %in% 
                                 rownames(installed.packages(
                                   priority = "base")))]
# but do include the base package
packageNames <- c("base", packageNames)
out <- NULL
for (p in packageNames) {
  out <- rbind(out, data.frame('Package' = p, 
                               'Citations' = paste(format(citation(p), 
                                                          style = 'textVersion'), 
                                                   collapse = '<br/><br/>')))
}

kable(out)
```

## Funding

Matt Jaquiery is funded by a studentship from the [Medical Research Council](https://mrc.ukri.org/) (reference 1943590) and the University of Oxford [Department of Experimental Psychology](https://www.psy.ox.ac.uk/) (reference 17/18_MSD_661552).

## Technical details  

```{r results = 'hold'}
cat(paste('Time stamp:', Sys.time(), '\n\n'))
cat('Runtime \n')
proc.time()
cat('\n')
sessionInfo()
```