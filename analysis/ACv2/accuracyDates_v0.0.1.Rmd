---
title: "Accuracy (Dates)"
author: "[Matt Jaquiery](https://github.com/mjaquiery) ([matt.jaquiery@psy.ox.ac.uk](mailto:matt.jaquiery@psy.ox.ac.uk))"
output:
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '3'
    css: ../src/writeUp.css
    includes:
      after_body: ../src/toc_menu.html
  html_notebook:
    toc: yes
    toc_depth: 3
    css: ../src/writeUp.css
    includes:
      after_body: ../src/toc_menu.html
editor_options:
  chunk_output_type: inline
---

June 2020

[Script run `r Sys.time()`]


```{r prematter, include = F}

library(tidyverse)
library(ggridges)
library(scales)
library(broom)
library(BayesFactor)
library(prettyMD)
library(knitr)
library(parallel)

opts_chunk$set('echo' = F)

set.seed(20191216)

# Plot setup
theme_set(theme_light() + 
            theme(panel.grid.major.x = element_blank()))

```

```{r loadData, include = F}
# rDir <- "http://localhost/ExploringSocialMetacognition/data/public/"
testData <- F

# Some of the data from earlier versions were corrupted due to mismatched columns, so we can load that data from the raw files
studyVersion <- "0-0-3"
studyName <- "accuracyDates"

useRawTrialData <- T

source("src/01_Load-data.R")
preserveWorkspaceVars <- T

# The rest of the data should be okay to load directly
studyVersion <- "0-0-4"
useRawTrialData <- F

exclude <- list(
  maxTrialRT = 60000,    # trials take < 1 minute
  minTrials = 11,        # at least 11 trials completed
  minChangeRate = .1,    # some advice taken on 10%+ of trials
  custom = list(
      tenKeyTrials = 
        # Each participant should have all ten choice trials
        function(x) {
          x %>% 
            filter(advisorChoice == 1) %>%
            nrow() != 10
        }
    )
  ) 

skipLoadData <- F

source("src/02_Exclusions.R")

if (testData) {
  # randomise the confidence of test data
  source("src/sim-confidence-estimation.R")
  model <- simulateCE(
    AdvisedTrial,
    agentInsensitivitySD = 8,
    agentConfidence = 2,
    agentConfidenceSD = 4,
    agentEgoBias = .7, 
    agentEgoBiasSD = .2,
    agentEffectSize = .1,
    agentEffectSizeSD = .1
    )
  AdvisedTrial <- model$trials
  decisions <- byDecision(AdvisedTrial)
}

```

# Introduction

The results from the Dots task have been hard to reconcile with the results of the Dates task. We therefore attempt to replicate some basic results from the Dots task using the Dates task. In this case, we attempt to demonstrate that people prefer to hear advice from an advisor who agrees with them more often, even when both advisors are balanced for objective accuracy in the advice they provide.

## Preregistration

This study was preregistered on the [OSF](https://osf.io/5xpvq).

# Method

The experimental code is available on [GitHub](https://github.com/oxacclab/ExploringSocialMetacognition), and the experiment can be performed by visiting [https://acclab.psy.ox.ac.uk/~mj221/ESM/ACBin/agr.html?PROLIFIC_PID=WriteUp](https://acclab.psy.ox.ac.uk/~mj221/ESM/ACBin/agr.html?PROLIFIC_PID=WriteUp). 

# Results

## Exclusions

```{r exclusions}

if (testData) {
  print("NOTE: Testing (simulated) data!!!!")
}

tmp <- suppressWarnings(left_join(exclusions, okayIds, by = "pid"))

tmp$condition <- factor(tmp$condition, labels = c("¬FbLowFirst",
                                                  "¬FbHighFirst",
                                                  "FbLowFirst",
                                                  "FbHighFirst"))

table(tmp$excluded, tmp$condition)

# Check conditions
tmp <- left_join(AdvisedTrial, tmp) %>% filter(block == 2)

table(tmp$condition, tmp$advisor0idDescription, tmp$feedback)

tmp <- unique(tmp %>% select(pid, condition)) %>%
  mutate(FeedbackCondition = if_else(str_starts(condition, 'Fb'),
                                     "Feedback", "No Feedback"))

f <- function(x) case_when(x == 1 ~ "Choice Phase", 
                           T ~ "Familiarization")

AdvisedTrial <- left_join(AdvisedTrial, tmp, by = "pid") %>%
  mutate(Phase = f(advisorChoice))

decisions <- byDecision(AdvisedTrial)

```

Our final participant list consists of `r length(unique(AdvisedTrial$pid))` participants who completed an average of `r num2str(mean(aggregate(advisor0 ~ pid, AdvisedTrial, length)$advisor0))` trials each, of which an average of `r num2str(mean(aggregate(advisor0 ~ pid, AdvisedTrial[!AdvisedTrial$feedback, ], length)$advisor0))` had no feedback.

## Task performance

First we offer a characterisation of the task, to provide the reader with a sense of how the participants performed. 

### Decisions

Participants offered estimates of whether various events took place before or after a given year. The correct answers were always between 1900 and 2000. Participants answered by selecting a point on one of two confidence bars, with the choice of bar indicating the chosen answer. 

#### Correctness

```{r accuracy}

decisions %>%
  filter(Phase == "Familiarization") %>%
  peek(responseCorrect, decision) %>%
  num2str.tibble(isProportion = T, precision = 3)

decisions %>% 
  filter(Phase == "Familiarization",
         FeedbackCondition == "Feedback") %>% 
  peek(responseCorrect, decision) %>% 
  num2str.tibble(isProportion = T, precision = 3) %>%
  cbind(FeedbackCondition = "Feedback") %>%
  bind_rows(
    decisions %>% 
      filter(Phase == "Familiarization",
             FeedbackCondition == "No Feedback") %>% 
      peek(responseCorrect, decision) %>% 
      num2str.tibble(isProportion = T, precision = 3) %>%
      cbind(FeedbackCondition = "No Feedback")
  )

```

```{r accuracyGraph}

tmp <- decisions %>% 
  filter(Phase == "Familiarization") %>%
  group_by(pid, decision, Phase, FeedbackCondition) %>%
  do(correct = mean_cl_normal(.$responseCorrect)) %>%
  unnest(correct) %>%
  transmute(pid, decision, Phase, FeedbackCondition,
            pCorrect = y, 
            ciLow = ymin,
            ciHigh = ymax)

ggplot(tmp, 
       aes(x = decision, y = pCorrect)) +
  geom_violin(alpha = .15, colour = NA, aes(fill = decision)) +
  geom_boxplot(fill = "white", outlier.color = NA, aes(colour = decision),
               width = .25, size = 1.25) +
  geom_line(alpha = .25, aes(group = pid)) + 
  scale_y_continuous(limits = c(0, 1)) +
  labs(x = "decision", 
       y = "p(response correct)") +
  facet_grid(~FeedbackCondition + Phase)

```

### Timing

We can look at the response time - the difference between the time the response is opened and the time the response is received.  

```{r time}

decisions$rt <- decisions$responseTimeEstimate - decisions$timeResponseOpen

decisions %>% peek(rt, decision) %>% 
  mutate_if(is.numeric, round)

```

```{r timeGraph}

tmp <- decisions %>% 
  filter(Phase == "Familiarization") %>% 
  group_by(pid, decision, Phase) %>% 
  summarise(rt = mean(rt))

ggplot(tmp, 
       aes(x = decision, y = rt)) +
  geom_violin(alpha = .15, colour = NA, aes(fill = decision)) +
  geom_boxplot(fill = "white", outlier.color = NA, aes(colour = decision),
               width = .25, size = 1.25) +
  geom_line(alpha = .25, aes(group = pid)) + 
  labs(x = "decision", 
       y = "response time (ms)")  +
  facet_wrap(~Phase)

```

### Summary {.summary}

!TODO[performance summary]

## Metacognitive performance

### Confidence

Each answer bar allowed the participant to express their confidence in that answer by selecting a higher point on the bar for a higher confidence, in a range of 0-100% for each decision.

```{r confidence}

decisions %>% 
  filter(Phase == "Familiarization") %>%
  peek(responseConfidence, decision) %>%
  num2str.tibble(isProportion = T, precision = 1)

decisions %>% 
  filter(Phase == "Familiarization",
         FeedbackCondition == "Feedback") %>% 
  peek(responseConfidence, decision) %>% 
  num2str.tibble(isProportion = T, precision = 3) %>%
  cbind(FeedbackCondition = "Feedback") %>%
  bind_rows(
    decisions %>% 
      filter(Phase == "Familiarization",
             FeedbackCondition == "No Feedback") %>% 
      peek(responseConfidence, decision) %>% 
      num2str.tibble(isProportion = T, precision = 3) %>%
      cbind(FeedbackCondition = "No Feedback")
  )

```

```{r confidence graph}

tmp <- decisions %>% 
  filter(Phase == "Familiarization") %>%
  group_by(pid, decision, FeedbackCondition, Phase) %>%
  do(confidence = mean_cl_normal(.$responseConfidence)) %>%
  unnest(confidence) %>%
  transmute(pid, decision, FeedbackCondition, Phase,
            confidence = y, 
            ciLow = ymin,
            ciHigh = ymax)

ggplot(tmp, 
       aes(x = decision, y = confidence)) +
  geom_violin(alpha = .15, colour = NA, aes(fill = decision)) +
  geom_boxplot(fill = "white", outlier.color = NA, aes(colour = decision),
               width = .25, size = 1.25) +
  geom_line(alpha = .25, aes(group = pid)) + 
  scale_y_continuous(limits = c(0, 100)) +
  labs(x = "decision", 
       y = "confidence") +
  facet_grid(~FeedbackCondition + Phase)

```

### Relationship between confidence and p(correct)

Participants indicated the confidence in their decisions simultaneously with the decisions themselves. We can thus explore how well a participant's confidence signals their accuracy. We provide only a brief examination here where normalised responses are split into high and low confidence for each participant, and the accuracies of the two categories are compared. This is done separately for first and last decisions.

```{r confidence normalisation}

decisions <- decisions %>% 
  group_by(pid, decision) %>% 
  dplyr::mutate(zConf = scale(responseConfidence)) %>%
  ungroup() 

```

```{r confidence binary split}

tmp <- decisions %>% 
  mutate(highConf = zConf > 0) %>%
  filter(Phase == "Familiarization") %>%
  group_by(pid, decision, highConf, Phase, FeedbackCondition) %>%
  summarise(pCorrect = mean(responseCorrect)) %>%
  group_by(pid, decision) %>%
  spread(highConf, pCorrect) %>%
  mutate(pCorrectDiff = `TRUE` - `FALSE`)

tmp %>% 
  peek(pCorrectDiff, decision) %>% 
  num2str.tibble(isProportion = T, precision = 3)

print("p(correct|high confidence) - p(correct|low confidence) for initial estimates")
ttestBF(tmp$pCorrectDiff[tmp$decision == "first"])
print("p(correct|high confidence) - p(correct|low confidence) for final decisions")
ttestBF(tmp$pCorrectDiff[tmp$decision == "last"])

```

```{r confidence binary split plot}

ggplot(tmp, aes(x = decision, y = pCorrectDiff)) +
  geom_violin(alpha = .15, colour = NA, aes(fill = decision)) +
  geom_boxplot(fill = "white", outlier.color = NA, aes(colour = decision),
               width = .25, size = 1.25) +
  geom_line(alpha = .25, aes(group = pid)) + 
  labs(x = "decision", 
       y = "p(Correct | sure) - p(Correct | unsure)") +
  scale_y_continuous(limits = c(-1, 1)) +
  facet_grid(~Phase + FeedbackCondition)
 
```

### Summary {.summary}

!TODO[Metacognitive summary]

## Advisor performance

The high and low agreement advisors should agree at different rates but be approximately equivalently accurate. !TODO[accuracy analysis]

### Accuracy

```{r adviceAccuracy}

AdvisedTrial$advisor0accurate <- ifelse(
  is.na(AdvisedTrial[, paste0(advisorNames[1], ".accurate")]),
  pull(AdvisedTrial[, paste0(advisorNames[2], ".accurate")]),
  pull(AdvisedTrial[, paste0(advisorNames[1], ".accurate")])
)

AdvisedTrial$advisor0agree <- ifelse(
  is.na(AdvisedTrial[, paste0(advisorNames[1], ".agree")]),
  pull(AdvisedTrial[, paste0(advisorNames[2], ".agree")]),
  pull(AdvisedTrial[, paste0(advisorNames[1], ".agree")])
)

tmp <- AdvisedTrial %>% 
  filter(Phase == "Familiarization") %>%
  group_by(pid, advisor0idDescription, FeedbackCondition, Phase) %>%
  summarise(advisorAccuracy = mean(advisor0accurate),
            advisorAgree = mean(advisor0agree)) 

tmp %>% 
  group_by(advisor0idDescription, FeedbackCondition) %>%
  summarise(advisorAccuracy = mean(advisorAccuracy),
            advisorAgree = mean(advisorAgree)) %>%
  num2str.tibble(isProportion = T, precision = 3)

```

```{r adviceAccuracyGraph}

tmp %>% 
  dplyr::rename(advisor = advisor0idDescription,
                `p(correct)` = advisorAccuracy,
                `p(agree)` = advisorAgree) %>%
  gather("var", "value", `p(correct)`:`p(agree)`) %>%
  ggplot(aes(x = advisor, y = value)) +
  geom_violin(alpha = .15, colour = NA, aes(fill = advisor)) +
  geom_boxplot(fill = "white", outlier.color = NA, aes(colour = advisor),
               width = .25, size = 1.25) +
  geom_line(alpha = .25, aes(group = pid)) + 
  labs(x = "advisor", 
       y = "mean") +
  scale_y_continuous(limits = c(0, 1)) +
  scale_fill_discrete(h.start = 45) + 
  scale_colour_discrete(h.start = 45) +
  facet_grid(var ~ Phase + FeedbackCondition)
  
  
```

### Summary {.summary}

The advisors' performances were within the expected ranges for both the probability and extent of agreement with the participants and the objective accuracy of their advice. The advisors didn't differ substantially from one another in any of these aspects.

### Influence

The measure of influence is the extent to which a participant's confidence moves in the expected direction as a function of the advice (increasing when agreed with, decreasing when disagreed with).

Influence is the primary outcome measure, and is thus expected to differ between advisors and feedback conditions.

```{r influence}

AdvisedTrial$advisor0influence <- 
  unlist(sapply(1:nrow(AdvisedTrial),
                function(i) 
                  AdvisedTrial[i, 
                               paste0(AdvisedTrial$advisor0idDescription[i], 
                                      ".influence")]))

tmp <- AdvisedTrial %>% 
  filter(Phase == "Familiarization") %>%
  group_by(pid, advisor0idDescription, FeedbackCondition, Phase) %>%
  summarise(advisorInfluence = mean(advisor0influence)) 

tmp %>% 
  group_by(advisor0idDescription, FeedbackCondition) %>%
  summarise(advisorInfluence = mean(advisorInfluence)) %>%
  num2str.tibble(precision = 1)

```

```{r influenceGraph}

tmp %>% 
  rename(advisor = advisor0idDescription) %>%
  ggplot(aes(x = advisor, y = advisorInfluence)) +
  geom_violin(alpha = .15, colour = NA, aes(fill = advisor)) +
  geom_boxplot(fill = "white", outlier.color = NA, aes(colour = advisor),
               width = .25, size = 1.25) +
  geom_line(alpha = .25, aes(group = pid)) + 
  scale_y_continuous(limits = c(-100, 100)) +
  scale_fill_discrete(h.start = 45) + 
  scale_colour_discrete(h.start = 45) +
  facet_grid(~Phase + FeedbackCondition)

```

#### Influence distribution

It's good to keep a general eye on the distribution of weight-on-advice on a trial-by-trial basis. 

```{r influenceDistribution}

ggplot(AdvisedTrial, aes(advisor0influence)) + 
  geom_histogram(aes(fill = advisor0idDescription)) +
  scale_fill_discrete(h.start = 45) + 
  facet_grid(FeedbackCondition ~ advisor0idDescription)

```

#### Is initial accuracy or initial confidence a better predictor of influence?

```{r influence correlations}

tmp <- AdvisedTrial %>% 
  filter(Phase == "Familiarization") %>%
  select(pid, responseCorrect, responseConfidence, advisor0influence) %>%
  nest(-pid) %>%
  mutate(
    accBF = map(data, ~ correlationBF(.x$responseCorrect, 
                                    .x$advisor0influence)),
    acc.BF = map(accBF, ~ exp(.@bayesFactor$bf)),
    conBF = map(data, ~ correlationBF(.x$responseConfidence, 
                                    .x$advisor0influence)),
    con.BF = map(conBF, ~ exp(.@bayesFactor$bf)),
    acc = map(data, ~ cor.test(as.numeric(.x$responseCorrect), 
                               .x$advisor0influence)), 
    acc = map(acc, tidy),
    con = map(data, ~ cor.test(.x$responseConfidence, .x$advisor0influence)), 
    con = map(con, tidy)
  ) %>% 
  unnest(acc, .sep = ".") %>%
  unnest(con, .sep = ".") %>%
  unnest(acc.BF, con.BF)

tmp <- bind_rows(
  tmp %>% 
    select_at(vars(starts_with('acc'))) %>%
    rename_all(~ str_replace(., 'acc\\.(.+)', '\\1')) %>%
    mutate(property = 'accuracy'),
  tmp %>% 
    select_at(vars(starts_with('con'))) %>%
    rename_all(~ str_replace(., 'con\\.(.+)', '\\1')) %>%
    mutate(property = 'confidence')
)

tmp %>% 
  group_by(property) %>%
  summarise_if(is.numeric, mean, na.rm = T)

```

### Summary {.summary}

!TODO[Advisor summary]

## Hypothesis testing

The hypotheses being tested here are:  

1. Participants will select the high agreement advisor at a rate greater than chance when given feedback.

```{r h1, results='asis'}

tmp <- AdvisedTrial %>% 
  dplyr::filter(Phase == "Choice Phase") %>%
  nest(d = c(-pid, -advisor0idDescription)) %>%
  mutate(n = map_int(d, nrow)) %>% 
  mutate(
    d = NULL,
    advisor0idDescription = str_extract(advisor0idDescription, "high|low")
    ) %>%
  spread(advisor0idDescription, n) %>%
  mutate_if(is.numeric, ~ ifelse(is.na(.), 0, .)) %>%
  mutate(pPickHigh = high / (high + low)) %>%
  left_join(AdvisedTrial %>% select(pid, FeedbackCondition) %>% unique(), by = "pid")

tmp %>% 
  filter(FeedbackCondition == "Feedback") %>%
  pull(pPickHigh) %>%
  md.ttest(.5, labels = c("*M*|Feedback")) %>%
  cat()

cat('\n\n')

tmp %>% 
  filter(FeedbackCondition != "Feedback") %>%
  pull(pPickHigh) %>%
  md.ttest(.5, labels = c("*M*|¬Feedback")) %>%
  cat()

ggplot(tmp, aes(y = FeedbackCondition, x = pPickHigh)) +
  geom_density_ridges(aes(fill = FeedbackCondition)) +
  geom_point(position = position_jitter(0, .05), alpha = .25)

```

### Summary {.summary}

!TODO[hypothesis testing summary]

## Exploration

### Does Confidence-contingent agreement predict influence?

Niccolo's project was looking at whether agreement based on confidence could allow the discovery of objective accuracy differences where agreement differences did not provide a valid proxy measurement. 
Agreement was controlled in his studies, and in this one they point the opposite way for many participants (High agreement advisor is less objectively accurate). 

```{r}

# categorise participants based on which advisor was most accurate
tmp <- AdvisedTrial %>% 
  filter(Phase == 'Familiarization') %>% 
  group_by(pid) %>% 
  summarise(
    h = mean(highAgreement.accurate, na.rm = T),
    l = mean(lowAgreement.accurate, na.rm = T)
  ) %>%
  transmute(
    pid,
    mostAccurateAdvisor = if_else(h > l, "High agreement", "Low agreement"),
    accuracyDifference = h - l
  ) %>% left_join(AdvisedTrial, by = 'pid')

tmp <- tmp %>% 
  remove_all_labels() %>% 
  nest(d = -pid) %>%
  mutate(
    d = map(d, ~ arrange(., responseConfidence) %>% 
              mutate(
                influenceQuantile = 
                  sapply(responseConfidence, 
                         function(x) 
                           which(rle(.$responseConfidence)$values == x)
                         ),
                confCat = case_when(
                  influenceQuantile > max(influenceQuantile) * .7 ~ 'high',
                  influenceQuantile < max(influenceQuantile) * .3 ~ 'low',
                  T ~ 'medium'
                )
              ))
    ) %>%
  unnest(d) %>% 
  select(pid, influenceQuantile, confCat, accuracyDifference, mostAccurateAdvisor, 
         ends_with('influence'), ends_with('agreement'), FeedbackCondition,
         advisor0idDescription, advisor0agree, advisor0accurate) 

for (f in unique(tmp$FeedbackCondition)) {
  print(
    tmp %>% 
      filter(FeedbackCondition == f) %>%
      nest(d = -pid) %>%
      slice(sample(1:nrow(.), 20)) %>%
      unnest_legacy(d) %>%
      ggplot(aes(x = advisor0influence, y = pid, fill = confCat, colour = confCat)) +
      geom_vline(xintercept = 0, linetype = 'dashed') +
      geom_density_ridges(alpha = 1/3, colour = 'black') +
      geom_point(position = position_jitter(0, .1), alpha = .75) +
      facet_grid(advisor0agree ~ advisor0idDescription + FeedbackCondition,
                 labeller = label_both)
  )
}


```

### Confidence change

```{r}

AdvisedTrial %>%
  filter(Phase == 'Familiarization') %>% 
  mutate(confDiff = case_when(
    responseAnswerSide == responseAnswerSideFinal ~ 
      responseConfidenceFinal - responseConfidence,
    T ~ 
      -(responseConfidenceFinal + responseConfidence)
    )) %>%
  ggplot(aes(x = confDiff, y = pid, fill = advisor0agree)) +
  geom_vline(xintercept = 0, linetype = 'dashed') +
  geom_density_ridges(alpha = .5) + 
  facet_grid(FeedbackCondition ~ Phase + advisor0idDescription)

```

### Influence x Pick rate

We want to look at the relationship between the influence difference of advisors in the Familiarization phase and the pick rates in the Choice phase. 

```{r}

tmp <- AdvisedTrial %>% 
  filter(Phase == "Familiarization") %>%
  group_by(pid, advisor0idDescription) %>%
  summarise(influence = mean(advisor0influence, na.rm = T))

tmp <- tmp %>% left_join(pickRates, by = 'pid') %>%
  pivot_wider(names_from = advisor0idDescription, values_from = influence, names_prefix = "influence.")

tmp <- tmp %>% 
  mutate(influenceDifference = influence.highAgreement - influence.lowAgreement) %>%
  select(pid, pPickHigh, FeedbackCondition, influenceDifference)

r <- tmp %>% 
  nest(d = -FeedbackCondition) %>%
  mutate(
    r = map(d, ~ cor.test(~ pPickHigh + influenceDifference, data = .)),
    r = map(r, tidy)
  ) %>% 
  unnest(r) %>%
  select(-d) %>%
  mutate(
    text = paste0('r ', lteq(prop2str(estimate, minPrefix = '')),
                  ' [95%CI ', prop2str(conf.low, minPrefix = ''), 
                  ', ', prop2str(conf.high, minPrefix = ''), 
                  ']\np ', lteq(prop2str(p.value, minPrefix = ''))) 
  )

ggplot(tmp, aes(x = pPickHigh, y = influenceDifference, 
                fill = FeedbackCondition, colour = FeedbackCondition)) +
  geom_hline(yintercept = 0, linetype = 'dashed') +
  geom_vline(xintercept = .5, linetype = 'dashed') +
  geom_smooth(method = 'lm') + 
  geom_point(alpha = .5, colour = 'black') +
  scale_x_continuous(limits = c(0, 1)) +
  geom_label(aes(x = .5, y = max(tmp$influenceDifference) * 1.4, label = text),
            data = r, fill = "white", colour = "black") +
  facet_wrap(~ FeedbackCondition)

```

### Questionnaires

#### Advisor questionnaires

Somehow I removed the individual advisor questionnaires during development and never put them back. I do not know why I did this stupid thing, but we don't have individual advisor questionnaire data to look at for these studies.

#### General feedback

```{r}

debrief.form %>% 
  arrange(desc(nchar(comment))) %>%
  select(pid, comment)

```

# Conclusions {.summary}

!TODO[Conclusions]

# Credits 

## Acknowledgements

Thanks as always to Nick Yeung and the other folks at the [ACC Lab](https://www.psy.ox.ac.uk/research/attention-cognitive-control-lab).

## R Packages

```{r results = 'asis'}
# list packages
packageNames <- (.packages())
# don't include very core package
packageNames <- packageNames[!(packageNames %in% 
                                 rownames(installed.packages(
                                   priority = "base")))]
# but do include the base package
packageNames <- c("base", packageNames)
out <- NULL
for (p in packageNames) {
  out <- rbind(out, data.frame('Package' = p, 
                               'Citations' = paste(format(citation(p), 
                                                          style = 'textVersion'), 
                                                   collapse = '<br/><br/>')))
}

kable(out)
```

## Funding

Matt Jaquiery is funded by a studentship from the [Medical Research Council](https://mrc.ukri.org/) (reference 1943590) and the University of Oxford [Department of Experimental Psychology](https://www.psy.ox.ac.uk/) (reference 17/18_MSD_661552).

## Technical details  

```{r results = 'hold'}
cat(paste('Time stamp:', Sys.time(), '\n\n'))
cat('Runtime \n')
proc.time()
cat('\n')
sessionInfo()
```