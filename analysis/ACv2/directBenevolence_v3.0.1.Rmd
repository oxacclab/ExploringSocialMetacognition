---
title: "Direct benevolence manipulation analysis"
author: "Matt Jaquiery (matt.jaquiery@psy.ox.ac.uk)"
output:
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '3'
    css: ../src/writeUp.css
    includes:
      after_body: ../src/toc_menu.html
  html_notebook:
    toc: yes
    toc_depth: 3
    css: ../src/writeUp.css
    includes:
      after_body: ../src/toc_menu.html
editor_options:
  chunk_output_type: inline
---

July 2019

[Script run `r Sys.time()`]


```{r prematter, include = F}

library(testthat)

library(tidyverse)
library(broom)

library(BayesFactor)
library(BayesMed)

library(prettyMD)

library(ez)

library(knitr)

opts_chunk$set('echo' = F)

set.seed(20190723)

# Plot setup
theme_set(theme_light() + 
            theme(panel.grid.major.x = element_blank()))

```

```{r loadData, include = F}

studyVersion <- "3-0-1"
studyName <- "directBenevolence"

overrideMarkerList <- c(11) # Someone managed to get marker width 8 for one question, not sure how

exclude <- list(
  maxAttnCheckFails = 0, # pass all attn checks
  maxTrialRT = 60000,    # trials take < 1 minute
  minTrials = 11,        # at least 11 trials completed
  minChangeRate = .1,    # some advice taken on 10%+ of trials
  qqLableWhitelist = c(  # Advice questionnaire responses must be one of:
    'Deceptive',
    'Possibly Deceptive',
    'Honest'
  ),
  multipleAttempts = T   # exclude multiple attempts
  ) 

skipLoadData <- F

source("src/02_Exclusions.R")

# Re-order factor levels for advice ratings
AdvisedTrial <- AdvisedTrial %>%
  mutate(advisor0questionnaireHonestyLabel = 
           factor(advisor0questionnaireHonestyLabel, 
                  levels = 
                    levels(advisor0questionnaireHonestyLabel)[c(1, 3, 2)]))

```

# Introduction

Observations from [evolutionary models](https://github.com/oxacclab/EvoEgoBias) show that egocentric discounting is a successful strategy in environments where advisors sometimes provide deliberately poor advice. We reason that human participants may show a sensitivity to these contextual factors underlying advice-taking and respond to them in a rational manner. To test the effects of potentially misleading advice, we used a direct benevolence manipulation to pair participant judges in a judge-advisor system with two advisors, one who they were told would consistently try to help them ('in-group advisor'), and one who they were told may occasionally try to mislead them ('out-group advisor').

We hypothesised that participants would place greater weight on the advice of the in-group vs out-group advisor, and that this relationship would be mediated by responses to questionnaires asking about the perceived benevolence of the advisor.

**Version 2.0.0 introduced a much clearer manipulation** by reminding participants of the properties of the advisor (whether they'd always be helpful) in a message the participant had to acknowledge, by keeping the participant's group visible throughout, and by including one trial on which the outgroup advisor did actually offer misleading advice.

**Version 2.1.0** removed the difference in advice and adjusted the to be consistent ('may' rather than 'will' sometime mislead...). 

**Version 3.0.0** adds a question after seeing advice but before giving a final reponse where participants rate the deceptiveness/honesty of the advice. We can examine how the weight on advice differs by both _advice_ and _advisor_. Will participants place less weight on all advice from the outGroup advisor, or only on those trials where they consider the advice deceptive?

**Version 3.0.1** is a [pre-registered](https://osf.io/tu3ev) replication of V3.0.0. Differences from the v3.0.1 script:

* We add a new exclusion rule for those people who use translation software. 

* Added exclusions for participants with NA values in the in- vs out-group t-test (where e.g. no outgroup advice was rated as 'honest').

* Included frequentist stats in the trustworthiness questionnaire item t-test.

* Fixed some labels on graphs

* Exploratory analyses expanded to include analysis of trials with WoA > .05

# Method

The experimental code is available on [GitHub](https://github.com/oxacclab/ExploringSocialMetacognition), and the experiement can be performed by visiting [https://acclab.psy.ox.ac.uk/~mj221/ESM/ACv2/db.html](https://acclab.psy.ox.ac.uk/~mj221/ESM/ACv2/db.html?PROLIFIC_PID=WriteUp). 

# Results

## Exclusions

```{r exclusions}

tmp <- suppressWarnings(left_join(exclusions, okayIds, by = "pid"))

tmp$condition <- factor(tmp$condition, labels = c("InFirst",
                                                  "OutFirst"))

table(tmp$excluded, tmp$condition)

```

Our final participant list consists of `r length(unique(AdvisedTrial$pid))` participants who completed an average of `r num2str(mean(aggregate(advisor0 ~ pid, AdvisedTrial, length)$advisor0))` trials each.

## Task performance

```{r bindAdvisors}

# bind feedback property from participants

# convert pid columns to character to allow joining
tmp <- PP
tmp$pid <- as.character(tmp$pid)

advisors$pid <- as.character(advisors$pid)

advisors <- advisors[advisors$pid %in% tmp$pid, ]
advisors <- left_join(advisors, unique(tmp[c("pid", "feedback")]), "pid")

# drop practice advisors
advisors <- advisors[advisors$idDescription != "Practice", ]

```

First we offer a characterisation of the task, to provide the reader with a sense of how the participants performed. 

### Decisions

Participants offered estimates of the year in which various events took place. The correct answers were always between 1900 and 2000, although the timeline on which participants responded went from 1890 to 2010 in order to allow extra room for advice. Participants answered by dragging a marker onto a timeline which covered a range of 11 years (e.g. 1940-1950). Participants were informed that a correct answer was one in which the marker covered the year in which the event took place.

#### Correctness

Responses are regarded as **correct** if the target year is included within the marker range.

```{r accuracy}

tmp <- markerBreakdown(responseCorrect, decisions, hideTotals = T)
tmp <- rbind(tmp$first, tmp$last)[, c(1, 3)]

num2str.tibble(tmp, isProportion = T, precision = 3)

```

```{r accuracyGraph}

ggplot(aggregate(responseCorrect ~ 
                   responseMarker + decision + pid,
                 decisions, mean), 
       aes(x = decision, y = responseCorrect)) +
  geom_violin(alpha = .25, colour = NA, fill = "grey75") +
  geom_boxplot(fill = NA, outlier.color = NA) +
  geom_line(alpha = .5, aes(colour = pid, group = pid)) + 
  geom_point(alpha = .5, aes(colour = pid)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  scale_linetype_manual(values = c("dashed")) +
  scale_color_discrete(guide = 'none') + 
  labs(x = "decision", 
       y = "p(response correct)")

```

#### Error (estimate mean)

The **error** is calcualted as the distance from the centre of the answer marker to the correct year. It is thus possible for **correct** answers to have non-zero error, and it is likely that the error for correct answers scales with the marker size.

```{r err}

tmp <- markerBreakdown(responseError, decisions, hideTotals = T)
tmp <- rbind(tmp$first, tmp$last)[, c(1, 3)]

num2str.tibble(tmp, isProportion = T, precision = 3)

```

```{r errGraph}

ggplot(aggregate(responseError ~ 
                   responseMarker + decision + pid,
                 decisions, mean), 
       aes(x = decision, y = responseError)) +
  geom_violin(alpha = .25, colour = NA, fill = "grey75") +
  geom_boxplot(fill = NA, outlier.color = NA) +
  geom_line(alpha = .5, aes(colour = pid, group = pid)) + 
  geom_point(alpha = .5, aes(colour = pid)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  scale_linetype_manual(values = c("dashed")) +
  scale_color_discrete(guide = 'none') + 
  labs(x = "decision", 
       y = "|target - response marker centre| (years)")

```

### Timing

We can look at the response time - the difference between the time the response is opened and the time the response is received.  

```{r time}

decisions$rt <- decisions$responseTimeEstimate - decisions$timeResponseOpen

tmp <- markerBreakdown(rt, decisions, hideTotals = T)
tmp <- rbind(tmp$first, tmp$last)[, c(1, 3)]

num2str.tibble(tmp, isProportion = T, precision = 3)

```

```{r timeGraph}

ggplot(aggregate(rt ~ 
                   responseMarker + decision + pid,
                 decisions, mean), 
       aes(x = decision, 
           y = rt / 1000)) +
  geom_violin(alpha = .25, colour = NA, fill = "grey75") +
  geom_boxplot(fill = NA, outlier.color = NA) +
  geom_line(alpha = .5, aes(colour = pid, group = pid)) + 
  geom_point(alpha = .5, aes(colour = pid)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  scale_linetype_manual(values = c("dashed")) +
  scale_color_discrete(guide = 'none') + 
  labs(x = "decision", 
       y = "response time (s)")

```

### Summary {.summary}

Participants took longer on their final decisions (partly due to answering the questionnaire) and improved their accuracy. This suggests that they took the time to weigh up the advice, and that they took it into account when making their (final) decisions.

## Advisor performance

The advice offered should be equivalent between in/out group advisors.

### Accuracy

```{r adviceAccuracy}

tmp <- NULL
for (a in advisorNames) {
  eq <- as.formula(paste0("cbind(", a, ".accurate, ", 
                          a, ".error) ~ pid"))
  r <- aggregate(eq, AdvisedTrial, mean, na.rm = T)
  
  colnames(r) <- c("pid", "accuracy", "error")
  r$advisor = a
  
  tmp <- rbind(tmp, as.tibble(r))
}

prop2str(as.tibble(aggregate(cbind(accuracy, error) ~ advisor, 
                             tmp, 
                             mean, na.rm = T)), 
         precision = 3)

```

```{r adviceAccuracyGraph}

tmp <- gather(tmp, key = "var", value = "value", c("accuracy", "error"))

for (v in unique(tmp$var)) 
  print(
    ggplot(tmp[tmp$var == v, ], aes(x = advisor, y = value, colour = pid)) +
      geom_violin(colour = NA, fill = "grey75", alpha = .25) +
      geom_boxplot(outlier.colour = NA, fill = NA, aes(group = advisor)) +
      geom_line(alpha = .5, aes(colour = pid, group = pid)) + 
      geom_point(alpha = .5, aes(colour = pid)) +
      stat_summary(geom = "line", fun.y = mean,
                   aes(group = 1, linetype = "mean"), size = 1.5) +
      labs(y = v) +
      scale_color_discrete(guide = 'none')
  )

```

### Agreement

```{r adviceAgreement}

tmp <- NULL
for (a in advisorNames) {
  eq <- as.formula(paste0(a, ".agree ~ pid"))
  r <- aggregate(eq, AdvisedTrial, mean, na.rm = T)
  
  colnames(r) <- c("pid", "agreement")
  r$advisor <- a
  tmp <- rbind(tmp, r)
}

prop2str(as.tibble(aggregate(agreement ~ advisor, 
                             tmp, mean, na.rm = T)), 
         precision = 3)

```

```{r adviceAgreementGraph}

ggplot(tmp, aes(x = advisor, y = agreement, colour = pid)) +
  geom_violin(colour = NA, fill = "grey75", alpha = .25) +
  geom_boxplot(outlier.colour = NA, fill = NA, aes(group = advisor)) +
  geom_line(alpha = .5, aes(colour = pid, group = pid)) + 
  geom_point(alpha = .5, aes(colour = pid)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  scale_color_discrete(guide = 'none')

```

### Distance

Distance is the continuous version of agreement - the difference between the centre of the advice and the centre of the initial estimate. 

```{r adviceDistance}

tmp <- NULL
for (a in advisorNames) {
  eq <- as.formula(paste0(a, ".distance ~ pid"))
  r <- aggregate(eq, AdvisedTrial, mean, na.rm = T)
  
  colnames(r) <- c("pid", "distance")
  r$advisor = a
  
  tmp <- rbind(tmp, as.tibble(r))
}

prop2str(as.tibble(aggregate(distance ~ advisor, 
                             tmp, 
                             mean, na.rm = T)), 
         precision = 3)

```

```{r adviceDistanceGraph}

ggplot(tmp, aes(x = advisor, y = distance, colour = pid)) +
  geom_violin(colour = NA, fill = "grey75", alpha = .25) +
  geom_boxplot(outlier.colour = NA, fill = NA, aes(group = advisor)) +
  geom_line(alpha = .5, aes(colour = pid, group = pid)) + 
  geom_point(alpha = .5, aes(colour = pid)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  scale_color_discrete(guide = 'none')

```

### Summary {.summary}

The advisors were similar in terms of their objective accuracy and their tendency to offer advice close to the participants' initial decisions. 

### Influence

The measure of influence is weight-on-advice. This is well-defined for values between 0 and 1 (trucated otherwise), and is
$$\text{WoA} = (\text{final} - \text{inital}) / (\text{advice} - \text{initial})$$
, or the degree to which the final decision moves towards the advised answer.

Influence is the primary outcome measure, and is thus expected to differ between advisors and feedback conditions.

```{r woa}

tmp <- NULL
for (a in advisorNames) {
  eq <- as.formula(paste0(a, ".woa ~ pid"))
  r <- aggregate(eq, AdvisedTrial, mean, na.rm = T)
  
  colnames(r) <- c("pid", "WoA")
  r$advisor <- a
  tmp <- rbind(tmp, r)
}

prop2str(as.tibble(aggregate(WoA ~ advisor, tmp, mean, na.rm = T)), 
         precision = 3)

```

```{r woaGraph}

ggplot(tmp, aes(x = advisor, y = WoA, colour = pid)) +
  geom_violin(colour = NA, fill = "grey75", alpha = .25) +
  geom_boxplot(outlier.colour = NA, fill = NA, aes(group = advisor)) +
  geom_line(alpha = .5, aes(colour = pid, group = pid)) + 
  geom_point(alpha = .5, aes(colour = pid)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  scale_y_continuous(limits = c(0, 1)) +
  scale_color_discrete(guide = 'none')

```

##### WoA distribution

It's good to keep a general eye on the distribution of weight-on-advice on a trial-by-trial basis. 

```{r woaDistribution}

ggplot(AdvisedTrial, aes(woa)) + 
  geom_histogram(stat = "count") +
  facet_grid(feedback ~ advisor0idDescription, labeller = label_both)

```

```{r percentage of participants with expected pattern}

tmp <- AdvisedTrial %>%
  group_by(pid, advisor0idDescription) %>%
  summarise(woa = mean(advisor0woa)) %>%
  spread(advisor0idDescription, woa) %>%
  dplyr::filter_all(all_vars(!is.na(.))) %>%
  summarise(asExpected = inGroup > outGroup)

```

### Summary {.summary}

Weight on Advice scores differ substantially between the advisors, as do the distributions. While the distributions vary between participants, most (`r round(mean(tmp$asExpected) * 100, 2)`%) participants show a preference for the inGroup advisor. Formal testing of the hypothesis is below.

#### WoA by Advice

We also want to know if weight on advice differs as a function of how participants rate the advice itself.

```{r woa by advice}

byAdvice <- AdvisedTrial %>%
  group_by(pid, advisor0idDescription, advisor0questionnaireHonestyLabel) %>%
  summarise(woa = mean(advisor0woa),
            n = n()) 

dw <- 1

# Plot ns
AdvisedTrial %>%
  group_by(pid,
           advisor0idDescription,
           advisor0questionnaireHonestyLabel) %>%
  summarise(nTrials = n()) %>%
  # CORRECT for missing (0 count) categories
  spread(advisor0questionnaireHonestyLabel, nTrials) %>%
  mutate_at(vars(-group_cols()), ~ ifelse(is.na(.), 0, .)) %>%
  gather('advisor0questionnaireHonestyLabel', 'nTrials', Deceptive:Honest) %>%
  mutate(advisor0questionnaireHonestyLabel = 
           factor(advisor0questionnaireHonestyLabel)) %>%
  mutate(
    advisor0questionnaireHonestyLabel = 
           factor(advisor0questionnaireHonestyLabel, 
                  levels = 
                    levels(advisor0questionnaireHonestyLabel)[c(1, 3, 2)])) %>%
  # PLOT
  ggplot(aes(x = advisor0questionnaireHonestyLabel, y = nTrials)) +
  geom_point(aes(group = pid),
             position = position_jitter(width = .2, height = 0), 
             alpha = .25) + 
  stat_summary(geom = 'line',
               aes(group = 1),
               fun.y = mean,
               size = 1,
               position = position_dodge(dw)) +
  stat_summary(geom = 'errorbar',
               fun.data = mean_cl_normal,
               width = 0,
               size = 1,
               position = position_dodge(dw)) + 
  guides(colour = 'none') +
  scale_y_continuous(limits = c(0, NA)) +
  facet_wrap(~advisor0idDescription)

# Plot WoA
byAdvice %>% ggplot(aes(x = advisor0questionnaireHonestyLabel,
                        y = woa, colour = pid)) +
  geom_violin(colour = NA, fill = "grey75", alpha = .25) +
  geom_boxplot(outlier.colour = NA, fill = NA, aes(group = advisor0questionnaireHonestyLabel)) +
  geom_line(alpha = .5, aes(colour = pid, group = pid)) + 
  geom_point(alpha = .5, aes(colour = pid)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  scale_y_continuous(limits = c(0, 1)) +
  scale_color_discrete(guide = 'none') +
  facet_wrap(~advisor0idDescription) +
  labs(x = 'Advice Honesty Rating',
       y = 'Weight on Advice')

```

#### Summary {.summary}

Firstly, participants appear to rate advice from the outgroup advisor as less honest than advice from the ingroup advisor. Secondly, we see that WoA for both advisors increases as the rating of the advice honesty increases. There appears to be a general discounting effect whereby the outgroup advisors are less trusted than the ingroup advisors, even where the advice is rated equally. We test this formally below in terms of advice rated as 'honest'. Furthermore, it's possible there is an interaction effect because the increase in weight on advice may be steeper as advice trustworthiness rises for the ingroup advisor. The statistical tests will disambiguate this.

## Questionnaire data

```{r questionnaires}

tmp <- debrief.advisors[debrief.advisors$pid %in% AdvisedTrial$pid, ]
tmp$sameGroup <- NA

for (i in seq(nrow(tmp))) {
  tmp$sameGroup[i] <- as.character(
    advisors$idDescription[advisors$pid == tmp$pid[i] &
                             advisors$id == tmp$advisorId[i]])
}

tmp <- gather(tmp, key = "Q", value = "rating", 
              c("knowledge", "helpfulness", "consistency", "trustworthiness"))

ggplot(tmp, aes(x = sameGroup, y = rating)) +
  geom_violin(colour = NA, fill = "grey75", alpha = .25) +
  geom_boxplot(outlier.colour = NA, fill = NA, aes(group = sameGroup)) +
  geom_line(alpha = .5, aes(colour = pid, group = pid)) + 
  geom_point(alpha = .5, aes(colour = pid)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  scale_y_continuous(limits = c(0, 100)) +
  scale_color_discrete(guide = 'none') +
  facet_wrap(. ~ Q, labeller = label_both) +
  labs(x = '', y = 'Rating')

```

### Summary {.summary} 

Across most questions there is a preference for the ingroup vs outgroup advisor overall. Participants did not appear to rate the knowledge of the advisors differently, whereas consistency, helpfulness, and trustworthiness were all rated in favour of the ingroup advisor. Given the manipulation was on the helpfulness of the advisors rather than their expertise, this pattern suggests participants understood the task. Several participants demonstrate higher ratings in some dimensions for the outgroup advisor.

### WoA x Trustworthiness rating

Participants' trustworthiness ratings should reflect their behavioural measure (weight on advice), such that the extent of advisor preference in one measure should be correlated with the other. 

```{r woa trustworthiness correlation}

tmp <- list()
tmp[['woa']] <- AdvisedTrial %>% 
  group_by(pid, advisor0idDescription) %>%
  summarise(woa = mean(advisor0woa)) %>%
  spread(advisor0idDescription, woa) %>%
  mutate(inGroupWoaPref = inGroup - outGroup,
         inGroup = NULL, outGroup = NULL)

tmp[['trust']] <- AdvisedTrial %>% 
  left_join(debrief.advisors, by = c('pid', 'advisor0id' = 'advisorId')) %>%
  group_by(pid, advisor0idDescription) %>%
  summarise(trust = mean(trustworthiness)) %>%
  spread(advisor0idDescription, trust) %>%
  summarise(inGroupTrustPref = inGroup - outGroup)

tmp <- tmp[['woa']] %>% left_join(tmp[['trust']], by = 'pid')

test <- cor.test(tmp$inGroupWoaPref, tmp$inGroupTrustPref)

```

```{r woa trustworthiness graph}

tmp %>% ggplot(aes(x = inGroupWoaPref, y = inGroupTrustPref, colour = pid)) +
  geom_rect(ymin = -Inf, ymax = Inf, xmin = -Inf, xmax = 0, 
            fill = 'grey75', colour = NA) +
  geom_rect(ymin = -Inf, ymax = 0, xmin = -Inf, xmax = Inf, 
            fill = 'grey75', colour = NA) +
  geom_rect(ymin = 0, ymax = Inf, xmin = 0, xmax = Inf, 
            fill = 'white', colour = NA) +
  geom_rect(ymin = -Inf, ymax = 0, xmin = -Inf, xmax = 0, 
            fill = 'grey35', colour = NA) +
  geom_abline(slope = 100, intercept = 0) + 
  geom_point() +
  geom_smooth(aes(group = 1), method = 'lm', se = F, fullrange = T,
              colour = 'black', linetype = 'dashed') + 
  scale_x_continuous(limits = c(-1, 1)) + 
  scale_y_continuous(limits = c(-100, 100)) +
  scale_color_discrete(guide = 'none') +
  coord_fixed(.01) +
  labs(title = 'Weight on Advice difference X Trustworthiness difference',
       x = 'inGroup WoA preference',
       y = 'inGroup trustworthiness preference',
       caption = paste('Correlation: r = ', round(test$estimate, 3), 
                       '; p = ', round(test$p.value, 4)))

```

### Summary {.summary}

A clear, significant correlation in the expected direct exists between the subjective and objective measures of trust difference: participants who place greater weight on the ingroup advisor's advice relative to the outgroup advisor's advice were also more likely to give higher relative scores on the questionnaires to the ingroup advisor. 

## Hypothesis testing

The hypotheses being tested here are:  

1. Participants will place higher weight on the advice of in-group advisors, which we will test for advice rated as honest.

2. Participants will place greater weight on advice on honest vs deceptive advice.

### 1. Group differences

Participants have different weight-on-advice for ingroup as opposed to outgroup advisors.

```{r h1, results='asis'}

tmp <- AdvisedTrial %>%
  dplyr::filter(advisor0questionnaireHonestyLabel == 'Honest') %>%
  group_by(pid, advisor0idDescription) %>%
  summarise(woa = mean(advisor0woa)) %>%
  spread(advisor0idDescription, woa) %>%
  dplyr::filter_all(all_vars(!is.na(.)))
  

r <- md.ttest(tmp$inGroup, tmp$outGroup, 
              c("*M*|inGroup", "*M*|outGroup"), paired = T)
cat(r)

```

### Summary {.summary}

The Bayes factor indicates there is evidence to support the claim that the participants place greater weight on the advice of inGroup advisor versus the outGroup advisor.

### Questionnaire measure

We perform the same comparison for the 'trustworthiness' questionnaire item:

```{r h1 questionnaire, results='asis'}

tmp <- debrief.advisors %>% 
  left_join(AdvisedTrial, by = c('pid', 'advisorId' = 'advisor0id')) %>%
  dplyr::select(pid, advisor0idDescription, trustworthiness) %>%
  unique() %>%
  filter_all(all_vars(!is.na(.))) %>%
  spread(advisor0idDescription, trustworthiness)

r <- md.ttest(tmp$inGroup, tmp$outGroup, 
                c("*M*|inGroup", "*M*|outGroup"), paired = T)
cat(r)

```

### Summary {.summary}

There is insufficient evidence to support the claim that participants give higher trustworthiness ratings to the inGroup versus outGroup advisor. What evidence there is suggests that there are no systematic differences.

### 2. Effect of advice rating

Participants should place greater weight on advice they rate as honest vs deceptive. 

```{r h2 woa by advice test, results='asis'}

tmp <- AdvisedTrial %>%
  dplyr::filter(advisor0questionnaireHonestyLabel %in% 
                  c('Honest', 'Deceptive')) %>%
  group_by(
    pid,
    advisor0questionnaireHonestyLabel
  ) %>%
  summarise(woa = mean(advisor0woa, na.rm = T)) %>%
  # strip missing cases
  spread(advisor0questionnaireHonestyLabel, woa) %>%
  dplyr::filter_all(all_vars(!is.na(.)))

r <- md.ttest(tmp$Deceptive, tmp$Honest, 
              c('*M*|deceptive', '*M*|honest'),
              paired = T)
  
cat(r)

```

### Summary {.summary}

There is strong evidence that participants place more weight on advice they consider honest rather than deceptive.

## Exploration

### Advisor differences for other advice types

```{r woa by advisor, results='asis'}

for (q in unique(AdvisedTrial$advisor0questionnaireHonestyLabel)) {
  tmp <- AdvisedTrial %>%
    dplyr::filter(advisor0questionnaireHonestyLabel == q) %>%
    group_by(pid, advisor0idDescription) %>%
    summarise(woa = mean(advisor0woa)) 
  
  # unpaired
  r <- md.ttest(tmp$woa[tmp$advisor0idDescription == 'inGroup'], 
                tmp$woa[tmp$advisor0idDescription == 'outGroup'], 
                c('*M*|inGroup', '*M*|outGroup'))
  cat(paste0(q, ' (unpaired): ', r, '\n'))
  
  # paired (strip pp with missing cells)
  tmp <- tmp  %>%
    spread(advisor0idDescription, woa) %>% 
    dplyr::filter_all(all_vars(!is.na(.)))
    
  r <- md.ttest(tmp$inGroup, tmp$outGroup, 
                c("*M*|inGroup", "*M*|outGroup"), paired = T)
  cat(paste0(q, ' (paired): ', r, '\n\n'))
}


```

### Unpaired advice contrasts by rating

```{r woa by advice tests, results='asis'}

x <- unique(AdvisedTrial$advisor0questionnaireHonestyLabel)

for (contrast in list(x[c(1,2)], x[c(1,3)], x[c(2,3)])) {
  tmp <- AdvisedTrial %>%
    dplyr::filter(advisor0questionnaireHonestyLabel %in% contrast) %>%
    group_by(
      pid,
      advisor0questionnaireHonestyLabel
    ) %>%
    summarise(woa = mean(advisor0woa, na.rm = T)) 
  
  r <- md.ttest(tmp$woa[tmp$advisor0questionnaireHonestyLabel == contrast[1]], 
                tmp$woa[tmp$advisor0questionnaireHonestyLabel == contrast[2]], 
                paste0('*M*|', contrast))
    
  cat(paste0('(unpaired) ', r, '\n'))
  
  tmp <- tmp %>%
    # strip missing cases
    spread(advisor0questionnaireHonestyLabel, woa) %>%
    dplyr::filter_all(all_vars(!is.na(.)))
  
  r <- md.ttest(tmp[[2]], tmp[[3]], 
                paste0('*M*|', contrast),
                paired = T)
    
  cat(paste0('(paired) ', r, '\n\n'))
}


```

### Mean distance of advice by rating and advisor

If participants are more vigilant when receiving advice from the outgroup advisor, they may classify the advice as deceptive more readily. 

```{r advice distance by rating and advisor}

tmp <- AdvisedTrial %>%
  group_by(pid, advisor0idDescription, advisor0questionnaireHonestyLabel) %>%
  summarise(distance = mean(if_else(is.na(inGroup.distance), 
                                    outGroup.distance, 
                                    inGroup.distance)))

ggplot(tmp, aes(x = advisor0idDescription, y = distance,
                fill = advisor0questionnaireHonestyLabel)) +
  geom_point(position = position_jitterdodge(jitter.width = .1), alpha = .25) +
  geom_boxplot(alpha = .5, outlier.color = NA)
  

```

#### Summary {.summary}

Distance does seem to be related to the probability of classifying advice as deceptive/honest. It's possible, but not overwhelmingly clear, that the ingroup advisor's answers can have higher distances than the outgroup's before being classified non-honest. 

### WoA where advice is taken

For parity with previous versions we can look at weight on advice for trials where advice is taken: does it still differ between the advisors?

```{r woa for taken advice}

AdvisedTrial %>%
  group_by(pid, advisor0idDescription) %>%
  summarise(adviceIgnored = mean(advisor0woa <= .05)) %>%
  ungroup() %>% group_by(advisor0idDescription) %>%
  summarise(
    p.adviceIgnored_m = mean(adviceIgnored),
    p.adviceIgnored_sd = sd(adviceIgnored)
  )

tmp <- AdvisedTrial %>%
  dplyr::filter(advisor0woa > .05) %>%
  group_by(pid, advisor0idDescription) %>%
  summarise(WoA = mean(advisor0woa)) 

tmp <- tmp %>%
  ungroup() %>%
  group_by(pid, advisor0idDescription) %>%
  summarise(WoA = mean(WoA)) %>%
  spread(advisor0idDescription, WoA) %>% 
  dplyr::filter_all(all_vars(!is.na(.)))
  

r <- md.ttest(tmp$inGroup, tmp$outGroup, 
              labels = c('inGroup', 'outGroup'),
              paired = T)

cat(r)

```

```{r woaGraph for taken advice}

tmp %>%
  gather('advisor', 'WoA', -pid) %>%
  ggplot(aes(x = advisor, y = WoA)) +
  geom_violin(colour = NA, aes(fill = advisor), alpha = .25) +
  geom_boxplot(outlier.colour = NA, fill = NA, aes(group = advisor),
               width = .2) +
  geom_line(alpha = .25, aes(group = pid)) + 
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  scale_y_continuous(limits = c(0, 1), expand = c(0, 0)) +
  labs(x = 'Advisor')

```

### Cleaner WoA graph

```{r}

# Plot WoA
byAdvice %>%
  group_by(advisor0questionnaireHonestyLabel, pid, advisor0idDescription) %>% 
  summarise(woa = mean(woa)) %>%
  ggplot(aes(x = advisor0questionnaireHonestyLabel, y = woa)) +
  geom_violin(colour = NA, fill = "grey90") +
  geom_boxplot(fill = "white", outlier.color = NA, width = .15, size = 1.25) +
  geom_line(alpha = .25, aes(group = pid)) + 
  stat_summary(geom = 'line', aes(group = advisor0idDescription, linetype = "Mean"), 
               fun.y = mean, size = 1.25) +
  scale_y_continuous(limits = c(0, 1)) +
  scale_linetype_manual(values = "dotted") +
  facet_wrap(~advisor0idDescription) +
  theme(text = element_text(size = 16),
        axis.text.x = element_text(size = 10),
        panel.grid = element_blank(),
        legend.position = "top") +
  labs(x = "Advice rating", y = "Weight on advice")

```

# Conclusions {.summary}

The experiment provided evidence for the hypotheses tested. Weight on (rated-as-honest) advice was higher for ingroup vs outgroup advisors, and weight on advice was higher for advice rated as honest than for advice rated as deceptive. This suggests that both the properties of the advice and of the advisor affect the extent to which advice is taken.

# Credits 

## Acknowledgements

Thanks as always to Nick Yeung and the other folks at the [ACC Lab](https://www.psy.ox.ac.uk/research/attention-cognitive-control-lab).

## R Packages

```{r results = 'asis'}
# list packages
packageNames <- (.packages())
# don't include very core package
packageNames <- packageNames[!(packageNames %in% 
                                 rownames(installed.packages(
                                   priority = "base")))]
# but do include the base package
packageNames <- c("base", packageNames)
out <- NULL
for (p in packageNames) {
  out <- rbind(out, data.frame('Package' = p, 
                               'Citations' = paste(format(citation(p), 
                                                          style = 'textVersion'), 
                                                   collapse = '<br/><br/>')))
}

kable(out)
```

## Funding

Matt Jaquiery is funded by a studentship from the [Medical Research Council](https://mrc.ukri.org/) (reference 1943590) and the University of Oxford [Department of Experimental Psychology](https://www.psy.ox.ac.uk/) (reference 17/18_MSD_661552).

## Technical details  

```{r results = 'hold'}
cat(paste('Time stamp:', Sys.time(), '\n\n'))
cat('Runtime \n')
proc.time()
cat('\n')
sessionInfo()
```