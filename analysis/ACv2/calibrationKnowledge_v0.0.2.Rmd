---
title: "Confidence estimation"
author: "[Matt Jaquiery](https://github.com/mjaquiery) ([matt.jaquiery@psy.ox.ac.uk](mailto:matt.jaquiery@psy.ox.ac.uk))"
output:
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '3'
    css: ../src/writeUp.css
    includes:
      after_body: ../src/toc_menu.html
  html_notebook:
    toc: yes
    toc_depth: 3
    css: ../src/writeUp.css
    includes:
      after_body: ../src/toc_menu.html
editor_options:
  chunk_output_type: inline
---

December 2019

[Script run `r Sys.time()`]


```{r prematter, include = F}

library(tidyverse)
library(scales)
library(broom)
library(BayesFactor)
library(prettyMD)
library(knitr)
library(parallel)
library(ez)

opts_chunk$set('echo' = F)

set.seed(20191216)

# Plot setup
theme_set(theme_light() + 
            theme(panel.grid = element_blank(),
                  legend.position = 'top'))

```

```{r loadData, include = F}

# rDir <- "http://localhost/ExploringSocialMetacognition/data/public/"
testData <- F

studyVersion <- "0-0-18"
studyName <- "calibrationKnowledge"

exclude <- list(
  maxTrialRT = 60000,    # trials take < 1 minute
  minTrials = 11,        # at least 11 trials completed
  minChangeRate = .1     # some advice taken on 10%+ of trials
  ) 

skipLoadData <- F

source("src/02_Exclusions.R")

if (testData) {
  # Construct some models of participant behaviour on the ‘prefer worst known to
  # unknown’, ‘treat unknown as worst known’, and ‘weight advice weights
  # according to how likely advice of a given confidence was to come from each
  # advisor’
  source("src/sim-confidence-estimation.R")
  model <- simulateCK(
    AdvisedTrial,
    agentInsensitivitySD = 8,
    agentConfidence = 2,
    agentConfidenceSD = 4,
    agentEgoBias = .7, 
    agentEgoBiasSD = .2,
    agentEffectSize = .1,
    agentEffectSizeSD = .1,
    strategy = 'PreferWorst'
    )
  AdvisedTrial <- model$trials
  decisions <- byDecision(AdvisedTrial)
}

```

# Introduction

Observations from [evolutionary models](https://github.com/oxacclab/EvoEgoBias) show that egocentric discounting is a successful strategy in environments where advisors cannot be clear on how advisors' expressed confidence relates to their actual confidence. We reason that human participants may show a sensitivity to these contextual factors underlying advice-taking and respond to them in a rational manner. To test the effects of known versus unknown confidence representations, we manipulated whether participants knew which of two advisors was giving them advice. Participants performed a block of 9 trials to get acquainted with each of two advisors, one who had high confidence and one who had low confidence (both were equally accurate). The advisors introduced themselves exhibiting their confidence. Participants then performed 20 trials where no feedback was given and advice came from one of the two advisors they had previously encountered. In some of these trials the advice was labelled as coming from the advising advisor, in others it was shown as coming from an unspecified one of the two advisors.

We hypothesised that participants would show greater sensitivity to the confidence of advice when they knew the identify of the advisor.

# Method

The experimental code is available on [GitHub](https://github.com/oxacclab/ExploringSocialMetacognition), and the experiment can be performed by visiting [https://acclab.psy.ox.ac.uk/~mj221/ESM/ACBin/ce.html](https://acclab.psy.ox.ac.uk/~mj221/ESM/ACBin/ck.html?PROLIFIC_PID=WriteUp). 

# Results

## Exclusions

```{r exclusions}

if (testData) {
  print("NOTE: Testing (simulated) data!!!!")
}

tmp <- suppressWarnings(left_join(exclusions, okayIds, by = "pid"))
tmp <- tmp %>% mutate(pid = as.character(pid))

tmp$condition <- factor(tmp$condition, labels = c("highFirst",
                                                  "lowFirst"))

table(tmp$excluded, tmp$condition)

# Test accuracy of the labels
AdvisedTrial %>% 
  mutate(pid = as.character(pid)) %>%
  group_by(pid, block, advisor0idDescription) %>% 
  filter(block == 2) %>% 
  summarise(n()) %>% 
  left_join(tmp, by = 'pid') %>% 
  ungroup() %>%
  select(advisor0idDescription, condition) %>%
  mutate(ok = if_else(advisor0idDescription == 'lowConf',
                      condition == 'lowFirst',
                      condition == 'highFirst')) %>%
  pull(ok) %>%
  all() %>%
  expect_equal(T)

```

```{r data visualisation variables}
# Add some variables which we can use for plotting prettily later

AdvisedTrial <- AdvisedTrial %>%
  mutate(Hybrid = if_else(advisor0name == '?', 'Hybrid', 'Labelled'),
         Advisor = capitalize(as.character(advisor0idDescription)),
         Phase = if_else(feedback, "Familiarization", "Test"))

decisions <- decisions %>% 
  mutate(Hybrid = if_else(advisor0name == '?', 'Hybrid', 'Labelled'),
         Advisor = capitalize(as.character(advisor0idDescription)),
         Phase = if_else(feedback, "Familiarization", "Test"),
         Decision = capitalize(decision))
```

Our final participant list consists of `r length(unique(AdvisedTrial$pid))` participants who completed an average of `r num2str(mean(aggregate(advisor0 ~ pid, AdvisedTrial, length)$advisor0))` trials each, of which an average of `r num2str(mean(aggregate(advisor0 ~ pid, AdvisedTrial[!AdvisedTrial$feedback, ], length)$advisor0))` had no feedback.

## Task performance

First we offer a characterisation of the task, to provide the reader with a sense of how the participants performed. 

### Decisions

Participants offered estimates of whether various events took place before or after a given year. The correct answers were always between 1900 and 2000. Participants answered by selecting a point on one of two confidence bars, with the choice of bar indicating the chosen answer. 

#### Correctness

```{r accuracy}

decisions %>% peek(responseCorrect, decision) %>%
  num2str.tibble(isProportion = T, precision = 3)

decisions %>% filter(feedback == T) %>% 
  peek(responseCorrect, decision) %>% 
  num2str.tibble(isProportion = T, precision = 3) %>%
  cbind(Phase = unique(decisions %>% filter(feedback) %>% pull(Phase)))

```

```{r accuracyGraph}

tmp <- decisions %>% group_by(pid, Decision, Phase) %>%
  do(correct = mean_cl_normal(.$responseCorrect)) %>%
  unnest(correct) %>%
  transmute(pid, Decision, Phase,
            pCorrect = y, 
            ciLow = ymin,
            ciHigh = ymax)

ggplot(tmp, 
       aes(x = Decision, y = pCorrect)) +
  geom_hline(yintercept = .5, linetype = 'dashed') +
  geom_violin(alpha = .15, colour = NA, aes(fill = Decision)) +
  geom_boxplot(fill = "white", outlier.color = NA, aes(colour = Decision),
               width = .25, size = 1.25) +
  geom_line(alpha = .25, aes(group = pid)) + 
  scale_y_continuous(limits = c(0, 1)) +
  labs(y = "P(Response correct)") +
  facet_grid(~Phase)

# caption = "Probability of correct responses on initial estimates and final decisions. Lines show individual participant means, while violins and boxplots give distributions. The dashed line shows chance performance."

```

### Timing

We can look at the response time - the difference between the time the response is opened and the time the response is received.  

```{r time}

decisions$rt <- decisions$responseTimeEstimate - decisions$timeResponseOpen

decisions %>% peek(rt, decision) %>% 
  mutate_if(is.numeric, round)

```

```{r timeGraph}

tmp <- decisions %>% group_by(pid, Decision) %>% 
  summarise(rt = mean(rt))

tmp %>%
  ggplot(aes(x = Decision, y = rt)) +
  geom_violin(alpha = .15, colour = NA, aes(fill = Decision)) +
  geom_boxplot(fill = "white", outlier.color = NA, aes(colour = Decision),
               width = .25, size = 1.25) +
  geom_line(alpha = .25, aes(group = pid)) + 
  labs(y = "Response time (ms)") 

# caption = "Response times for initial estimates and final decisions. Lines show individual participant means, while violins and boxplots give distributions."

```

### Summary {.summary}

Participants initial estimates were slightly better than chance in both the familiarization and test phases. Final decisions, made after seeing advice, were better still. These results indicate that participants likely found the task difficult but possible, and found the advice helpful. Participants made final decisions much more quickly than initial estimates, although they were likely processing advice information during the 2s window in which advice is animated on the screen.

## Metacognitive performance

### Confidence

Each answer bar allowed the participant to express their confidence in that answer by selecting a higher point on the bar for a higher confidence, in a range of 0-100% for each decision.

```{r confidence}

decisions %>% peek(responseConfidence, decision) %>%
  num2str.tibble(isProportion = T, precision = 1)

decisions %>% filter(feedback == T) %>% 
  peek(responseConfidence, decision) %>% 
  num2str.tibble(isProportion = T, precision = 3) %>%
  cbind(Phase = unique(decisions %>% filter(feedback) %>% pull(Phase)))

```

```{r confidence graph}

tmp <- decisions %>% group_by(pid, Decision, Phase) %>%
  do(confidence = mean_cl_normal(.$responseConfidence)) %>%
  unnest(confidence) %>%
  transmute(pid, Decision, Phase,
            confidence = y, 
            ciLow = ymin,
            ciHigh = ymax)

ggplot(tmp, 
       aes(x = Decision, y = confidence)) +
  geom_violin(alpha = .15, colour = NA, aes(fill = Decision)) +
  geom_boxplot(fill = "white", outlier.color = NA, aes(colour = Decision),
               width = .25, size = 1.25) +
  geom_line(alpha = .25, aes(group = pid)) + 
  scale_y_continuous(limits = c(0, 100)) +
  labs(y = "Confidence") +
  facet_grid(~Phase)

# caption = "Confidence of initial estimates and final decisions. Lines show individual participant means, while violins and boxplots give distributions."

```

### Relationship between confidence and p(correct)

Participants indicated the confidence in their decisions simultaneously with the decisions themselves. We can thus explore how well a participant's confidence signals their accuracy. We provide only a brief examination here where normalised responses are split into high and low confidence for each participant, and the accuracies of the two categories are compared. This is done separately for first and last decisions.

```{r confidence normalisation}

decisions <- decisions %>% 
  group_by(pid, decision) %>% 
  dplyr::mutate(zConf = scale(responseConfidence)) %>%
  ungroup() 

```

```{r confidence binary split}

tmp <- decisions %>% 
  mutate(highConf = zConf > 0) %>%
  group_by(pid, decision, Decision, highConf) %>%
  summarise(pCorrect = mean(responseCorrect)) %>%
  filter(!is.na(highConf)) %>%
  group_by(pid, decision, Decision) %>%
  spread(highConf, pCorrect) %>%
  mutate(pCorrectDiff = `TRUE` - `FALSE`)

drop <- length(unique(tmp$pid)) != length(unique(decisions$pid))
if (drop > 0) {
  print(paste0("Dropping ", drop, " rows from analysis for unstandardizable confidence."))
}

tmp %>% 
  peek(pCorrectDiff, decision) %>% 
  num2str.tibble(isProportion = T, precision = 3)

print("p(correct|high confidence) - p(correct|low confidence) for initial estimates")
ttestBF(tmp$pCorrectDiff[tmp$decision == "first"])
print("p(correct|high confidence) - p(correct|low confidence) for final decisions")
ttestBF(tmp$pCorrectDiff[tmp$decision == "last"])

```

```{r confidence binary split plot}

tmp %>% 
ggplot(aes(x = Decision, y = pCorrectDiff)) +
  geom_hline(yintercept = 0, linetype = 'dashed') +
  geom_violin(alpha = .15, colour = NA, aes(fill = Decision)) +
  geom_boxplot(fill = "white", outlier.color = NA, aes(colour = Decision),
               width = .25, size = 1.25) +
  geom_line(alpha = .25, aes(group = pid)) + 
  labs(y = "P(Correct | sure) - P(Correct | unsure)") +
  scale_y_continuous(limits = c(-1, 1))  

# caption = "Difference in the probability of correct responses on initial estimates and final decisions when made with high versus low confidence. Lines show individual participant means, while violins and boxplots give distributions. The dashed line shows chance performance."
 
```

### Summary {.summary}

Participants' answers were generally around the upper midpoint on the scale, with a fair amount of variation between participants. Confidence in final decisions did not appear to be consistently higher than initial estimates, although this is unsurprising given the final decisions are not split by whether the advisor agreed with the initial decision.

The relationship between confidence and the probability of being correct is essentially zero for initial estimates, indicating that participants are guessing on initial estimates. Final decisions did show a relationship where higher confidence in decisions was indicative of a greater probability of being correct, presumably because participants' confidence was readily affected by advice. This pattern was true for most participants individually, as well as for the sample as a whole.

## Advisor performance

Participants in should get an appropriate experience of each advisor in the feedback condition. This means that the accuracy and metacognitive sensitivity of the advisors should be balanced, while the confidence (metacognitive bias) is not. 

### Accuracy

```{r adviceAccuracy}

AdvisedTrial$advisor0accurate <- ifelse(
  is.na(AdvisedTrial[, paste0(advisorNames[1], ".accurate")]),
  pull(AdvisedTrial[, paste0(advisorNames[2], ".accurate")]),
  pull(AdvisedTrial[, paste0(advisorNames[1], ".accurate")])
)

AdvisedTrial$advisor0agree <- ifelse(
  is.na(AdvisedTrial[, paste0(advisorNames[1], ".agree")]),
  pull(AdvisedTrial[, paste0(advisorNames[2], ".agree")]),
  pull(AdvisedTrial[, paste0(advisorNames[1], ".agree")])
)

tmp <- AdvisedTrial %>% 
  filter(feedback) %>%
  group_by(pid, Advisor) %>%
  summarise(advisorAccuracy = mean(advisor0accurate),
            advisorAgree = mean(advisor0agree)) 

tmp %>% 
  group_by(Advisor) %>%
  summarise(advisorAccuracy = mean(advisorAccuracy),
            advisorAgree = mean(advisorAgree)) %>%
  num2str.tibble(isProportion = T, precision = 3)

```

```{r adviceAccuracyGraph}

tmp %>% 
  dplyr::rename(`p(correct)` = advisorAccuracy,
                `p(agree)` = advisorAgree) %>%
  gather("var", "value", `p(correct)`:`p(agree)`) %>%
  mutate(var = capitalize(var)) %>%
  ggplot(aes(x = Advisor, y = value)) +
  geom_hline(yintercept = .5, linetype = 'dashed') +
  # geom_hline(yintercept = .75, linetype = 'dashed', colour = 'lightgrey') +
  geom_violin(alpha = .15, colour = NA, aes(fill = Advisor)) +
  geom_boxplot(fill = "white", outlier.color = NA, aes(colour = Advisor),
               width = .25, size = 1.25) +
  geom_line(alpha = .25, aes(group = pid)) + 
  labs(y = "Mean") +
  scale_y_continuous(limits = c(0, 1)) + 
  scale_colour_discrete(h.start = 45) + 
  scale_fill_discrete(h.start = 45) +
  facet_wrap(~var)
  
# caption = "Probability of agreement (left panel) and correct responses (right panel) for advice from the two advisors. Lines show individual participant means, while violins and boxplots give distributions. The dashed line shows chance level."
  
```

### Advisor confidence

The advisors differ by design in their confidence, so participants should experience these differences.

```{r advisor confidence}

tmp <- AdvisedTrial %>% 
  filter(feedback) %>%
  group_by(pid, Advisor) %>%
  summarise(confidence = mean(advisor0adviceConfidence))

tmp %>% 
  group_by(Advisor) %>%
  summarise(confidence = mean(confidence)) %>%
  num2str.tibble(precision = 1)


```

```{r advisor confidence graph}

tmp %>%
  ggplot(aes(x = Advisor, y = confidence)) +
  geom_violin(alpha = .15, colour = NA, aes(fill = Advisor)) +
  geom_boxplot(fill = "white", outlier.color = NA, aes(colour = Advisor),
               width = .25, size = 1.25) +
  geom_line(alpha = .25, aes(group = pid)) + 
  scale_y_continuous(limits = c(0, 100)) +
  scale_colour_discrete(h.start = 45) + 
  scale_fill_discrete(h.start = 45) +
  labs(y = 'Mean advisor confidence')

# caption = "Mean advisor confidence for each advisor during the training phase. Lines show the mean experience of an individual participant, while violins and boxplots show the distributions."

```

#### Individual participants' experience

We should take a look at a handful of specific distributions experienced by individual participants to ensure the individual-level view tallies with the sample view. 

```{r advisor confidence by participant}

tmp <- AdvisedTrial %>%
  filter(feedback) %>%
  select(pid, Advisor, Confidence = advisor0adviceConfidence)

ggplot(tmp, aes(x = Advisor, y = Confidence, colour = Advisor)) +
  geom_violin() +
  geom_point(position = position_jitter(.1), alpha = .5) + 
  scale_colour_discrete(h.start = 45) + 
  facet_wrap(~pid) +
  theme(strip.text = element_blank())

# caption = "Individual participant experience of the advisors during the familiarity phase. Each point shows a single trial, while the violins indicate the distribution."
  
ggplot(tmp, aes(x = Advisor, y = Confidence, colour = Advisor)) +
  geom_violin() +
  geom_point(position = position_jitter(width = .1), alpha = .1) +
  scale_colour_discrete(h.start = 45)

# caption = "Overall advice of advisors during the familiarity phase. Each point shows a single trial, while the violins indicate the distribution."

```

### Distance

Distance is the continuous version of agreement - the difference along the confidence scale between the advice and the initial estimate. Where different scales are endorsed, the difference is the combination of how sure the participant was on one scale plus how sure the advisor was on the other.

```{r adviceDistance}

AdvisedTrial$advisor0distance <- ifelse(
  is.na(AdvisedTrial[, paste0(advisorNames[1], ".distance")]),
  pull(AdvisedTrial[, paste0(advisorNames[2], ".distance")]),
  pull(AdvisedTrial[, paste0(advisorNames[1], ".distance")])
)
AdvisedTrial$advisor0distanceFinal <- ifelse(
  is.na(AdvisedTrial[, paste0(advisorNames[1], ".distanceFinal")]),
  pull(AdvisedTrial[, paste0(advisorNames[2], ".distanceFinal")]),
  pull(AdvisedTrial[, paste0(advisorNames[1], ".distanceFinal")])
)

tmp <- AdvisedTrial %>% 
  filter(feedback) %>%
  group_by(pid, Advisor) %>%
  summarise(adviceDistance = mean(advisor0distance)) 

tmp %>% 
  group_by(Advisor) %>%
  summarise(adviceDistance = mean(adviceDistance)) %>%
  num2str.tibble(precision = 1)

```

```{r adviceDistanceGraph}

tmp %>% 
  ggplot(aes(x = Advisor, y = adviceDistance)) +
  geom_violin(alpha = .15, colour = NA, aes(fill = Advisor)) +
  geom_boxplot(fill = "white", outlier.color = NA, aes(colour = Advisor),
               width = .25, size = 1.25) +
  geom_line(alpha = .25, aes(group = pid)) + 
  scale_colour_discrete(h.start = 45) + 
  scale_fill_discrete(h.start = 45) +
  scale_y_continuous(limits = c(0, 200)) +
  labs(y = 'Advice distance')

# caption = "Distance in scale points between the participants' initial estimates and the advisors' advice. Lines show individual participant means, while violins and boxplots give distributions."

```

### Summary {.summary}

The advisors' performances were within the expected ranges for both the probability and extent of agreement with the participants and the objective accuracy of their advice. The advisors didn't differ substantially from one another in any of these aspects.

Advisor confidence was systematically different between the high and low confidence advisors, with a moderate overlap between the distributions, as designed. This pattern was true both at the individual participant level and at the overall sample level. The low confidence advisor's estimates were less liable to extreme distances from the participant's initial estimate, but this is a direct consequence of the high confidence advisor using the upper extremes of the scale which amplify the distance for disagreement trials.

### Influence

The measure of influence is the extent to which a participant's confidence moves in the expected direction as a function of the advice (increasing when agreed with, decreasing when disagreed with).

Influence is the primary outcome measure. It is likely to differ between high and low confidence advisors because people tend to find high-confidence advice more persuasive. Relevant for our hypotheses, it may differ between hybrids and labelled advisors.

```{r influence}

AdvisedTrial$advisor0influence <- 
  unlist(sapply(1:nrow(AdvisedTrial),
                function(i) 
                  AdvisedTrial[i, 
                               paste0(AdvisedTrial$advisor0idDescription[i], 
                                      ".influence")]))

tmp <- AdvisedTrial %>% 
  filter(!feedback) %>%
  mutate(hybrid = advisor0name == '?') %>%
  group_by(pid, advisor0idDescription, hybrid) %>%
  summarise(advisorInfluence = mean(advisor0influence)) 

tmp %>% 
  group_by(advisor0idDescription, hybrid) %>%
  summarise(advisorInfluence = mean(advisorInfluence)) %>%
  num2str.tibble(precision = 1)

```

```{r influenceGraph}

tmp %>% 
  mutate(Advisor = capitalize(paste0(advisor0idDescription, if_else(hybrid, '?', '')))) %>%
  ggplot(aes(x = Advisor, y = advisorInfluence)) +
  geom_hline(yintercept = 0, linetype = 'dashed') +
  geom_violin(alpha = .15, colour = NA, aes(fill = Advisor)) +
  geom_boxplot(fill = "white", outlier.color = NA, aes(colour = Advisor),
               width = .25, size = 1.25) +
  geom_line(alpha = .25, aes(group = pid)) + 
  scale_colour_discrete(h.start = 45) + 
  scale_fill_discrete(h.start = 45) +
  scale_y_continuous(limits = c(-100, 100)) +
  labs(y = 'Advisor influence')

# caption = "Influence of the advisors on test trials. Advisors with their own names show data from labelled trials, while those with a '?' show data from trials where the advice was given a hybrid label. Lines show means for individual participants, while violins and boxplots show distributions. The dashed line indicates zero influence."

```

#### Influence scatter plots

```{r}

AdvisedTrial %>%
  filter(!feedback) %>%
  mutate(Advisor = if_else(advisor0name == '?', 
                           'Hybrid', 
                           capitalize(as.character(advisor0idDescription)))) %>%
  ggplot(aes(x = advisor0adviceConfidence, 
             y = advisor0influence, 
             colour = Advisor)) +
  geom_hline(yintercept = 0, linetype = 'dashed') +
  geom_point(alpha = .25) +
  geom_smooth(aes(group = Advisor),
               method = 'lm', se = F, fullrange = T) +
  stat_smooth(aes(group = paste0(pid, Advisor, sep = '.')),
              geom = 'line', method = 'lm', alpha = .25) +
  scale_colour_manual(values = hue_pal(h.start = 45)(4)[1:3]) + 
  scale_y_continuous(limits = c(-200, 200)) +
  labs(x = 'Advisor confidence', y = 'Advisor influence')

# caption = "Relationship between advisor confidence and advisor influence. Dots show individual trials, while faint lines link a participant's trials for a given advisor. The heavy lines show data for each advisor aggregated over all participants. The dashed black line indicates zero influence."

```

#### Influence distribution

It's good to keep a general eye on the distribution of weight-on-advice on a trial-by-trial basis. 

```{r influenceDistribution}

AdvisedTrial %>% 
  filter(!feedback) %>%
  ggplot(aes(advisor0influence, colour = Advisor, fill = Advisor)) + 
  geom_histogram() +
  facet_grid(Hybrid ~ Advisor) +
  scale_colour_discrete(h.start = 45) + 
  scale_fill_discrete(h.start = 45) +
  labs(y = 'Trial count', x = 'Advisor influence')

# caption = "Histogram of advisor influence by advisor and hybrid status for test trials."

```

#### Is initial accuracy or initial confidence a better predictor of influence?

```{r influence correlations}

tmp <- AdvisedTrial %>% 
  select(pid, responseCorrect, responseConfidence, advisor0influence) %>%
  nest(data = -pid) %>%
  mutate(
    accBF = map(data, ~ correlationBF(.x$responseCorrect, 
                                    .x$advisor0influence)),
    acc.BF = map(accBF, ~ exp(.@bayesFactor$bf)),
    conBF = map(data, ~ correlationBF(.x$responseConfidence, 
                                    .x$advisor0influence)),
    con.BF = map(conBF, ~ exp(.@bayesFactor$bf)),
    acc = map(data, ~ cor.test(as.numeric(.x$responseCorrect), 
                               .x$advisor0influence)), 
    acc = map(acc, tidy),
    con = map(data, ~ cor.test(.x$responseConfidence, .x$advisor0influence)), 
    con = map(con, tidy)
  ) %>% 
  unnest(acc, names_sep = ".") %>%
  unnest(con, names_sep = ".") %>%
  unnest(c(acc.BF, con.BF))

tmp <- bind_rows(
  tmp %>% 
    select_at(vars(starts_with('acc'))) %>%
    rename_all(~ str_replace(., 'acc\\.(.+)', '\\1')) %>%
    mutate(property = 'accuracy'),
  tmp %>% 
    select_at(vars(starts_with('con'))) %>%
    rename_all(~ str_replace(., 'con\\.(.+)', '\\1')) %>%
    mutate(property = 'confidence')
)

tmp %>% 
  group_by(property) %>%
  summarise_if(is.numeric, mean, na.rm = T)

```

### Summary {.summary}

The influence of the advisors (formally tested below) did not show obvious differences between the advisors' presentation (labelled as themselves or as a hybrid). Numerically, there were decreases in the influence of the advice of advisors when advice was presented as coming from hybrids. As advice was more confident it was more likely to be influential, and this pattern was true for both high and low confidence advisors, and for both advisors combined under the hybrid presentation. The latter more closely resembles the low confidence advisor. The histograms show hints of a bimodal distribution between ignoring and averaging advice for both advisors under both presentations. 

Participant initial estimate accuracy is a strong predictor of influence, while initial estimate confidence is not. 

## Manipulation checks

Participants should learn the advisor mappings during the course of the experiment. This means that they should treat the low- and high-confidence advisors' advice differently when those advisors express medium confidence on the scale. Specifically, they should be more highly influenced by the low-confidence advisor than by the high-confidence advisor.

```{r}

boundaries <- c(min(AdvisedTrial$highConf.adviceConfidence, na.rm = T),
                max(AdvisedTrial$lowConf.adviceConfidence, na.rm = T))

tmp <- AdvisedTrial %>%
  filter(!feedback,
         Hybrid == 'Labelled',
         advisor0adviceConfidence >= boundaries[1],
         advisor0adviceConfidence <= boundaries[2])

ggplot(tmp, aes(x = advisor0adviceConfidence, y = advisor0influence, colour = Advisor)) + 
  geom_hline(yintercept = 0, linetype = 'dashed') +
  stat_smooth(aes(group = paste0(pid, Advisor, sep = '.')),
              geom = 'line', method = 'lm', alpha = .25) +
  geom_point(alpha = .25) +
  geom_smooth(aes(group = Advisor),
               method = 'lm', se = F, fullrange = T) +
  scale_y_continuous(limits = c(-100, 100)) +
  scale_colour_discrete(h.start = 45) + 
  scale_fill_discrete(h.start = 45) +
  facet_wrap(~Advisor) +
  labs(x = 'Advisor confidence', y = 'Advisor influence') 

# caption = "Relationship between advisor confidence and advisor influence for labelled advisors during test trials. Dots show individual trials, while faint lines show individual participants' linear model fits. Heavy lines show linear fits aggregated over all participants, while the black dashed line shows zero influence."

```

### Summary {.summary}

The slope for the low confidence advisor looks steeper than that of the high confidence advisor, suggesting that participants were more sensitive to changes in the low confidence advisor's confidence in the middle of the scale. There is no obvious difference between overall influence levels, however, suggesting that participants do not fully adapt to the calibration of the low-confidence advisor.

## Advisor preferences

In the hypothesis tests we need to know which advisor is most influential for each participant. We explore this influence over only those trials on which the participants receive feedback.

```{r favourite advisors}

# Find favourite advisor where confidence is balanced
tmp <- AdvisedTrial %>%
  filter(feedback,
         advisor0adviceConfidence >= boundaries[1],
         advisor0adviceConfidence <= boundaries[2]) %>%
  group_by(pid, advisor0id) %>%
  summarise(influence = mean(advisor0influence)) %>%
  mutate(influence = if_else(is.na(influence), -Inf, influence)) %>%
  spread(advisor0id, influence) %>%
  group_by(pid) %>%
  summarise(favouriteAdvisorId = if_else(`1` > `2`, 1, 2))

AdvisedTrial <- AdvisedTrial %>% left_join(tmp, by = 'pid')

if (any(is.na(AdvisedTrial$favouriteAdvisorId))) {
  pids <- AdvisedTrial %>%
    group_by(pid) %>% 
    summarise(favouriteAdvisorId = mean(favouriteAdvisorId)) %>%
    filter(is.na(favouriteAdvisorId)) %>%
    pull(pid)
  warning("Removing ", length(pids), " participants with NAs in advisor influence. Ids: ", paste0(pids, collapse = ", "))
}

# Plot influence for feedback trials
tmp <- AdvisedTrial %>% 
  filter(!is.na(favouriteAdvisorId),
         feedback,
         advisor0adviceConfidence >= boundaries[1],
         advisor0adviceConfidence <= boundaries[2]) %>%
  group_by(pid, Advisor) %>%
  summarise(advisorInfluence = mean(advisor0influence)) %>%
  spread(Advisor, advisorInfluence) %>%
  mutate(Favourite = if_else(LowConf > HighConf, 'LowConf', 'HighConf')) %>%
  gather("Advisor", "advisorInfluence", LowConf:HighConf)

tmp %>% 
  ggplot(aes(x = Advisor, y = advisorInfluence)) +
  geom_hline(yintercept = 0, linetype = 'dashed') + 
  geom_violin(alpha = .15, colour = NA, aes(fill = Advisor)) +
  geom_boxplot(fill = "white", outlier.color = NA, aes(colour = Advisor),
               width = .25, size = 1.25) +
  geom_line(alpha = .25, size = .75, aes(group = pid, linetype = Favourite)) + 
  scale_colour_discrete(h.start = 45) + 
  scale_fill_discrete(h.start = 45) +
  scale_y_continuous(limits = c(-100, 100)) +
  labs(y = 'Advisor influence')

# caption = "Advisor influence by advisor on familiarity trials where the advisor's confidence was within the medium-confidence window. Lines show individual participant's means, with the line type indicating which advisor had the higher mean influence for that participant on these trials. Violins and boxplots show distributions, while the full width black dashed line indicates zero influence."

tmp %>%
  group_by(pid) %>% 
  summarise(favourite = first(Favourite)) %>%
  group_by(favourite) %>%
  filter(!is.na(favourite)) %>%
  summarise(n())

```

### Summary {.summary}

Participants are perhaps surprisingly fairly evenly spilt in terms of the advisor they find most influential. We might expect a high confidence advisor to be more influential than a low confidence one because people generally prefer high confidence, and because low confidence advice may tempt some people to reduce their confidence even if agreed with. 

## Hypothesis testing

The hypotheses being tested here are:  

1. Participants will place different weights on their least favourite advisor when they know that advisor's identity compared to when that advisor is labelled as a hybrid (controlling for confidence).

```{r h1}
  
# Test
tmp <- AdvisedTrial %>%
  filter(!feedback, 
         advisor0id != favouriteAdvisorId,
         advisor0adviceConfidence >= boundaries[1],
         advisor0adviceConfidence <= boundaries[2]) %>%
  group_by(pid, Hybrid) %>%
  summarise(influence = mean(advisor0influence)) %>%
  spread(Hybrid, influence)

pids <- 
  tmp %>% 
  gather("advisor", "influence", -pid) %>%
  filter(is.na(influence)) %>%
  pull(pid)

if (length(pids)) {
  warning("Removing ", length(pids), " participants with NAs in advisor influence. Ids: ", paste0(pids, collapse = ", "))
  tmp <- tmp %>% filter(!(pid %in% pids))
}

cat(md.ttest(tmp$Labelled, tmp$Hybrid, paired = T, 
             labels = c('*M*|Labelled', '*M*|Hybrid')))

# Graph
tmp %>% 
  gather("advisor", "influence", -pid) %>%
  left_join(
    AdvisedTrial %>% 
      group_by(pid) %>% 
      filter(advisor0id != favouriteAdvisorId) %>%
      summarise(nonfavourite = unique(Advisor)),
    by = 'pid') %>%
  rename(Advisor = advisor) %>%
  ggplot(aes(x = Advisor, y = influence)) +
  geom_hline(yintercept = 0, linetype = 'dashed') +
  geom_violin(alpha = .15, colour = NA, aes(fill = Advisor)) +
  geom_boxplot(fill = "white", outlier.color = NA, aes(colour = Advisor),
               width = .25, size = 1.25) +
  geom_line(alpha = .25, size = .75, aes(group = pid, linetype = nonfavourite)) + 
  scale_colour_discrete(h.start = 135) + # match colours with hybrid green above
  scale_fill_discrete(h.start = 135) +
  scale_y_continuous(limits = c(-100, 100)) +
  scale_linetype_discrete(name = 'Least favourite advisor') +
  labs(x = 'Advisor presentation', y = 'Advisor influence')

# caption = "Advice influence for the least influential advisor in the critical trials. Lines show individual participants' means, while box plots and violins show distributions. Labelled trials are those where the advisor's identity is displayed to the participant, while hybrid trials are those where the advice could be coming from either advisor. The line types show the identity of the least advisor whose influence is plotted for that participant. The full-width black dashed line shows zero influence."

```

### Summary {.summary}

There is some evidence to suggest that advice is treated similarly when it comes from the least favourite advisor regardless of whether that advice is labelled as belonging to the least favourite advisor or labelled with the hybrid avatar. This is an intuitively sensible strategy, though not in line with our hypothesis that people will have a superadditive dislike of source uncertainty.

## Exploration

### ANOVA

The experiment is structured suitably for a neat ANOVA comparing the effects of advisor and advisor labelling.

```{r anova}

tmp <- AdvisedTrial %>% 
  dplyr::filter(feedback == F,
                advisor0adviceConfidence >= boundaries[1],
                advisor0adviceConfidence <= boundaries[2]) %>%
  mutate(hybrid = factor(advisor0name == '?')) %>%
  group_by(pid, advisor0idDescription, hybrid) %>% 
  summarise(influence = mean(advisor0influence)) %>%
  pivot_wider(names_from = c(hybrid, advisor0idDescription), 
              values_from = influence) %>%
  pivot_longer(-pid, 
               names_to = c('hybrid', 'advisor0idDescription'),
               names_sep = '_', 
               values_to = 'influence')
  
pids <- tmp %>% 
  group_by(pid) %>%
  summarise(influence = mean(influence)) %>%
  filter(is.na(influence)) %>%
  pull(pid)

if (length(pids) < length(unique(tmp$pid))) {
  warning("Removing ", length(pids), " participants with NAs in advisor influence. Ids: ", paste0(pids, collapse = ", "))
  tmp <- tmp %>% filter(!(pid %in% pids))
}

tmp <- tmp %>% 
  ungroup() %>%
  mutate(
  pid = factor(pid),
  hybrid = factor(hybrid),
  advisor0idDescription = factor(advisor0idDescription)
)

anv <- tmp %>% 
  ezANOVA(dv = influence,
          wid = pid, 
          within = c(hybrid, advisor0idDescription))

anv

dw <- .25
AdvisedTrial %>%
  filter(!feedback) %>%
  group_by(pid, Advisor, Hybrid) %>%
  summarise(influence = mean(advisor0influence)) %>%
  ungroup() %>%
  mutate(Advisor = factor(Advisor), 
         Hybrid = factor(Hybrid)) %>%
  ggplot(aes(x = Hybrid, y = influence, colour = Advisor)) +
  geom_hline(yintercept = 0, linetype = 'dashed') +
  scale_x_discrete() +
  geom_line(aes(
    group = paste0(pid, Advisor),
    x = as.numeric(Hybrid) - .75 * dw + .5 * dw * as.numeric(Advisor)
    ), alpha = .25) +
  stat_summary(aes(group = Advisor), geom = 'line', fun.y = mean, size = 1,
               position = position_dodge(dw)) +
  stat_summary(aes(group = paste0(Advisor, Hybrid)), geom = 'point', 
               size = 4, fun.y = mean, position = position_dodge(dw)) +
  stat_summary(aes(group = paste0(Advisor, Hybrid)), geom = 'errorbar', 
               width = 0, size = 1, fun.data = mean_cl_normal, 
               position = position_dodge(dw)) +
  scale_colour_discrete(h.start = 45) +
  labs(y = 'Advisor influence', x = 'Advisor presentation')

# caption = "ANOVA plot of advisor influence on test trials by advisor identity and presentation. Faint lines show individual participant means, while heavy lines and circles give means aggregated over all participants, with error bars showing 95% confidence intervals. The black dashed line shows zero influence."

if (testData) {
  for (s in c('LeastPref', 
              'PreferWorst'#, 
              # 'LikelihoodWeighted'
              )) {
    print(paste0('=== ', s, ' ==='))
    tmp <- AdvisedTrial %>% 
      dplyr::filter(feedback == F) %>%
      mutate(advisor0agree = as.logical(advisor0agree))
    tmp$r <- pull(tmp, paste0('responseConfidenceFinal', s))
    tmp <- tmp %>% 
      mutate(
        influence = case_when(
          advisor0agree & r >= 0 ~ r - responseConfidence,
          advisor0agree & r < 0 ~ -(responseConfidence + r),
          !advisor0agree & r >= 0 ~ -(responseConfidence - r),
          !advisor0agree & r < 0 ~ responseConfidence + r
        )
    )
    tmp %>%
      mutate(hybrid = factor(advisor0name == '?')) %>%
      group_by(pid, advisor0idDescription, hybrid) %>% 
      summarise(influence = mean(influence)) %>% 
      ezANOVA(dv = influence,
              wid = pid, 
              within = c(hybrid, advisor0idDescription)) %>%
      print()
  }
}

```

#### Summary {.summary}

Over the medium-range advice zone which both advisors occupy, the high-confidence advisor's advice is more influential, and this does not differ as a function of presentation. This is somewhat puzzling. Firstly, the high confidence advisor is less sure for the same level of confidence than the low-confidence advisor, so the low-confidence advisor's advice ought to be more highly weighted. Secondly, there is no effect of presentation, or interaction with presentation, meaning that the high confidence advisor's advice is probably also more influential when labelled as a hybrid than when the low-confidence advisor's advice is given with the same label in a similar confidence range. This latter point is perhaps partially explained by the tailing off of the frequency of low-confidence advice at the higher end of the shared band, meaning that participants may be more justified in treating advice from a hybrid towards the upper end of the shared band as coming from the high-confidence advisor. 

### Comparing influence by favourite advisor

Hybrid vs labelled for favourite advisor.
```{r}
tmp <- AdvisedTrial %>%
  filter(!feedback, 
         advisor0id == favouriteAdvisorId,
         advisor0adviceConfidence >= boundaries[1],
         advisor0adviceConfidence <= boundaries[2]) %>%
  group_by(pid, Hybrid) %>%
  summarise(influence = mean(advisor0influence)) %>%
  spread(Hybrid, influence)

pids <- 
  tmp %>% 
  gather("advisor", "influence", -pid) %>%
  filter(is.na(influence)) %>%
  pull(pid)

if (length(pids)) {
  warning("Removing ", length(pids), " participants with NAs in advisor influence. Ids: ", paste0(pids, collapse = ", "))
  tmp <- tmp %>% filter(!(pid %in% pids))
}

cat(md.ttest(tmp$Labelled, tmp$Hybrid, paired = T, 
             labels = c('*M*|Labelled', '*M*|Hybrid')))
```

```{r}
tmp <- AdvisedTrial %>%
  filter(!feedback, 
         Hybrid == "Labelled",
         advisor0adviceConfidence >= boundaries[1],
         advisor0adviceConfidence <= boundaries[2]) %>%
  mutate(isFavourite = if_else(advisor0id == favouriteAdvisorId,
                               "Favourite", "Non-favourite")) %>%
  group_by(pid, isFavourite) %>%
  summarise(influence = mean(advisor0influence)) %>%
  spread(isFavourite, influence)

pids <- 
  tmp %>% 
  gather("advisor", "influence", -pid) %>%
  filter(is.na(influence)) %>%
  pull(pid)

if (length(pids)) {
  warning("Removing ", length(pids), " participants with NAs in advisor influence. Ids: ", paste0(pids, collapse = ", "))
  tmp <- tmp %>% filter(!(pid %in% pids))
}

cat(md.ttest(tmp$Favourite, tmp$`Non-favourite`, paired = T, 
             labels = c('*M*|Favourite', '*M*|Non-favourite')))
```

# As above but comparing favourite to least favourite advisor


### Time taken vs accuracy

There is considerable variation in the time participants take to produce their initial estimates - to what extent is this variation predictive of accuracy? 

Is the same pattern true within participants as well as between them?

### Time taken vs confidence-weighted accuracy

How does time taken relate to metacognitive accuracy?

### Agreement rates in the test trials

```{r}
tmp <- AdvisedTrial %>% 
  filter(!feedback) %>%
  mutate(hybrid = advisor0name == '?') %>%
  group_by(pid, advisor0idDescription, hybrid) %>%
  summarise(agreement = mean(advisor0agree)) 

n <- length(unique(tmp$pid))
bw <- .2

tmp %>% 
  mutate(Advisor = capitalize(paste0(advisor0idDescription, if_else(hybrid, 'Hybrid', '')))) %>%
  ggplot(aes(x = agreement, fill = Advisor)) +
  geom_histogram(binwidth = bw) +
  stat_bin(aes(label = ..count..), 
           geom = "text", vjust = -.5, binwidth = bw, y = n * 1.1) +
  scale_fill_discrete(h.start = 45, guide = "none") +
  scale_y_continuous(limits = c(0, n)) +
  labs(x = "Agreement rate") +
  facet_wrap(~Advisor)

# caption = "Histograms of the number of participants who experienced agreement rates for each advisor and presentation in the test trials. There were five trials per unique advisor-presentation combination per participant, so most participants have agreement rates neatly divisible by 5. Some participants had one or more trials excluded, meaning these participants's agreement rates for that combination may not be exactly divisible by 5."

```

How many participants would be excluded if we required 1 agreement and 1 disagreement trial in all 4 categories? 

```{r}

tmp %>% 
  mutate(okay = agreement > 0 & agreement < 1) %>%
  group_by(pid) %>%
  summarise(okay = all(okay)) %>%
  ungroup() %>%
  summarise(okay = mean(okay))

```

### Influence differences by agreement

```{r}
tmp <- AdvisedTrial %>% 
  group_by(pid, Advisor, advisor0agree) %>%
  summarise(influence = mean(advisor0influence)) %>%
  mutate(Agree = if_else(advisor0agree, "Agree", "Disagree"))

tmp %>% 
  ggplot(aes(x = Advisor, y = influence)) +
  geom_violin(alpha = .15, colour = NA, aes(fill = Advisor)) +
  geom_boxplot(fill = "white", outlier.color = NA, aes(colour = Advisor),
               width = .25, size = 1.25) +
  geom_line(alpha = .25, aes(group = pid)) + 
  scale_colour_discrete(h.start = 45) + 
  scale_fill_discrete(h.start = 45) +
  scale_y_continuous(limits = c(-200, 200)) +
  labs(y = 'Advice influence') +
  facet_wrap(~Agree)

# caption = "Influence of the advisors broken down by whether or not the advisor endorsed the same side as the participant selected in their intial estimate. Presentations are collapsed and both familiarisation and test trials are combined."

```

# Conclusions {.summary}

!TODO[Conclusions]

# Credits 

## Acknowledgements

Thanks as always to Nick Yeung and the other folks at the [ACC Lab](https://www.psy.ox.ac.uk/research/attention-cognitive-control-lab).

## R Packages

```{r results = 'asis'}
# list packages
packageNames <- (.packages())
# don't include very core package
packageNames <- packageNames[!(packageNames %in% 
                                 rownames(installed.packages(
                                   priority = "base")))]
# but do include the base package
packageNames <- c("base", packageNames)
out <- NULL
for (p in packageNames) {
  out <- rbind(out, data.frame('Package' = p, 
                               'Citations' = paste(format(citation(p), 
                                                          style = 'textVersion'), 
                                                   collapse = '<br/><br/>')))
}

as_tibble(out)
```

## Funding

Matt Jaquiery is funded by a studentship from the [Medical Research Council](https://mrc.ukri.org/) (reference 1943590) and the University of Oxford [Department of Experimental Psychology](https://www.psy.ox.ac.uk/) (reference 17/18_MSD_661552).

## Technical details  

```{r results = 'hold'}
cat(paste('Time stamp:', Sys.time(), '\n\n'))
cat('Runtime \n')
proc.time()
cat('\n')
sessionInfo()
```