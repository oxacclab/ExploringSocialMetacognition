---
title: "Confidence estimation"
author: "[Matt Jaquiery](https://github.com/mjaquiery) ([matt.jaquiery@psy.ox.ac.uk](mailto:matt.jaquiery@psy.ox.ac.uk))"
output:
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '3'
    css: ../src/writeUp.css
    includes:
      after_body: ../src/toc_menu.html
  html_notebook:
    toc: yes
    toc_depth: 3
    css: ../src/writeUp.css
    includes:
      after_body: ../src/toc_menu.html
editor_options:
  chunk_output_type: inline
---

December 2019

[Script run `r Sys.time()`]


```{r prematter, include = F}

library(tidyverse)
library(scales)
library(broom)
library(BayesFactor)
library(prettyMD)
library(knitr)
library(parallel)

opts_chunk$set('echo' = F)

set.seed(20191216)

# Plot setup
theme_set(theme_light() + 
            theme(panel.grid.major.x = element_blank()))

```

```{r loadData, include = F}

rDir <- "http://localhost/ExploringSocialMetacognition/data/public/"
testData <- F

studyVersion <- "0-0-6"
studyName <- "calibrationKnowledge"

exclude <- list(
  maxTrialRT = 60000,    # trials take < 1 minute
  minTrials = 11,        # at least 11 trials completed
  minChangeRate = .1     # some advice taken on 10%+ of trials
  ) 

skipLoadData <- F

source("src/02_Exclusions.R")

if (testData) {
  # randomise the confidence of test data
  source("src/sim-confidence-estimation.R")
  model <- simulateCE(
    AdvisedTrial,
    agentInsensitivitySD = 8,
    agentConfidence = 2,
    agentConfidenceSD = 4,
    agentEgoBias = .7, 
    agentEgoBiasSD = .2,
    agentEffectSize = .1,
    agentEffectSizeSD = .1
    )
  AdvisedTrial <- model$trials
  decisions <- byDecision(AdvisedTrial)
}

```

# Introduction

Observations from [evolutionary models](https://github.com/oxacclab/EvoEgoBias) show that egocentric discounting is a successful strategy in environments where advisors cannot be clear on how advisors' expressed confidence relates to their actual confidence. We reason that human participants may show a sensitivity to these contextual factors underlying advice-taking and respond to them in a rational manner. To test the effects of known versus unknown confidence representations, we manipulated whether participants knew which of two advisors was giving them advice. Participants performed a block of 9 trials to get acquainted with each of two advisors, one who had high confidence and one who had low confidence (both were equally accurate). The advisors introduced themselves exhibiting their confidence. Participants then performed 20 trials where no feedback was given and advice came from one of the two advisors they had previously encountered. In some of these trials the advice was labelled as coming from the advising advisor, in others it was shown as coming from an unspecified one of the two advisors.

We hypothesised that participants would show greater sensitivity to the confidence of advice when they knew the identify of the advisor.

# Method

The experimental code is available on [GitHub](https://github.com/oxacclab/ExploringSocialMetacognition), and the experiment can be performed by visiting [https://acclab.psy.ox.ac.uk/~mj221/ESM/ACBin/ce.html](https://acclab.psy.ox.ac.uk/~mj221/ESM/ACBin/ck.html?PROLIFIC_PID=WriteUp). 

# Results

## Exclusions

```{r exclusions}

if (testData) {
  print("NOTE: Testing (simulated) data!!!!")
}

tmp <- suppressWarnings(left_join(exclusions, okayIds, by = "pid"))

tmp$condition <- factor(tmp$condition, labels = c("highFirst",
                                                  "lowFirst"))

table(tmp$excluded, tmp$condition)

# Test accuracy of the labels
AdvisedTrial %>% 
  group_by(pid, block, advisor0idDescription) %>% 
  filter(block == 2) %>% 
  summarise(n()) %>% 
  left_join(tmp, by = 'pid') %>% 
  ungroup() %>%
  select(advisor0idDescription, condition) %>%
  mutate(ok = if_else(advisor0idDescription == 'lowConf',
                      condition == 'lowFirst',
                      condition == 'highFirst')) %>%
  pull(ok) %>%
  all() %>%
  expect_equal(T)

```

Our final participant list consists of `r length(unique(AdvisedTrial$pid))` participants who completed an average of `r num2str(mean(aggregate(advisor0 ~ pid, AdvisedTrial, length)$advisor0))` trials each, of which an average of `r num2str(mean(aggregate(advisor0 ~ pid, AdvisedTrial[!AdvisedTrial$feedback, ], length)$advisor0))` had no feedback.

## Task performance

First we offer a characterisation of the task, to provide the reader with a sense of how the participants performed. 

### Decisions

Participants offered estimates of whether various events took place before or after a given year. The correct answers were always between 1900 and 2000. Participants answered by selecting a point on one of two confidence bars, with the choice of bar indicating the chosen answer. 

#### Correctness

```{r accuracy}

decisions %>% peek(responseCorrect, decision) %>%
  num2str.tibble(isProportion = T, precision = 3)

decisions %>% filter(feedback == T) %>% 
  peek(responseCorrect, decision) %>% 
  num2str.tibble(isProportion = T, precision = 3) %>%
  cbind(feedback = T)

```

```{r accuracyGraph}

tmp <- decisions %>% group_by(pid, decision, feedback) %>%
  do(correct = mean_cl_normal(.$responseCorrect)) %>%
  unnest(correct) %>%
  transmute(pid, decision,
            pCorrect = y, 
            ciLow = ymin,
            ciHigh = ymax,
            feedback = if_else(feedback, "With Feedback", "No feedback"))

ggplot(tmp, 
       aes(x = decision, y = pCorrect)) +
  geom_violin(alpha = .15, colour = NA, aes(fill = decision)) +
  geom_boxplot(fill = "white", outlier.color = NA, aes(colour = decision),
               width = .25, size = 1.25) +
  geom_line(alpha = .25, aes(group = pid)) + 
  scale_y_continuous(limits = c(0, 1)) +
  labs(x = "decision", 
       y = "p(response correct)") +
  facet_grid(~feedback)

```

### Timing

We can look at the response time - the difference between the time the response is opened and the time the response is received.  

```{r time}

decisions$rt <- decisions$responseTimeEstimate - decisions$timeResponseOpen

decisions %>% peek(rt, decision) %>% 
  mutate_if(is.numeric, round)

```

```{r timeGraph}

tmp <- decisions %>% group_by(pid, decision) %>% 
  summarise(rt = mean(rt))

ggplot(tmp, 
       aes(x = decision, y = rt)) +
  geom_violin(alpha = .15, colour = NA, aes(fill = decision)) +
  geom_boxplot(fill = "white", outlier.color = NA, aes(colour = decision),
               width = .25, size = 1.25) +
  geom_line(alpha = .25, aes(group = pid)) + 
  labs(x = "decision", 
       y = "response time (ms)") 

```

### Summary {.summary}

Participants' initial decisions were only slightly better than chance. Their final decisions were substantially better, indicating a reasonably high rate of switching from one answer to the other following advice. The time taken to answer each question was consistent with previous experiments.

## Metacognitive performance

### Confidence

Each answer bar allowed the participant to express their confidence in that answer by selecting a higher point on the bar for a higher confidence, in a range of 0-100% for each decision.

```{r confidence}

decisions %>% peek(responseConfidence, decision) %>%
  num2str.tibble(isProportion = T, precision = 1)

decisions %>% filter(feedback == T) %>% 
  peek(responseConfidence, decision) %>% 
  num2str.tibble(isProportion = T, precision = 3) %>%
  cbind(feedback = T)

```

```{r confidence graph}

tmp <- decisions %>% group_by(pid, decision, feedback) %>%
  do(confidence = mean_cl_normal(.$responseConfidence)) %>%
  unnest(confidence) %>%
  transmute(pid, decision,
            confidence = y, 
            ciLow = ymin,
            ciHigh = ymax,
            feedback = if_else(feedback, "With Feedback", "No feedback"))

ggplot(tmp, 
       aes(x = decision, y = confidence)) +
  geom_violin(alpha = .15, colour = NA, aes(fill = decision)) +
  geom_boxplot(fill = "white", outlier.color = NA, aes(colour = decision),
               width = .25, size = 1.25) +
  geom_line(alpha = .25, aes(group = pid)) + 
  scale_y_continuous(limits = c(0, 100)) +
  labs(x = "decision", 
       y = "confidence") +
  facet_grid(~feedback)

```

### Relationship between confidence and p(correct)

Participants indicated the confidence in their decisions simultaneously with the decisions themselves. We can thus explore how well a participant's confidence signals their accuracy. We provide only a brief examination here where normalised responses are split into high and low confidence for each participant, and the accuracies of the two categories are compared. This is done separately for first and last decisions.

```{r confidence normalisation}

decisions <- decisions %>% 
  group_by(pid, decision) %>% 
  dplyr::mutate(zConf = scale(responseConfidence)) %>%
  ungroup() 

```

```{r confidence binary split}

tmp <- decisions %>% 
  mutate(highConf = zConf > 0) %>%
  group_by(pid, decision, highConf) %>%
  summarise(pCorrect = mean(responseCorrect)) %>%
  group_by(pid, decision) %>%
  spread(highConf, pCorrect) %>%
  mutate(pCorrectDiff = `TRUE` - `FALSE`)

tmp %>% 
  peek(pCorrectDiff, decision) %>% 
  num2str.tibble(isProportion = T, precision = 3)

print("p(correct|high confidence) - p(correct|low confidence) for initial estimates")
ttestBF(tmp$pCorrectDiff[tmp$decision == "first"])
print("p(correct|high confidence) - p(correct|low confidence) for final decisions")
ttestBF(tmp$pCorrectDiff[tmp$decision == "last"])

```

```{r confidence binary split plot}

ggplot(tmp, aes(x = decision, y = pCorrectDiff)) +
  geom_violin(alpha = .15, colour = NA, aes(fill = decision)) +
  geom_boxplot(fill = "white", outlier.color = NA, aes(colour = decision),
               width = .25, size = 1.25) +
  geom_line(alpha = .25, aes(group = pid)) + 
  labs(x = "decision", 
       y = "p(Correct | sure) - p(Correct | unsure)") +
  scale_y_continuous(limits = c(-1, 1))     
 
```

### Summary {.summary}

Participants' answers were generally around the midpoint on the scale, with a fair amount of variation between participants. Confidence in final decisions did not appear to be consistently higher than initial estimates, although this is unsurprising given the final decisions are not split by whether the advisor agreed with the initial decision.

The relationship between confidence and the probability of being correct is essentially zero for initial estimates, indicating that participants are guessing on initial decisions. Final decisions did show a relationship where higher confidence in decisions was indicative of a greater probability of being correct, presumably because participants' confidence was readily affected by advice.

## Advisor performance

Participants in should get an appropriate experience of each advisor in the feedback condition. This means that the accuracy and metacognitive sensitivity of the advisors should be balanced, while the confidence (metacognitive bias) is not. 

### Accuracy

```{r adviceAccuracy}

AdvisedTrial$advisor0accurate <- ifelse(
  is.na(AdvisedTrial[, paste0(advisorNames[1], ".accurate")]),
  pull(AdvisedTrial[, paste0(advisorNames[2], ".accurate")]),
  pull(AdvisedTrial[, paste0(advisorNames[1], ".accurate")])
)

AdvisedTrial$advisor0agree <- ifelse(
  is.na(AdvisedTrial[, paste0(advisorNames[1], ".agree")]),
  pull(AdvisedTrial[, paste0(advisorNames[2], ".agree")]),
  pull(AdvisedTrial[, paste0(advisorNames[1], ".agree")])
)

tmp <- AdvisedTrial %>% 
  filter(feedback) %>%
  group_by(pid, advisor0idDescription) %>%
  summarise(advisorAccuracy = mean(advisor0accurate),
            advisorAgree = mean(advisor0agree)) 

tmp %>% 
  group_by(advisor0idDescription) %>%
  summarise(advisorAccuracy = mean(advisorAccuracy),
            advisorAgree = mean(advisorAgree)) %>%
  num2str.tibble(isProportion = T, precision = 3)

```

```{r adviceAccuracyGraph}

tmp %>% 
  dplyr::rename(advisor = advisor0idDescription,
                `p(correct)` = advisorAccuracy,
                `p(agree)` = advisorAgree) %>%
  gather("var", "value", `p(correct)`:`p(agree)`) %>%
  ggplot(aes(x = advisor, y = value)) +
  geom_violin(alpha = .15, colour = NA, aes(fill = advisor)) +
  geom_boxplot(fill = "white", outlier.color = NA, aes(colour = advisor),
               width = .25, size = 1.25) +
  geom_line(alpha = .25, aes(group = pid)) + 
  labs(x = "advisor", 
       y = "mean") +
  scale_y_continuous(limits = c(0, 1)) + 
  facet_wrap(~var)
  
  
```

### Advisor confidence

The advisors differ by design in their confidence, so participants should experience these differences.

```{r advisor confidence}

tmp <- AdvisedTrial %>% 
  filter(feedback) %>%
  group_by(pid, advisor0idDescription) %>%
  summarise(confidence = mean(advisor0adviceConfidence))

tmp %>% 
  group_by(advisor0idDescription) %>%
  summarise(confidence = mean(confidence)) %>%
  num2str.tibble(precision = 1)


```

```{r advisor confidence graph}

tmp %>% 
  rename(advisor = advisor0idDescription) %>%
  ggplot(aes(x = advisor, y = confidence)) +
  geom_violin(alpha = .15, colour = NA, aes(fill = advisor)) +
  geom_boxplot(fill = "white", outlier.color = NA, aes(colour = advisor),
               width = .25, size = 1.25) +
  geom_line(alpha = .25, aes(group = pid)) + 
  scale_y_continuous(limits = c(0, 100)) +
  labs(y = 'mean advisor confidence')

```

#### Individual participants' experience

We should take a look at a handful of specific distributions experienced by individual participants to ensure the individual-level view tallies with the sample view. 

```{r advisor confidence by participant}

tmp <- AdvisedTrial %>%
  filter(feedback) %>%
  select(pid, 
         advisor = advisor0idDescription, 
         confidence = advisor0adviceConfidence) %>%
  nest(data = c(advisor, confidence)) %>%
  # sample_n(6) %>%
  unnest_legacy()

ggplot(tmp, aes(x = advisor, y = confidence)) +
  geom_violin() +
  geom_point(position = position_jitter(.1), alpha = .5) + 
  facet_wrap(~pid)
  
ggplot(tmp, aes(x = advisor, y = confidence)) +
  geom_violin() +
  geom_point(position = position_jitter(width = .1), alpha = .1)

```

### Distance

Distance is the continuous version of agreement - the difference along the confidence scale between the advice and the initial estimate. Where different scales are endorsed, the difference is the combination of how sure the participant was on one scale plus how sure the advisor was on the other.

```{r adviceDistance}

AdvisedTrial$advisor0distance <- ifelse(
  is.na(AdvisedTrial[, paste0(advisorNames[1], ".distance")]),
  pull(AdvisedTrial[, paste0(advisorNames[2], ".distance")]),
  pull(AdvisedTrial[, paste0(advisorNames[1], ".distance")])
)
AdvisedTrial$advisor0distanceFinal <- ifelse(
  is.na(AdvisedTrial[, paste0(advisorNames[1], ".distanceFinal")]),
  pull(AdvisedTrial[, paste0(advisorNames[2], ".distanceFinal")]),
  pull(AdvisedTrial[, paste0(advisorNames[1], ".distanceFinal")])
)

tmp <- AdvisedTrial %>% 
  filter(feedback) %>%
  group_by(pid, advisor0idDescription) %>%
  summarise(adviceDistance = mean(advisor0distance)) 

tmp %>% 
  group_by(advisor0idDescription) %>%
  summarise(adviceDistance = mean(adviceDistance)) %>%
  num2str.tibble(precision = 1)

```

```{r adviceDistanceGraph}

tmp %>% 
  rename(advisor = advisor0idDescription) %>%
  ggplot(aes(x = advisor, y = adviceDistance)) +
  geom_violin(alpha = .15, colour = NA, aes(fill = advisor)) +
  geom_boxplot(fill = "white", outlier.color = NA, aes(colour = advisor),
               width = .25, size = 1.25) +
  geom_line(alpha = .25, aes(group = pid)) + 
  scale_y_continuous(limits = c(0, 200))

```

### Summary {.summary}

The advisors' performances were within the expected ranges for both the probability and extent of agreement with the participants and the objective accuracy of their advice. The advisors didn't differ substantially from one another in any of these aspects.

### Advisors seen in final block

```{r final bock advice}
tmp <- AdvisedTrial %>% 
  filter(block == 4) %>% 
  mutate(advisor = paste0(advisor0idDescription, 
                          if_else(advisor0name == '?', '?', ''))) %>% 
  group_by(pid, advisor) %>% 
  summarise(n = n())

tmp %>% 
  spread(advisor, n)
```

### Influence

The measure of influence is the extent to which a participant's confidence moves in the expected direction as a function of the advice (increasing when agreed with, decreasing when disagreed with).

Influence is the primary outcome measure, and is thus expected to differ between advisors and feedback conditions.

```{r influence}

AdvisedTrial$advisor0influence <- 
  unlist(sapply(1:nrow(AdvisedTrial),
                function(i) 
                  AdvisedTrial[i, 
                               paste0(AdvisedTrial$advisor0idDescription[i], 
                                      ".influence")]))

tmp <- AdvisedTrial %>% 
  group_by(pid, advisor0idDescription, feedback) %>%
  summarise(advisorInfluence = mean(advisor0influence)) 

tmp %>% 
  group_by(advisor0idDescription, feedback) %>%
  summarise(advisorInfluence = mean(advisorInfluence)) %>%
  num2str.tibble(precision = 1)

```

```{r influenceGraph}

tmp %>% 
  rename(advisor = advisor0idDescription) %>%
  ggplot(aes(x = advisor, y = advisorInfluence)) +
  geom_violin(alpha = .15, colour = NA, aes(fill = advisor)) +
  geom_boxplot(fill = "white", outlier.color = NA, aes(colour = advisor),
               width = .25, size = 1.25) +
  geom_line(alpha = .25, aes(group = pid)) + 
  scale_y_continuous(limits = c(-100, 100)) +
  facet_grid(~feedback, labeller = label_both)

```

#### Influence distribution

It's good to keep a general eye on the distribution of weight-on-advice on a trial-by-trial basis. 

```{r influenceDistribution}

ggplot(AdvisedTrial, aes(advisor0influence)) + 
  geom_histogram() +
  facet_grid(feedback ~ advisor0idDescription, labeller = label_both)

```

#### Is initial accuracy or initial confidence a better predictor of influence?

```{r influence correlations}

tmp <- AdvisedTrial %>% 
  select(pid, responseCorrect, responseConfidence, advisor0influence) %>%
  nest(-pid) %>%
  mutate(
    accBF = map(data, ~ correlationBF(.x$responseCorrect, 
                                    .x$advisor0influence)),
    acc.BF = map(accBF, ~ exp(.@bayesFactor$bf)),
    conBF = map(data, ~ correlationBF(.x$responseConfidence, 
                                    .x$advisor0influence)),
    con.BF = map(conBF, ~ exp(.@bayesFactor$bf)),
    acc = map(data, ~ cor.test(as.numeric(.x$responseCorrect), 
                               .x$advisor0influence)), 
    acc = map(acc, tidy),
    con = map(data, ~ cor.test(.x$responseConfidence, .x$advisor0influence)), 
    con = map(con, tidy)
  ) %>% 
  unnest(acc, .sep = ".") %>%
  unnest(con, .sep = ".") %>%
  unnest(acc.BF, con.BF)

tmp <- bind_rows(
  tmp %>% 
    select_at(vars(starts_with('acc'))) %>%
    rename_all(~ str_replace(., 'acc\\.(.+)', '\\1')) %>%
    mutate(property = 'accuracy'),
  tmp %>% 
    select_at(vars(starts_with('con'))) %>%
    rename_all(~ str_replace(., 'con\\.(.+)', '\\1')) %>%
    mutate(property = 'confidence')
)

tmp %>% 
  group_by(property) %>%
  summarise_if(is.numeric, mean, na.rm = T)

```

### Summary {.summary}

The influence of the advisors (formally tested below) did not show obvious differences between the advisors. while many participants were slightly more influenced by the single advisor, the participants with the largest differences between advisors were influenced more by the mass advisor. As usual, the vast majority of individual trials showed almost no influence of advice; this is somewhat surprising given that participants' initial answers were barely more accurate than guessing.

## Hypothesis testing

The hypotheses being tested here are:  

1. Participants will place higher weight on the advice of the single consistent advisor.

```{r h1, results='asis'}

tmp <- AdvisedTrial %>% 
  dplyr::filter(feedback == F) %>%
  group_by(pid, advisor0idDescription) %>% 
  summarise(influence = mean(advisor0influence)) %>% 
  mutate(
    advisor0idDescription = str_extract(advisor0idDescription, "mass|single")
    ) %>%
  spread(advisor0idDescription, influence)

r <- md.ttest(tmp$single, tmp$mass, 
              c("*M*|single", "*M*|mass"), paired = T)
cat(r)

```

### Summary {.summary}

The null model in which the advisors' advice is the same is 5x more likely than the alternative given the data observed. It would be appropriate to conclude that this manipulation does not produce a meaningful difference in advice processing.

## Exploration

### Do people interpret lower-confidence agreement as supportive or counter?

According to a rational integration of advice, agreement, no matter how low in confidence, ought to increase one's confidence in one's decision. Nevertheless, when presented visually with two markers on a bar, participants may instinctively average within the bar, rather than adding the advisor's confidence to their own. We can test these two models of behaviour in our data.

_Note:_ we only plot a few graphs for an overview; we don't need 30 of them here.

```{r add or average}

capConf <- function(fc, ic) {
  ifelse(fc < 0, pmax(fc, -100), pmin(100, fc))
}

#' Model a participant as adding a weighted version of agreeing advice to their 
#' initial confidence. 
#' @param is initial side
#' @param ic initial confidence 
#' @param as advice side
#' @param ac advice confidence
#' @param w weighting of the advice relative to the decisions (0 for ignored; .5
#'   for even)
#'
#' @return vector of final decision confidence, where -ve values represent 
#'   switching side
add <- function(is, ic, as, ac, w) {
  # compensate for disagreement
  ac <- ifelse(is == as, ac, -ac)
  capConf(ic + (ac * w), ic)
}
avg <- function(is, ic, as, ac, w) {
  # compensate for disagreement
  ac <- ifelse(is == as, ac, -ac)
  capConf(((1 - w) * ic) + (ac * w), ic)
}

pidModel <- function(p, tbl) {
  require(tidyverse)
  d <- tbl %>%
    filter(pid == p) %>% 
    select(is = responseAnswerSide, ic = responseConfidence,
           as = advisor0adviceSide, ac = advisor0adviceConfidence,
           fs = responseAnswerSideFinal, fc = responseConfidenceFinal) %>%
    mutate(f = if_else(is == as, fc, -fc))
  
  tmp <- tibble(pid = p, x = seq(-10, 20, .01))
  tmp$add <- sapply(tmp$x, function(w) mean((d$f - add(d$is, d$ic, d$as, d$ac, w)) ^ 2))
  tmp$avg <- sapply(tmp$x, function(w) mean((d$f - avg(d$is, d$ic, d$as, d$ac, w)) ^ 2))
  
  tmp %>% gather("model", "mse", c(add, avg))
}

modelPlot <- function(m) {
  low <- m %>% 
    group_by(model) %>% 
    filter(mse == min(mse)) %>%
    group_by(pid, model) %>%
    summarise_all(mean) %>%
    mutate(mse = mse ^ .5)
  
  ggplot(m, aes(x, mse ^ .5, colour = model)) + 
    geom_rect(fill = 'grey', colour = NA, xmin = -Inf, xmax = 0, ymin = -Inf, ymax = Inf) +
    geom_rect(fill = 'grey', colour = NA, xmin = 1, xmax = Inf, ymin = -Inf, ymax = Inf) +
    geom_vline(aes(xintercept = x, colour = model), data = low, size = 1,
               linetype = 'dashed') +
    geom_hline(aes(yintercept = mse, colour = model), data = low, size = 1, 
               linetype = 'dashed') +
    geom_line(size = 1) + 
    geom_label(aes(label = sprintf('%.02f', x), x = x, 
                   y = -(max(m$mse ^ .5) / 10) * (model == 'avg')), 
               data = low, nudge_y = max(m$mse ^ .5)) +
    geom_label(aes(label = sprintf('%.02f', mse), y = mse, 
                   x = -(max(m$x) / 6) * (model == 'avg')),
               data = low, nudge_x = max(m$x)) +
    labs(caption = paste0("Participant ", unique(m$pid)),
         y = "Root mean squared error from model",
         x = "Weight of advice relative to initial estimate") 
}

cl <- makeCluster(detectCores() - 4)
clusterExport(cl, c("add", "avg", "capConf"))

fits <- parLapply(cl, unique(AdvisedTrial$pid), pidModel, tbl = AdvisedTrial)

stopCluster(cl)

fits %>% head(30) %>% walk(~ modelPlot(.) %>% print())

```

```{r add or average model property analysis}

fits <- bind_rows(fits)

tmp <- fits %>% 
  group_by(pid, model) %>%
  filter(mse == min(mse)) %>%
  summarise_all(mean)

tmp %>% 
  group_by(model) %>%
  summarise(okx = mean(x >= 0 & x <= 1))

tmp %>% 
  group_by(pid) %>%
  mutate(ok = x >= 0 & x <= 1,
         okboth = all(ok),
         okneither = all(!ok)) %>%
  select(-x, -ok) %>%
  spread(model, mse) %>%
  mutate(avgBest = if_else(avg <= add, T, F)) %>%
  ungroup() %>%
  summarise_if(is.logical, mean)

```

```{r show best fit models by participant}

ws <- tmp %>% 
  select(-mse) %>%
  spread(model, x)

d <- AdvisedTrial %>% 
  mutate(agree = if_else(is.na(single.agree | mass.agree), F, T)) %>%
  select(
    pid,
    agree,
    stim = stimHTML,
    anchor = anchorDate,
    ans = correctAnswer,
    side = correctAnswerSide,
    is = responseAnswerSide,
    ic = responseConfidence,
    as = advisor0adviceSide,
    ac = advisor0adviceConfidence,
    fs = responseAnswerSideFinal,
    fc = responseConfidenceFinal
    ) %>%
  left_join(ws, by = 'pid') %>%
  mutate(
    fc.target = if_else(is == fs, fc, -fc),
    fc.add = add(is, ic, as, ac, add),
    fc.avg = avg(is, ic, as, ac, avg)
  )

d

d %>% gather('model', 'prediction', c(fc.add, fc.avg, ic)) %>%
  group_by(pid) %>%
  group_map(
    ~ ggplot(data = .x, aes(x = fc.target, y = prediction, colour = model)) +
      geom_abline(intercept = 0, slope = 1, linetype = 'dashed', colour = 'black') +
      geom_point() +
      geom_smooth(method = 'lm', se = F) +
      scale_x_continuous(limits = c(-100, 100), expand = c(0, 0)) +
      scale_y_continuous(limits = c(-100, 100), expand = c(0, 0)) +
      scale_color_manual(values = c(hue_pal()(2), 'black')) +
      coord_fixed() +
      labs(caption = paste0('Participant ', .y$pid[1], 
                            '; w.add = ', round(.x$add[1], 2), 
                            '; w.avg = ', round(.x$avg[1], 2)))
  )

```

This pattern may be dominated by disagreeing trials, which have a more intuitive presentation where averaging produces a more add-like final answer. So let's just look at agreement trials. We also only want those trials where the participant didn't (rather inexplicably given agreement) change their mind. Again, we plot a few graphs to see how things look.

```{r add or average agree only}

cl <- makeCluster(detectCores() - 4)
clusterExport(cl, c("add", "avg", "capConf"))

tmp <- AdvisedTrial %>%
  mutate(agree = if_else(is.na(single.agree | mass.agree), F, T)) %>%
  filter(agree, responseAnswerSide == responseAnswerSideFinal)

fitsAgree <- parLapply(cl, unique(AdvisedTrial$pid), pidModel, 
                  tbl = tmp)

stopCluster(cl)

fitsAgree %>% head(30) %>% walk(~ modelPlot(.) %>% print())

fitsAgree <- bind_rows(fitsAgree)

tmp <- fitsAgree %>% 
  group_by(pid, model) %>%
  filter(mse == min(mse)) %>%
  summarise_all(mean)

tmp %>% 
  group_by(model) %>%
  summarise(okx = mean(x >= 0 & x <= 1))

compare <- tmp %>% 
  group_by(pid) %>%
  mutate(ok = x >= 0 & x <= 1,
         okboth = all(ok),
         okneither = all(!ok)) %>%
  select(-x, -ok) %>%
  spread(model, mse) %>%
  mutate(
    `avg-add` = avg - add,
    avgBest = if_else(avg <= add, T, F)
    )

compare %>%
  ungroup() %>%
  summarise_if(is.logical, mean)

```

```{r best fit models by participant}

ws <- tmp %>% 
  select(-mse) %>%
  spread(model, x)

d <- d %>%
  left_join(ws, by = 'pid', suffix = c('', '.agrOnly')) %>%
  mutate(
    fc.add.agrOnly = add(is, ic, as, ac, add.agrOnly),
    fc.avg.agrOnly = avg(is, ic, as, ac, avg.agrOnly)
  )

d %>% gather('model', 'prediction', c(fc.add.agrOnly, fc.avg.agrOnly, ic)) %>%
  filter(agree, is == fs) %>% 
  group_by(pid) %>%
  group_map(
    ~ ggplot(data = .x, aes(x = fc.target, y = prediction, colour = model)) +
      geom_abline(intercept = 0, slope = 1, linetype = 'dashed', colour = 'black') +
      geom_point() +
      geom_smooth(method = 'lm', se = F) +
      scale_x_continuous(limits = c(0, 100), expand = c(0, 0)) +
      scale_y_continuous(limits = c(0, 100), expand = c(0, 0)) +
      scale_colour_manual(values = c(hue_pal()(2), 'black')) +
      coord_fixed() +
      labs(caption = paste0('Participant ', .y$pid[1], 
                            '; w.add = ', round(.x$add.agrOnly[1], 2), 
                            '; w.avg = ', round(.x$avg.agrOnly[1], 2)))
  )

```

Another way we can look at this is to ask what the probability of increasing your confidence is, given you answer in the same direction and the advisor agreed with you. Rationally, you should always increase your confidence, but if you're doing visual averaging you'll always decrease your confidence. 

```{r rational response}

tmp <- d %>% filter(agree, is == fs, ic != fc) %>% 
  mutate(ihigh = ic > ac,
         increase = fc > ic) 

tmp %>%
  group_by(pid, ihigh) %>%
  summarise(increase = mean(increase)) %>%
  spread(ihigh, increase, sep = "=") %>%
  rename_at(vars(2:3), ~ paste0('p(increase|', ., ')'))

tmp %>%
  filter(ihigh) %>%
  group_by(pid) %>%
  summarise(increase = mean(increase)) %>%
  ggplot(aes(x = "", y = increase)) +
  geom_violin(colour = NA, fill = 'lightgrey', size = 0) +
  geom_boxplot(outlier.color = NA, width = .1, size = 2) +
  geom_point(aes(x = 1.12), size = 2, alpha = .5, 
             position = position_jitter(.02)) +
  labs(x = "", y = "p(confidence increases)")

tmp %>% 
  mutate(adviceConf = cut(ac, breaks = c(-1, 25, 50, 75, 101))) %>%
  ggplot(aes(x = ic, y = fc, colour = adviceConf)) +
  geom_abline(intercept = 0, slope = 1, linetype = 'dashed') +
  geom_smooth(aes(linetype = ihigh), method = 'lm', se = F) +
  geom_point(aes(shape = ihigh), size = 2) +
  scale_colour_discrete(name = "Advice confidence") +
  scale_shape_manual(values = c(4, 16), 
                     name = "Initial confidence > advice confidence") +
  scale_linetype_discrete(name = "Initial confidence > advice confidence") + 
  coord_fixed() +
  labs(x = "Initial estimate confidence",
       y = "Final decision confidence") +
  theme(legend.position = "top",
        panel.grid = element_blank())

tmp %>% 
  filter(ihigh) %>%
  mutate(difference = as.numeric((ic - ac) / ic)) %>%
  ggplot(aes(x = ic, y = fc, colour = difference)) +
  geom_abline(intercept = 0, slope = 1, linetype = 'dashed') +
  geom_smooth(method = 'lm', se = F) +
  geom_point(size = 4) +
  scale_colour_continuous(name = "(initial confidence - advice confidence) / initial confidence",
                          type = 'viridis') +
  coord_fixed() +
  labs(x = "Initial estimate confidence",
       y = "Final decision confidence") +
  theme(legend.position = "top",
        panel.grid = element_blank())

```

# Conclusions {.summary}

!TODO[Conclusions]

# Credits 

## Acknowledgements

Thanks as always to Nick Yeung and the other folks at the [ACC Lab](https://www.psy.ox.ac.uk/research/attention-cognitive-control-lab).

## R Packages

```{r results = 'asis'}
# list packages
packageNames <- (.packages())
# don't include very core package
packageNames <- packageNames[!(packageNames %in% 
                                 rownames(installed.packages(
                                   priority = "base")))]
# but do include the base package
packageNames <- c("base", packageNames)
out <- NULL
for (p in packageNames) {
  out <- rbind(out, data.frame('Package' = p, 
                               'Citations' = paste(format(citation(p), 
                                                          style = 'textVersion'), 
                                                   collapse = '<br/><br/>')))
}

kable(out)
```

## Funding

Matt Jaquiery is funded by a studentship from the [Medical Research Council](https://mrc.ukri.org/) (reference 1943590) and the University of Oxford [Department of Experimental Psychology](https://www.psy.ox.ac.uk/) (reference 17/18_MSD_661552).

## Technical details  

```{r results = 'hold'}
cat(paste('Time stamp:', Sys.time(), '\n\n'))
cat('Runtime \n')
proc.time()
cat('\n')
sessionInfo()
```