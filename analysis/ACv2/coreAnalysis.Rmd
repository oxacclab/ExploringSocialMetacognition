---
title: "Date estimation analysis"
author: "Matt Jaquiery (matt.jaquiery@psy.ox.ac.uk)"
output:
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '3'
  html_notebook:
    includes:
      after_body: ../src/toc_menu.html
    toc: yes
    toc_depth: 3
editor_options:
  chunk_output_type: inline
---

April 2019

[Script run `r Sys.time()`]


```{r prematter, include = F}

library(testthat)

library(tidyverse)

library(lsr)
library(BayesFactor)
library(BANOVA)

library(knitr)
library(prettyMD)

opts_chunk$set('echo' = F)

set.seed(20190425)

# Plot setup
theme_set(theme_light() + 
            theme(panel.grid.major.x = element_blank()))

```

```{r constants}

zThresh <- 3 # threshold for outliers

markerList <- list(thin = 1, medium = 3, wide = 9)

```

```{r specificFunctions}

markerPoints <- function(width) 27 / width

```

```{r miscFunctions}

#' strip newlines and html tags from string
stripTags <- function(s) {
  s <- gsub("[\r\n]", "", s)
  
  while (any(grepl("  ", s, fixed = T)))
    s <- gsub("  ", " ", s, fixed = T)
  
  s <- gsub("^ ", "", s, perl = T)
  s <- gsub(" $", "", s, perl = T)
  
  while (any(grepl("<([^\\s>]+)[^>]*>([\\s\\S]*?)<\\/\\1>", s, perl = T)))
    s <- gsub("<([^\\s>]+)[^>]*>([\\s\\S]*?)<\\/\\1>", "\\2", s, perl = T)
  
  s <- gsub("<[^>]+\\/>", "", s)
  s
}

#' Return the first match for a regexpr
reFirstMatch <- function(pattern, str, ...) {
  re <- regexpr(pattern, str, ..., perl = T)
  name <- substr(str, attr(re, "capture.start"), 
                 attr(re, "capture.start") + attr(re, "capture.length") - 1)
  name
}

expect_equal(reFirstMatch("\\w+\\W+(\\w+)", "First, Second, Third"), "Second")

#' rbind with NA padding for missing columns
#' @params x list of data frames to join
#' @params padWith value for missing entries
safeBind <- function(x, padWith = NA) {
  
  out <- NULL
  first <- T
  
  for (y in x) {
  
    if (!is.data.frame(y))
      y <- as.data.frame(y)
    
    if (first) {
      out <- y
      first <- F
    } else {
      y[, names(out)[names(out) %in% names(y) == F]] <- padWith
      out[, names(y)[names(y) %in% names(out) == F]] <- padWith
      out <- rbind(out, y)
    }
  }
  
  out
}

expect_equal(dim(safeBind(list(data.frame(x = 1:5, y = runif(5), rnorm(5)),
                                data.frame(x = 6:10, z = 1:5)))),
             c(10, 4))

#' List the unique values of a vector and a "total" item with all unique values
#' Designed for outputting aggregate counts and totals
uniqueTotal <- function(x) {
  out <- as.list(unique(x))
  out[[length(out) + 1]] <- unique(x)
  out
}

expect_equal(uniqueTotal(c("a", "b", "c")), 
             list("a", "b", "c", c("a", "b", "c")))
```

## Load data

```{r loadData}
version <- "0-0-14"
prefix <- paste0("datesStudy_v", version)

files <- list.files("../../data/public/", full.names = T)
files <- files[grepl(prefix, files, fixed = T)]

okayIds <- read.csv(paste0("../../data/public/", prefix, "_okayIds.csv"))

for (f in files) {
  tmp <- as.tibble(read.csv(f))
  
  # screen out non-okay ids
  if ("pid" %in% names(tmp))
    tmp <- tmp[tmp$pid %in% okayIds$pid[okayIds$okay], ]
  
  if ("stimHTML" %in% names(tmp)) {
    tmp$stimHTML <- stripTags(tmp$stimHTML)
  }
  
  # type coersion
  if ("comment" %in% names(tmp))
    tmp$comment <- as.character(tmp$comment)
  
  n <- grep("advisor[0-9]+(name|validTypes|nominalType|actualType)$", 
            names(tmp), value = T)
  for (x in n)
    tmp[, x] <- lapply(tmp[, x], as.character)
  
  n <- grep("responseEstimateLabel", names(tmp), value = T)
  for (x in n)
    tmp[, x] <- lapply(tmp[, x], function(y) 
      as.numeric(stripTags((as.character(y)))))
  
  if ("responseMarkerWidth" %in% names(tmp))
    tmp$responseMarker <- factor(tmp[["responseMarkerWidth"]])
  if ("responseMarkerWidthFinal" %in% names(tmp))
    tmp$responseMarkerFinal <- factor(tmp[["responseMarkerWidthFinal"]])
  
  # assign to workspace
  name <- reFirstMatch("([^_]+)\\.csv", f)
  name <- sub("-", ".", name)
  assign(name, tmp)
}


```

### Utility variables

```{r utilityVariables}

# Reference variables 
# Gather a list of advisor names and advice types

names <- NULL
types <- NULL
i <- 0
while (T) {
  if (!length(grep(paste0("advisor", i), names(AdvisedTrial)))) {
    break()
  }
  names <- unique(c(names, 
                    unique(AdvisedTrial[, paste0("advisor", i, 
                                                 "idDescription")])))
  types <- unique(c(types,
                    unique(AdvisedTrial[, paste0("advisor", i, 
                                                 "actualType")]),
                    unique(AdvisedTrial[, paste0("advisor", i,
                                                 "nominalType")])))
  i <- i + 1
}
advisorNames <- unlist(names)
advisorTypes <- unlist(types)


# Produce equivalents of the advisor1|2... variables which are named for the 
# advisor giving the advice

for (v in names(AdvisedTrial)[grepl("advisor0", names(AdvisedTrial))]) {
  suffix <- reFirstMatch("advisor0(\\S+)", v)
  for (a in advisorNames) {
    
    s <- paste0(a, ".", suffix)
    AdvisedTrial[, s] <- NA
    
    for (i in 1:nrow(AdvisedTrial)) {
      x <- 0
      while (T) {
        if (!length(grep(paste0("advisor", x), 
                                names(AdvisedTrial)))) {
          break()
        }
        
        if (AdvisedTrial[i, paste0("advisor", x, "idDescription")] == a) {
          AdvisedTrial[i, s] <- AdvisedTrial[i, paste0("advisor", x, suffix)]
          break()
        }
        
        x <- x + 1
      }
      
    }
  }
}

# Trials
AdvisedTrial$hasFeedback <- !is.na(AdvisedTrial$timeFeedbackOn)

AdvisedTrial$responseCorrect <- 
  AdvisedTrial$correctAnswer >= AdvisedTrial$responseEstimateLeft &
  AdvisedTrial$correctAnswer <= AdvisedTrial$responseEstimateLeft + 
  AdvisedTrial$responseMarkerWidth

AdvisedTrial$responseCorrectFinal <- 
  AdvisedTrial$correctAnswer >= AdvisedTrial$responseEstimateLeftFinal &
  AdvisedTrial$correctAnswer <= AdvisedTrial$responseEstimateLeftFinal + 
  AdvisedTrial$responseMarkerWidthFinal

Trial$responseCorrect <- 
  Trial$correctAnswer >= Trial$responseEstimateLeft &
  Trial$correctAnswer <= Trial$responseEstimateLeft + 
  Trial$responseMarkerWidth

AdvisedTrial$responseError <- abs(AdvisedTrial$correctAnswer - 
                            AdvisedTrial$responseEstimateLeft + 
                            (AdvisedTrial$responseMarkerWidth / 2))

AdvisedTrial$responseErrorFinal <- abs(AdvisedTrial$correctAnswer - 
                                         AdvisedTrial$responseEstimateLeftFinal 
                                       + (AdvisedTrial$responseMarkerWidthFinal 
                                          / 2))

AdvisedTrial$responseScore <- 
  ifelse(AdvisedTrial$responseCorrect, 
         27 / AdvisedTrial$responseMarkerWidth, 0)

AdvisedTrial$responseScoreFinal <- 
  ifelse(AdvisedTrial$responseCorrectFinal, 
         27 / AdvisedTrial$responseMarkerWidthFinal, 0)

AdvisedTrial$accuracyChange <- AdvisedTrial$responseCorrectFinal -
  AdvisedTrial$responseCorrect

AdvisedTrial$scoreChange <- AdvisedTrial$responseScoreFinal -
  AdvisedTrial$responseScore

AdvisedTrial$estimateLeftChange <- abs(AdvisedTrial$responseEstimateLeftFinal -
  AdvisedTrial$responseEstimateLeft)

AdvisedTrial$confidenceChange <- 
  (4 - as.numeric(AdvisedTrial$responseMarkerFinal)) -
  (4 - as.numeric(AdvisedTrial$responseMarker))

# Trials - advisor-specific variables
for (a in advisorNames) {
  # Accuracy
  AdvisedTrial[, paste0(a, ".accurate")] <- 
    (AdvisedTrial[, paste0(a, ".advice")] - 
       (AdvisedTrial[, paste0(a, ".adviceWidth")] / 2)) <= 
    AdvisedTrial[, "correctAnswer"] &
    (AdvisedTrial[, paste0(a, ".advice")] + 
       (AdvisedTrial[, paste0(a, ".adviceWidth")] / 2)) >= 
    AdvisedTrial[, "correctAnswer"]
  
  # Error
  AdvisedTrial[, paste0(a, ".error")] <- 
    abs(AdvisedTrial[, paste0(a, ".advice")] - AdvisedTrial[, "correctAnswer"])
  
  # Weight on Advice
  i <- AdvisedTrial[, "responseEstimateLeft"] + 
    (AdvisedTrial[, "responseMarkerWidth"] - 1) / 2
  f <- AdvisedTrial[, "responseEstimateLeftFinal"] + 
    (AdvisedTrial[, "responseMarkerWidthFinal"] - 1) / 2
  adv <- AdvisedTrial[, paste0(a, ".advice")]
  
  x <- ((f - i) / (adv - i))
  AdvisedTrial[, paste0(a, ".woaRaw")] <- x
  
  x[x < 0] <- 0
  x[x > 1] <- 1
  
  AdvisedTrial[, paste0(a, ".woa")] <- x
  
  # Agreement
  for (d in c("", "Final")) {
    minA <- AdvisedTrial[, paste0(a, ".advice")] - 
         (AdvisedTrial[, paste0(a, ".adviceWidth")] / 2)
    maxA <- AdvisedTrial[, paste0(a, ".advice")] + 
         (AdvisedTrial[, paste0(a, ".adviceWidth")] / 2)
    
    minP <- AdvisedTrial[, paste0("responseEstimateLeft", d)]
    maxP <- minP + AdvisedTrial[, paste0("responseMarkerWidth", d)]
    
    AdvisedTrial[, paste0(a, ".agree", d)] <- 
      ((minA >= minP) & (minA <= maxP)) | ((maxA >= minP) & (maxA <= minP))  
  }
  
  # Agreement change
  AdvisedTrial[, paste0(a, ".agreementChange")] <- 
  AdvisedTrial[, paste0(a, ".agreeFinal")] - 
  AdvisedTrial[, paste0(a, ".agree")]
}


# Advisors
advisors$meanPosition <- NA
for (i in 1:nrow(advisors)) {
  if (advisors$idDescription[i] == "Practice") {
    next()
  }
  tmp <- AdvisedTrial[AdvisedTrial$pid %in% advisors$pid[i], ]
  advisors$meanPosition[i] <- mean(tmp[[paste0(advisors$idDescription[i], ".position")]])
}
  


# Produce a data frame of the trials where each decision gets a unique row
decisions <- AdvisedTrial[, !grepl("^response(?=\\S+Final$)", 
                                   names(AdvisedTrial), 
                                   perl = T)]

decisions <- rbind(decisions, decisions)
decisions$decision <- sapply(1:nrow(decisions), 
                             function(x) 
                               if (x <= nrow(AdvisedTrial)) "first" else "last")

for (i in (nrow(AdvisedTrial) + 1):nrow(decisions)) {
  for (n in names(decisions)[grepl("^response", names(decisions), perl = T)]) {
    decisions[i, n] <- AdvisedTrial[i - nrow(AdvisedTrial), paste0(n, "Final")]
  }
}

```

### Exclusions

In the tables that follow, reasons for exclusion and their counts are provided. **FALSE** indicates that a trial or participant is not excluded. 

First, participants are excluded if they fail attention checks.

```{r exclusions}

exclusions <- tibble(pid = unique(decisions$pid))
exclusions$excluded <- F

for (p in unique(exclusions$pid)) {
  excluded <- NULL
  tmp <- Trial[Trial$pid == p, ]
  if (any(tmp$responseCorrect == F))
    excluded <- c(excluded, "attnCheckYear")
  if (any(tmp$responseMarkerWidth != 1))
    excluded <- c(excluded, "attnCheckMarker")
  
  exclusions$excluded[exclusions$pid == p] <- ifelse(is.null(excluded), 
                                                     F, 
                                                     paste(excluded, 
                                                           collapse = ", "))
}

table(exclusions$excluded)

# Drop excluded participants' trials
tmp <- NULL
for (i in 1:nrow(decisions))
  if (exclusions$excluded[exclusions$pid == decisions$pid[i]] == F)
    tmp <- rbind(tmp, decisions[i, ])

decisions <- tmp
```

Next, outlying trials are removed.

```{r outliers}

# Remove outlying trials

checkList <- c("timeEnd")
decisions$outlier <- F

for (v in checkList) {
  tmp <- scale(decisions[[v]])
  decisions$outlier[abs(tmp) > zThresh] <- 
    ifelse(decisions$outlier[abs(tmp) > zThresh] == F, 
           v, paste(decisions$outlier, v, collapse = ", "))
  
  if (any(abs(tmp) > zThresh))
    print(ggplot(decisions, aes(x = "", y = !!ensym(v))) +
            geom_violin(alpha = .25, color = NA, fill = "grey75") + 
            geom_boxplot(outlier.shape = NA, fill = NA) + 
            geom_point(position = position_jitter(.33), alpha = .5) +
            labs(x = "trials"))
}

tmp <- ggplot(decisions, aes(!!ensym(v))) + geom_histogram()

table(decisions$outlier)

```

Now outliers have been removed, participants who lost too many trials (>3) are also excluded. 

```{r outlyingTrialCount}

nMaxOutliers <- 2

for (p in unique(exclusions$pid)) {
  excluded <- exclusions$excluded[exclusions$pid == p]
  if (excluded == F) 
    excluded <- NULL
  if (length(decisions$pid[decisions$pid == p & decisions$outlier != F]) > 
      nMaxOutliers)
    excluded <- c(excluded, "outlyingTrials")
  
  exclusions$excluded[exclusions$pid == p] <- 
    if (is.null(excluded)) F else excluded
}

table(exclusions$excluded)

# Drop excluded participants' trials
tmp <- NULL
for (i in 1:nrow(decisions))
  if (exclusions$excluded[exclusions$pid == decisions$pid[i]] == F)
    tmp <- rbind(tmp, decisions[i, ])

decisions <- tmp

```

We are now ready to construct a data frame of participant averages.

```{r participantDataFrame}

# Participants data frame
ns <- c("timeEnd", "responseCorrect", "responseError", "number")
ss <- c("pid", "responseMarker", "hasFeedback", "decision")

eq <- paste0("cbind(", paste(ns, collapse = ", "), ") ~ ", 
             paste(ss, collapse = " + "))

PP <- as.tibble(aggregate(as.formula(eq), decisions, mean))
PP$excluded <- sapply(PP$pid, function(p) 
  exclusions$excluded[exclusions$pid == p])

# record the n of each row so weighted averaging can be used later
PP$number <- aggregate(as.formula(paste("number ~", 
                                        paste(ss, collapse = " +"))), 
                       decisions, length)$number

```

We can now check for participants who are outliers as a whole, and remove them. 

```{r participantExclusions}

checkList <- c("timeEnd", "responseError", "responseCorrect")

for (v in checkList) {
  p <- aggregate(as.formula(paste(v, "~ pid + hasFeedback")), 
                 AdvisedTrial, 
                 mean)
  p[, v] <- scale(p[, v])
  
  for (i in 1:nrow(p)) {
    if (abs(p[i, v] <= zThresh))
        next()
    
    exclusions$excluded[exclusions$pid == p$pid[i]] <- 
      if (exclusions$excluded[exclusions$pid == p$pid[i]] == F) v else
        paste(exclusions$excluded[PP$pid == p$pid[i]], ", ", v)
  }
  
  if (any(abs(p[, v]) > zThresh))
    print(ggplot(p, aes(x = "", y = !!ensym(v), colour = hasFeedback)) +
            geom_violin(alpha = .25, color = NA, fill = "grey75") + 
            geom_boxplot(outlier.shape = NA, fill = NA, aes(group = 1)) + 
            geom_point(position = position_jitter(.33), alpha = .5) +
            labs(x = "participants"))
}


PP$excluded <- sapply(PP$pid, 
                      function(x) exclusions$excluded[exclusions$pid == x])

table(exclusions$excluded)

```

Lastly, we can check the debrief information for participants who appeared to guess the manipulation (one advisor agrees with them) and remove those participants. 

```{r manualExclusions}

debrief.form$guessedManipulation <- F
  # c(F, F, F, F, F, T, F, F, F, F,
  #   T, T, F, F, F, F, F, F, F, F, 
  #   F, F, F, F, F, F, F, F, F, F)
debrief.form[, c("pid", "comment", "guessedManipulation")]

for (p in exclusions$pid) {
  if (p %in% debrief.form$pid) {
    if (debrief.form$guessedManipulation[debrief.form$pid == p]) {
      exclusions$excluded[exclusions$pid == p] <-
        if (exclusions$excluded[exclusions$pid == p] == F)
          "guessedManipulation" else
            paste(c(exclusions$excluded[exclusions$pid == p], 
                    "guessedManipulation"), collapse = ", ")
    }
  }
}

table(exclusions$excluded)
```

```{r doExclusions}

AdvisedTrial <- AdvisedTrial[AdvisedTrial$pid %in% 
                               exclusions$pid[exclusions$excluded == F], ]
PP <- PP[PP$pid %in% exclusions$pid[exclusions$excluded == F], ]

```

Our final participant list consists of `r length(unique(PP$pid))` participants who completed an average of `r num2str(mean(aggregate(number ~ pid, PP, function(x) sum(x) / 2)$number))` trials each. `r sum(aggregate(hasFeedback ~ pid, PP, any)$hasFeedback)` of these received feedback on the task, while the remaining `r sum(!aggregate(hasFeedback ~ pid, PP, any)$hasFeedback)` did not. 

## Task performance

```{r bindAdvisors}

# bind feedback property from participants
advisors <- advisors[advisors$pid %in% PP$pid, ]
advisors <- left_join(advisors, unique(PP[c("pid", "hasFeedback")]), "pid")

# drop practice advisors
advisors <- advisors[advisors$idDescription != "Practice", ]

```

```{r analysisPrep}

# Calculate the proportion of trials each breakdown in PP accounts for
PP$proportion <- sapply(1:nrow(PP), 
                        function(i) 
                          2 * PP$number[i] / 
                          sum(PP$number[PP$pid == PP$pid[i]]))

# Pad out the proportions with 0s
for (p in unique(PP$pid)) {
  for (d in unique(PP$decision))
    for (m in markerList)
      if (nrow(PP[PP$pid == p & 
                  PP$decision == d &
                  PP$responseMarker == m, ]) == 0)
        PP <- safeBind(list(PP, 
                            tibble(pid = p,
                                   responseMarker = m,
                                   hasFeedback = PP$hasFeedback[PP$pid == p][1],
                                   decision = d,
                                   number = 0,
                                   excluded = PP$excluded[PP$pid == p][1],
                                   proportion = 0)))
}

#' Means of v for each marker after converting df entries to participant means
#' @params v column
#' @params df dataframe containing v
markerBreakdown <- function(v, df, hideMarkerTotal = F, missingValue = NA, ...) {
  v <- substitute(v)
  
  fun <- function(x) {
    if (!nrow(x))
      return(missingValue)
    eq <- as.formula(paste(ensym(v), "~ + pid"))
    tmp <- aggregate(eq, x, mean, ...)
    mean(tmp[, ncol(tmp)])
  }
  
  # rename total fields
  n <- function(x, alt = NA) if (length(x) == 1) x else alt
  
  out <- list()
  for (d in uniqueTotal(df$decision)) {
    if (length(d) != 1)
      next()

    for (f in uniqueTotal(df$hasFeedback)) {
      tmp <- tibble(decision = n(d), feedback = n(f))
        
      for (m in uniqueTotal(markerList)) {
        if (length(m) != 1 && hideMarkerTotal)
          next()
        
        x <- fun(df[df$decision %in% d & 
                      df$hasFeedback %in% f &
                      df$responseMarker %in% m, ])
        
        if (is.na(n(m)))
          tmp$mean <- x
        else
          tmp[paste0("mean|m=", m)] <- x
      }
    
      out[[d]] <- rbind(out[[d]], tmp)
    }
  }
    
  out
}

#' Return a version of df with only the trials with a single advisor, 
#' and with all advice columns accessible as onlyAdvisor.x where x is the 
#' name of the advisor column.
#' @param df data frame to process
singleAdvisorTrials <- function(df) {
  # Find the number of advisors by counting advisorXadvice columns
  advCols <- grep("advisor[0-9]+advice$", names(df))
  df$advisorCount <- sapply(1:nrow(df), function(x) sum(!is.na(df[x, advCols])))
  
  # Only keep trials with a single advisor
  out <- df[df$advisorCount == 1, ]
  
  if (nrow(out) <= 0) return(NULL)
  
  # fill in advisor info for the single advisor on each trial
  for (v in grep(advisorNames[1], names(out))) {
    x <- reFirstMatch(paste0(advisorNames[1], "(.\\S+)$"), names(out)[v])
    y <- sapply(1:nrow(out), 
                function(i)
                  for (n in grep("\\.advice$", names(out))) {
                    if (!is.na(out[i, n]))
                      return(out[i, sub("^(\\S+)\\.advice$", 
                                           paste0("\\1", x), names(out)[n])])
                  })
    
    out[, paste0("onlyAdvisor", x)] <- unlist(y)
  }
  out
}
AdvisedTrial[, grep(".advice", names(AdvisedTrial))]
block2 <- singleAdvisorTrials(AdvisedTrial)
block2Decisions <- singleAdvisorTrials(decisions)
```

First we offer a characterisation of the task, to provide the reader with a sense of how the participants performed. 

The statistics for many of these are broken down as a cross-section of two factors, **decision** and **feedback**. **Decision** is a within-subjects variable, and indicates whether the judgement under consideration was the *first* (pre-advice) or *last* (post-advice) decision. **Feedback** is a between-subjects variable, and indicates whether the participant received feedback immediately following the last decision on a trial. Feedback allows participants to track the value of advice directly.

**Note:** *"first" and "last" are used as terms simply because they arrange the factors into alphabetical order with no messing about. Other terms would work equally well (e.g. initial/final is common in the literature).*

### Decisions

Participants offered estimates of the year in which various events took place. The correct answers were always between 1900 and 2000, although the timeline on which participants responded went from 1890 to 2010 in order to allow extra room for advice. Participants answered by dragging a marker onto a timeline. Markers of various widths were available for the participants to choose, with wider markers which covered more years being worth fewer points. Participants were informed that a correct answer was one in which the marker covered the year in which the event took place.

#### Marker usage

Three different markers were available: 

marker | years | points  
-------|------:|-------:
thin   | 3 | 9 | 
medium | 9 | 3 |
wide   | 27| 1 |

##### Table

These markers were used by the participants as described in the table below:

```{r markerUse}
  
tmp <- markerBreakdown(proportion, PP, hideMarkerTotal = T)

# Proportions within a row should sum to 1
for (x in tmp)
  expect_equal(apply(x[, 3:5], 1, sum), rep(1, nrow(x)))

num2str.tibble(tmp$first, isProportion = T, precision = 3)
num2str.tibble(tmp$last, isProportion = T, precision = 3)

```

**Marker usage summary table (means) for initial and final decisions**  
*Shows mean marker usage proportion for final and initial decisions for each feedback condition. Columns with NA represent totals across that variable.*  
*Data are aggregated within each participant before combination (and hence do not sum to 1). Except where otherwise mentioned, data presented will be in this manner - aggregations of individual participants' means.*

##### Graph

```{r markerGraph}

ggplot(PP[!is.na(PP$responseMarker), ], 
       aes(x = responseMarker, y = proportion)) +
  geom_violin(alpha = .25, colour = NA, fill = "grey75") +
  geom_boxplot(fill = NA, outlier.color = NA) +
  geom_line(alpha = .5, aes(colour = pid, group = pid)) + 
  geom_point(alpha = .5, aes(colour = pid)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  facet_grid(hasFeedback ~ decision, labeller = label_both) +
  scale_linetype_manual(values = c("dashed")) + 
  labs(x = "response marker width (years)", 
       y = "p(marker used)")

```

**Marker usage graph**
*Shows the proportion of marker usage for each participant by decision and feedback status.*

#### Correctness

Responses are regarded as **correct** if the target year is included within the marker range.

```{r accuracy}

tmp <- markerBreakdown(responseCorrect, decisions)
num2str.tibble(tmp$first, isProportion = T, precision = 3)
num2str.tibble(tmp$last, isProportion = T, precision = 3)

```

```{r accuracyGraph}

ggplot(aggregate(responseCorrect ~ 
                   responseMarker + decision + hasFeedback + pid,
                 decisions, mean), 
       aes(x = responseMarker, y = responseCorrect)) +
  geom_violin(alpha = .25, colour = NA, fill = "grey75") +
  geom_boxplot(fill = NA, outlier.color = NA) +
  geom_line(alpha = .5, aes(colour = pid, group = pid)) + 
  geom_point(alpha = .5, aes(colour = pid)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  facet_grid(hasFeedback ~ decision, labeller = label_both) +
  scale_linetype_manual(values = c("dashed")) + 
  labs(x = "response marker width (years)", 
       y = "p(response correct)")

```

#### Error (estimate mean)

The **error** is calcualted as the distance from the centre of the answer marker to the correct year. It is thus possible for **correct** answers to have non-zero error, and it is likely that the error for correct answers scales with the marker size.

```{r err}

tmp <- markerBreakdown(responseError, decisions)
num2str.tibble(tmp$first)
num2str.tibble(tmp$last)

```

```{r errGraph}

ggplot(aggregate(responseError ~ 
                   responseMarker + decision + hasFeedback + pid,
                 decisions, mean), 
       aes(x = responseMarker, y = responseError)) +
  geom_violin(alpha = .25, colour = NA, fill = "grey75") +
  geom_boxplot(fill = NA, outlier.color = NA) +
  geom_line(alpha = .5, aes(colour = pid, group = pid)) + 
  geom_point(alpha = .5, aes(colour = pid)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  facet_grid(hasFeedback ~ decision, labeller = label_both) +
  scale_linetype_manual(values = c("dashed")) + 
  labs(x = "response marker width (years)", 
       y = "|target - response marker centre| (years)")

```

##### Single-advisor trials

###### Table

```{r errBlock2}
  
tmp <- markerBreakdown(responseError, block2Decisions)
num2str.tibble(tmp$first)
num2str.tibble(tmp$last)

```

###### Graph

```{r errGraphBlock2}

ggplot(aggregate(responseError ~ 
                   responseMarker + decision + hasFeedback + pid,
                 block2Decisions, mean), 
       aes(x = responseMarker, y = responseError)) +
  geom_violin(alpha = .25, colour = NA, fill = "grey75") +
  geom_boxplot(fill = NA, outlier.color = NA) +
  geom_line(alpha = .5, aes(colour = pid, group = pid)) + 
  geom_point(alpha = .5, aes(colour = pid)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  facet_grid(hasFeedback ~ decision, labeller = label_both) +
  scale_linetype_manual(values = c("dashed")) + 
  labs(x = "response marker width (years)", 
       y = "|target - response marker centre| (years)")

```

#### Score

Points are scored only on **correct** trials. The points scored for a correct trial are equal to 27/marker width, meaning that the wider a marker is the fewer points are scored. 

```{r score}

tmp <- markerBreakdown(responseScore, decisions)
num2str.tibble(tmp$first)
num2str.tibble(tmp$last)

```

```{r scoreGraph}

ggplot(aggregate(responseScore ~ 
                   responseMarker + decision + hasFeedback + pid,
                 decisions, mean), 
       aes(x = responseMarker, y = responseScore)) +
  geom_violin(alpha = .25, colour = NA, fill = "grey75") +
  geom_boxplot(fill = NA, outlier.color = NA) +
  geom_line(alpha = .5, aes(colour = pid, group = pid)) + 
  geom_point(alpha = .5, aes(colour = pid)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  facet_grid(hasFeedback ~ decision, labeller = label_both) +
  scale_linetype_manual(values = c("dashed")) + 
  labs(x = "response marker width (years)", 
       y = "points scored")

```

#### Metacognitive performance

We cannot calculate calibration using p(correct), because the marker width serves as the indicator for confidence and changing marker also changes p(correct) by increasing the range of values considered correct. 

We can, however, use one of the following: 

  * error at different marker widths as an indicator of the degree to which increased accuracy is associated with increased precision  
  
  * p(correct|3yr), indicating whether the answer would have been correct had the 3yr marker been used centred in the same place  

### Timing

Firstly, we can consider the time taken for the entire trial.

```{r timeTotal}

tmp <- markerBreakdown(timeEnd, decisions)
num2str.tibble(tmp$last)

```

Second we can look at the response time - the difference between the time the response is opened and the time the response is received.  

```{r timeTotalGraph}

ggplot(aggregate(timeEnd ~ 
                   responseMarker + decision + hasFeedback + pid,
                 decisions, mean), 
       aes(x = responseMarker, 
           y = timeEnd / 1000)) +
  geom_violin(alpha = .25, colour = NA, fill = "grey75") +
  geom_boxplot(fill = NA, outlier.color = NA) +
  geom_line(alpha = .5, aes(colour = pid, group = pid)) + 
  geom_point(alpha = .5, aes(colour = pid)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  facet_grid(~ hasFeedback, labeller = label_both) +
  scale_linetype_manual(values = c("dashed")) + 
  labs(x = "response marker width (years)", 
       y = "response time (s)")

```

```{r time}

decisions$rt <- decisions$responseTimeEstimate - decisions$timeResponseOpen

tmp <- markerBreakdown(rt, decisions)
num2str.tibble(tmp$first)
num2str.tibble(tmp$last)

```

```{r timeGraph}

ggplot(aggregate(rt ~ 
                   responseMarker + decision + hasFeedback + pid,
                 decisions, mean), 
       aes(x = responseMarker, 
           y = rt / 1000)) +
  geom_violin(alpha = .25, colour = NA, fill = "grey75") +
  geom_boxplot(fill = NA, outlier.color = NA) +
  geom_line(alpha = .5, aes(colour = pid, group = pid)) + 
  geom_point(alpha = .5, aes(colour = pid)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  facet_grid(hasFeedback ~ decision, labeller = label_both) +
  scale_linetype_manual(values = c("dashed")) + 
  labs(x = "response marker width (years)", 
       y = "response time (s)")

```

## Advisor performance

We want to know how the advisors behave. They are programmed to be different, but the actual advice they can offer is limited by the circumstances of a trial. If, for instance, they are instructed to *agree and be correct*, this is only possible if the difference between the edge of the initial response marker and the correct answer is less than the advisor's precision.

### Descriptives

Advice consists in the placement of a marker on the timeline, similar to how participants make their decisions. Advice is classified according to two key properties:  

* **Agreement** occurs when there is at least one year of overlap between the participant's marker and the advisor's marker.  

* **Accuracy**. Advice is said to be accurate when the advisor's marker touches the target year.

It is possible for advice to be both accurate and agreeing, to be one but not the other, or to be neither. Each of these cases are expected to occur within the study for most participants, according to most advice profiles.

**Advice profiles** determine the kinds of advice an advisor attempts to provide. They specify relative quantities of advice rules, and are selected from a exhaustible pool. In cases where the selected advice rules cannot be fulfilled (e.g.*agree and be correct* where the participant's answer is too far from the correct answer), a **fallback** rule set is invoked. 

The following advice types are available:

name      | description     | fallback
:--------:|:----------------|:---------------  
**Correct** | The advisor gives a correct answer | *none*
**Agree** | The advisor gives an agreeing answer | *none*
**Correct Agree** | The advisor gives advice which is both correct and agreeing | Agree
**Correct Disagree** | The advisor gives advice which is correct, but which does not agree | Correct
**Disagree Reflected** | The advisor gives advice which is the participant's answer reflected in the correct answer, while disagreeing with the participant | Disagree Reversed
**Disagree Reversed** | The advisor gives advice which is the correct answer reflected in the participant's answer, while disagreeing with the participant | *always possible if Disagree Reflected is not*

#### Advice profile order

The advisors appear in different orders across blocks. The first advisor (position 0) is introduced first, appears above the second, and on gives its advice first on each trial. The order is randomised each block, and thus on average should balance out to around 0.5 for each advisor. 

```{r adviceOrder} 

as.tibble(aggregate(meanPosition ~ idDescription + hasFeedback, advisors, mean))

```

#### Advice offered

The advice offered, both nominal and actual, should be equivalent between feedback conditions.

The **nominal type** of the advice is the advice selected for the advisor to give.
```{r adviceRequested}

out <- list()
for (f in unique(AdvisedTrial$hasFeedback)) {
  m <- AdvisedTrial$hasFeedback == f
    
  tmp <- NULL
  for (a in advisorNames) {
    r <- tibble(feedback = f, advisor = a)
    for (x in adviceTypes) {
      eq <- as.formula(paste0(a, ".nominalType ~ pid"))
      r[, x] <- mean(aggregate(eq, 
                               AdvisedTrial[m, ], 
                               function(q) mean(q == x))[, 2])
    }
    tmp <- rbind(tmp, r)
  }

  out[[as.character(f)]] <- tmp
}

prop2str(out$`TRUE`, precision = 3)
prop2str(out$`FALSE`, precision = 3)

```

The **actual type** of advice is the advice the advisor actually gave, i.e. allowing for fallbacks where the requested advice type could not be supplied. 

```{r adviceGiven}

out <- list()
for (f in unique(AdvisedTrial$hasFeedback)) {
  m <- AdvisedTrial$hasFeedback == f
 
  tmp <- NULL
  for (a in advisorNames) {
    r <- tibble(feedback = f, advisor = a)
    for (x in adviceTypes) {
      eq <- as.formula(paste0(a, ".actualType ~ pid"))
      r[, x] <- mean(aggregate(eq, 
                               AdvisedTrial[m, ], 
                               function(q) mean(q == x))[, 2])
    }
    tmp <- rbind(tmp, r)
  }
  
  out[[as.character(f)]] <- tmp
}

prop2str(out$`TRUE`, precision = 3)
prop2str(out$`FALSE`, precision = 3)

```

### Accuracy

Advisors are supposed to differ in their accuracy. This is expected to hold true whether accuracy is cast in terms of the proportion of advice which is correct or mean error. These values should also be stable between feedback conditions.

```{r adviceAccuracy}

tmp <- NULL
for (a in advisorNames) {
  eq <- as.formula(paste0("cbind(", a, ".accurate, ", 
                          a, ".error) ~ pid + hasFeedback"))
  r <- aggregate(eq, AdvisedTrial, mean, na.rm = T)
  
  colnames(r) <- c("pid", "feedback", "accuracy", "error")
  r$advisor = a
  
  tmp <- rbind(tmp, as.tibble(r))
}

prop2str(as.tibble(aggregate(cbind(accuracy, error) ~ advisor + feedback, 
                             tmp, 
                             mean, na.rm = T)), 
         precision = 3)

```

```{r adviceAccuracyGraph}

tmp <- gather(tmp, "var", "value", accuracy:error)

ggplot(tmp, aes(x = advisor, y = value, colour = pid)) +
  geom_violin(colour = NA, fill = "grey75", alpha = .25) +
  geom_boxplot(outlier.colour = NA, fill = NA, aes(group = advisor)) +
  geom_line(alpha = .5, aes(colour = pid, group = pid)) + 
  geom_point(alpha = .5, aes(colour = pid)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  facet_grid(var ~ feedback, scales = "free_y", labeller = label_both)

```

### Agreement

Advisor are also supposed to differ in their agreement rates.

```{r adviceAgreement}

tmp <- NULL
for (a in advisorNames) {
  eq <- as.formula(paste0(a, ".agree ~ pid + hasFeedback"))
  r <- aggregate(eq, AdvisedTrial, mean, na.rm = T)
  
  colnames(r) <- c("pid", "feedback", "agreement")
  r$advisor <- a
  tmp <- rbind(tmp, r)
}

prop2str(as.tibble(aggregate(agreement ~ advisor + feedback, 
                             tmp, mean, na.rm = T)), 
         precision = 3)

```

```{r adviceAgreementGraph}

ggplot(tmp, aes(x = advisor, y = agreement, colour = pid)) +
  geom_violin(colour = NA, fill = "grey75", alpha = .25) +
  geom_boxplot(outlier.colour = NA, fill = NA, aes(group = advisor)) +
  geom_line(alpha = .5, aes(colour = pid, group = pid)) + 
  geom_point(alpha = .5, aes(colour = pid)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  facet_wrap(~feedback, labeller = label_both)

```

### Influence

The measure of influence is weight-on-advice. This is well-defined for values between 0 and 1 (trucated otherwise), and is
$$\text{WoA} = (\text{final} - \text{inital}) / (\text{advice} - \text{initial})$$
, or the degree to which the final decision moves towards the advised answer.

Influence is the primary outcome measure, and is thus expected to differ between advisors and feedback conditions.

```{r woa}

tmp <- NULL
for (a in advisorNames) {
  eq <- as.formula(paste0(a, ".woa ~ pid + hasFeedback"))
  r <- aggregate(eq, AdvisedTrial, mean, na.rm = T)
  
  colnames(r) <- c("pid", "feedback", "WoA")
  r$advisor <- a
  tmp <- rbind(tmp, r)
}

prop2str(as.tibble(aggregate(WoA ~ advisor + feedback, tmp, mean, na.rm = T)), 
         precision = 3)

```

```{r woaGraph}

ggplot(tmp, aes(x = advisor, y = WoA, colour = pid)) +
  geom_violin(colour = NA, fill = "grey75", alpha = .25) +
  geom_boxplot(outlier.colour = NA, fill = NA, aes(group = advisor)) +
  geom_line(alpha = .5, aes(colour = pid, group = pid)) + 
  geom_point(alpha = .5, aes(colour = pid)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  facet_wrap(~feedback, labeller = label_both)

```

##### Single-advisor trials

###### Table

```{r woaBlock2}
  
tmp <- NULL
for (a in advisorNames) {
  x <- block2[block2$onlyAdvisor.idDescription == a, ]
  
  eq <- as.formula(paste0(a, ".woa ~ pid + hasFeedback"))
  r <- aggregate(eq, x, mean, na.rm = T)
  
  colnames(r) <- c("pid", "feedback", "WoA")
  r$advisor <- a
  tmp <- rbind(tmp, r)
}

prop2str(as.tibble(aggregate(WoA ~ advisor + feedback, tmp, mean, na.rm = T)), 
         precision = 3)

```

###### Graph

```{r woaGraphBlock2}

ggplot(tmp, aes(x = advisor, y = WoA, colour = pid)) +
  geom_violin(colour = NA, fill = "grey75", alpha = .25) +
  geom_boxplot(outlier.colour = NA, fill = NA, aes(group = advisor)) +
  geom_line(alpha = .5, aes(colour = pid, group = pid)) + 
  geom_point(alpha = .5, aes(colour = pid)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  facet_wrap(~feedback, labeller = label_both)

```

##### WoA distribution

It's good to keep a general eye on the distribution of weight-on-advice on a trial-by-trial basis. 

```{r woaDistribution}

low <- 0
high <- 1
n <- 11
block2$woa <- ""
block2$woa[block2$onlyAdvisor.woaRaw >= 1] <- ">=1"
for (x in rev(seq(low, high, length.out = n))) {
  block2$woa[block2$onlyAdvisor.woaRaw < x] <- paste0("<", x)
}
block2$woa <- factor(block2$woa)

block2 <- block2[!is.nan(block2$onlyAdvisor.woaRaw), ]

ggplot(block2, aes(woa)) + 
  geom_histogram(stat = "count") +
  facet_grid(hasFeedback ~ onlyAdvisor.idDescription, labeller = label_both)

```

#### Accuracy changes

Participants may benefit from advice in terms of the accuracy of their responses. Here we code a correct response ammended to an incorrect response as -1, responses whose correctness is unchanged as 0, and incorrect responses ammended to correct responses as 1. 

```{r accuracyChanges}

tmp <- aggregate(accuracyChange ~ pid + hasFeedback, 
                 AdvisedTrial, mean, na.rm = T)

num2str(as.tibble(aggregate(accuracyChange ~ hasFeedback, 
                            tmp, mean, na.rm = T)))

```

```{r accuracyChangeGraph}

ggplot(tmp, aes(x = hasFeedback, y = accuracyChange, colour = pid)) + 
  geom_violin(alpha = .25, colour = NA, fill = "grey75") + 
  geom_boxplot(fill = NA, outlier.color = NA, aes(group = hasFeedback)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  geom_point(alpha = .5, aes(colour = pid))

```

##### Single-advisor trials

###### Table

```{r accuracyChangesBlock2}

tmp <- aggregate(accuracyChange ~ pid + hasFeedback + onlyAdvisor.idDescription, 
                 block2, mean, na.rm = T)

num2str(as.tibble(aggregate(accuracyChange ~ 
                              hasFeedback + onlyAdvisor.idDescription, 
                            tmp, mean, na.rm = T)))

```

##### Graph

```{r accuracyChangeGraphBlock2}

ggplot(tmp, 
       aes(x = onlyAdvisor.idDescription, y = accuracyChange, colour = pid)) + 
  geom_violin(alpha = .25, colour = NA, fill = "grey75") + 
  geom_boxplot(fill = NA, outlier.color = NA, 
               aes(group = onlyAdvisor.idDescription)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  geom_point(alpha = .5, aes(colour = pid)) +
  facet_wrap(~hasFeedback, labeller = label_both)

```

#### Score changes

Score changes are very straightforward - simply the points awarded for the final response minus the points awarded for the initial response, so positive values indicate improvements in score while negative values indicate the final response was less valuable than the initial response.

```{r scoreChange}

tmp <- aggregate(scoreChange ~ pid + hasFeedback, AdvisedTrial, mean, na.rm = T)

num2str(as.tibble(aggregate(scoreChange ~ hasFeedback, tmp, mean, na.rm = T)))

```

```{r scoreChangeGraph}

ggplot(tmp, aes(x = hasFeedback, y = scoreChange, colour = pid)) + 
  geom_violin(alpha = .25, colour = NA, fill = "grey75") + 
  geom_boxplot(fill = NA, outlier.color = NA, aes(group = hasFeedback)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  geom_point(alpha = .5, aes(colour = pid))

```

##### Single-advisor trials

###### Table

```{r scoreChangesBlock2}

tmp <- aggregate(scoreChange ~ pid + hasFeedback + onlyAdvisor.idDescription, 
                 block2, mean, na.rm = T)

num2str(as.tibble(aggregate(scoreChange ~ 
                              hasFeedback + onlyAdvisor.idDescription, 
                            tmp, mean, na.rm = T)))

```

##### Graph

```{r scoreChangeGraphBlock2}

ggplot(tmp, aes(x = onlyAdvisor.idDescription, y = scoreChange, colour = pid)) + 
  geom_violin(alpha = .25, colour = NA, fill = "grey75") + 
  geom_boxplot(fill = NA, outlier.color = NA, 
               aes(group = onlyAdvisor.idDescription)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  geom_point(alpha = .5, aes(colour = pid)) +
  facet_wrap(~hasFeedback, labeller = label_both)

```

#### Mean shifts

It can be helpful to get a sense of how much participants are adjusting their responses in general, i.e. the mean distance between their first and last responses. 

```{r meanShift}

tmp <- aggregate(estimateLeftChange ~ pid + hasFeedback, 
                 AdvisedTrial, mean, na.rm = T)

num2str(as.tibble(aggregate(estimateLeftChange ~ hasFeedback, 
                            tmp, mean, na.rm = T)))

```

```{r meanShiftGraph}

ggplot(tmp, aes(x = hasFeedback, y = estimateLeftChange, colour = pid)) + 
  geom_violin(alpha = .25, colour = NA, fill = "grey75") + 
  geom_boxplot(fill = NA, outlier.color = NA, aes(group = hasFeedback)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) + 
  geom_point(alpha = .5, aes(colour = pid))

```

#### Single advisor trials

##### Table

```{r meanShiftBlock2}

tmp <- aggregate(estimateLeftChange ~ 
                   pid + hasFeedback + onlyAdvisor.idDescription, 
                 block2, mean, na.rm = T)

num2str(as.tibble(aggregate(estimateLeftChange ~ 
                              hasFeedback + onlyAdvisor.idDescription, 
                            tmp, mean, na.rm = T)))

```

###### Graph

```{r meanShiftGraphBlock2}

ggplot(tmp, 
       aes(x = onlyAdvisor.idDescription, y = estimateLeftChange, colour = pid)) + 
  geom_violin(alpha = .25, colour = NA, fill = "grey75") + 
  geom_boxplot(fill = NA, outlier.color = NA, 
               aes(group = onlyAdvisor.idDescription)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) + 
  geom_point(alpha = .5, aes(colour = pid)) +
  facet_wrap(~hasFeedback, labeller = label_both)

```

#### Confidence changes

Marker widths are coded as 3, 2, and 1 for 3, 9, and 27 years respectively. Final - initial confidence gives the change. 

```{r confidenceChange}

tmp <- aggregate(confidenceChange ~ pid + hasFeedback, 
                 AdvisedTrial, mean, na.rm = T)

num2str(as.tibble(aggregate(confidenceChange ~ hasFeedback, 
                            tmp, mean, na.rm = T)))

```

```{r confidenceChangeGraph}

ggplot(tmp, aes(x = hasFeedback, y = confidenceChange, colour = pid)) + 
  geom_violin(alpha = .25, colour = NA, fill = "grey75") + 
  geom_boxplot(fill = NA, outlier.color = NA, aes(group = hasFeedback)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  geom_point(alpha = .5, aes(colour = pid))

```

#### Single advisor trials

##### Table

```{r confidenceShiftBlock2}

tmp <- aggregate(confidenceChange ~ 
                   pid + hasFeedback + onlyAdvisor.idDescription, 
                 block2, mean, na.rm = T)

num2str(as.tibble(aggregate(confidenceChange ~ 
                              hasFeedback + onlyAdvisor.idDescription, 
                            tmp, mean, na.rm = T)))

```

###### Graph

```{r confidenceShiftGraphBlock2}

ggplot(tmp, 
       aes(x = onlyAdvisor.idDescription, y = confidenceChange, colour = pid)) + 
  geom_violin(alpha = .25, colour = NA, fill = "grey75") + 
  geom_boxplot(fill = NA, outlier.color = NA, 
               aes(group = onlyAdvisor.idDescription)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) + 
  geom_point(alpha = .5, aes(colour = pid)) +
  facet_wrap(~hasFeedback, labeller = label_both)

```

#### Agreement changes

Advisor agreement is calculated using the participant's initial decision and the advice. In the same way, it is possible to calculate agreement between the advice and the participant's final decision, and to examine whether this changes (i.e. whether the participant shifts to follow disagreeing advice).

```{r agreementChange}

eq <- NULL
for (a in advisorNames) {
  eq <- c(eq, paste0(a, ".agreementChange"))
}

eq <- paste0("cbind(", paste(eq, collapse = ", "), ") ~ pid + hasFeedback")

tmp <- aggregate(as.formula(eq), AdvisedTrial, mean, na.rm = T)

tmp <- gather(tmp, "advisor", "agreementChange", 
              grep("\\.agreementChange", names(tmp)))
tmp$advisor <- sapply(tmp$advisor, function(s) reFirstMatch("([^\\.]+)", s))

num2str(as.tibble(aggregate(agreementChange ~ hasFeedback, 
                            tmp, mean, na.rm = T)))

```

```{r agreementChangeGraph}

ggplot(tmp, aes(x = advisor, y = agreementChange, colour = pid)) + 
  geom_violin(alpha = .25, colour = NA, fill = "grey75") + 
  geom_boxplot(fill = NA, outlier.color = NA, aes(group = advisor)) +
  geom_line(alpha = .5, aes(colour = pid, group = pid)) + 
  geom_point(alpha = .5, aes(colour = pid)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  facet_wrap(~hasFeedback, labeller = label_both)

```

## Hypothesis testing

The hypotheses being tested here are:  

1. Participants benefit from advice 

    a. when feedback is available
    
    b. and when feedback is not available
  
2. Participants use feedback to determine the quality of advice, and thus put more weight on accurate advisors in the feedback condition compared to the no-feedback condition

3. Participants denied feedback use agreement as a proxy for accuracy, and thus put more weight on agreeing advisors in the no-feedback condition compared to the feedback condition

All hypotheses are tested using the trials on which only one advisor provided advice.

### Benefits of advice

Participants should have *lower error on their last decisions than on their first decisions*.

```{r h1, results='asis'}

tmp <- aggregate(cbind(responseError, responseErrorFinal) ~ pid, 
                 block2, 
                 mean, na.rm = T)

r <- md.ttest(tmp$responseErrorFinal, tmp$responseError, c("*M*|last", "*M*|first"))
cat(r)

```

Supporting this primary result, participants should also score *more points on their last answer than on their first answer*, although given that they are allowed to change their confidence, they may sometimes miss the correct answer because they choose a thinner marker.

```{r h1score, results='asis'}

tmp <- aggregate(cbind(responseScore, responseScoreFinal) ~ pid, 
                 block2, 
                 mean, na.rm = T)

r <- md.ttest(tmp$responseScoreFinal, tmp$responseScore, c("*M*|last", "*M*|first"))
cat(r)

```

The above results could be because participants increase their confidence (i.e. not moving the centre of their marker and therefore not changing the error estimate, but changing the points scored). This can be assessed by looking for differences in correctness.

```{r h1correctness, results = 'asis'}

tmp <- aggregate(cbind(responseCorrect, responseCorrectFinal) ~ pid, 
                 block2, 
                 mean, na.rm = T)

r <- md.ttest(tmp$responseCorrectFinal, 
              tmp$responseCorrect, 
              c("*M*|last", "*M*|first"))

cat(r)

```

#### with feedback

The above effect should hold for participants in the feedback condition.

```{r h1a, results='asis'}

tmp <- aggregate(cbind(responseError, responseErrorFinal) ~ pid, 
                 AdvisedTrial[AdvisedTrial$hasFeedback, ], 
                 mean, na.rm = T)

r <- md.ttest(tmp$responseErrorFinal, tmp$responseError, c("*M*|last", "*M*|first"))
cat(r)

```

And for score as the outcome:

```{r h1aScore, results='asis'}

tmp <- aggregate(cbind(responseScore, responseScoreFinal) ~ pid, 
                 AdvisedTrial[AdvisedTrial$hasFeedback, ], 
                 mean, na.rm = T)

r <- md.ttest(tmp$responseScoreFinal, tmp$responseScore, c("*M*|last", "*M*|first"))
cat(r)

```

#### without feedback

The effect should also hold for participants in the no-feedback condition.

```{r h1b, results='asis'}

tmp <- aggregate(cbind(responseError, responseErrorFinal) ~ pid, 
                 AdvisedTrial[!AdvisedTrial$hasFeedback, ], 
                 mean, na.rm = T)

r <- md.ttest(tmp$responseErrorFinal, tmp$responseError, c("*M*|last", "*M*|first"))
cat(r)

```

And for score as the outcome:

```{r h1bScore, results='asis'}

tmp <- aggregate(cbind(responseScore, responseScoreFinal) ~ pid, 
                 AdvisedTrial[!AdvisedTrial$hasFeedback, ], 
                 mean, na.rm = T)

r <- md.ttest(tmp$responseScoreFinal, tmp$responseScore, c("*M*|last", "*M*|first"))
cat(r)

```

### Weighting of accuracy by feedback

Participants in the feedback condition should have *higher weight on advice for the accurate advisor* than participants in the no-feedback condition. 

```{r h2, results='asis'}

tmp <- aggregate(Accurate.woa ~ pid + hasFeedback, 
                 AdvisedTrial, mean, na.rm = T)

r <- md.ttest(tmp$Accurate.woa[tmp$hasFeedback], 
                tmp$Accurate.woa[!tmp$hasFeedback],
                labels = c("*M*|feedback", "*M*|feedback"))
cat(r)

```

### Weighting of agreement by feedback

The opposite pattern is expected for the agreeing advisor: feedback should result in *lower weight on advice*. 

```{r h3, results='asis'}

tmp <- aggregate(Agreeing.woa ~ pid + hasFeedback, 
                 AdvisedTrial, mean, na.rm = T)

r <- md.ttest(tmp$Agreeing.woa[tmp$hasFeedback], 
                tmp$Agreeing.woa[!tmp$hasFeedback],
                labels = c("*M*|feedback", "*M*|feedback"))
cat(r)

```

### General preference

We can also ask which advisor was preferred (if any) by participants in each condition:

```{r advisorPreference, results='asis'}

tmp <- aggregate(onlyAdvisor.woa ~ 
                   pid + hasFeedback + onlyAdvisor.idDescription, 
                 block2, mean)

cat("No feedback: ")
cat(md.ttest(tmp$onlyAdvisor.woa[tmp$hasFeedback & 
                                   tmp$onlyAdvisor.idDescription == "Agreeing"],
             tmp$onlyAdvisor.woa[tmp$hasFeedback & 
                                   tmp$onlyAdvisor.idDescription != "Agreeing"],
             labels = c("*M*|agr", "*M*|acc")))

cat("\nFeedback: ")
cat(md.ttest(tmp$onlyAdvisor.woa[!tmp$hasFeedback & 
                                   tmp$onlyAdvisor.idDescription == "Agreeing"],
             tmp$onlyAdvisor.woa[!tmp$hasFeedback & 
                                   tmp$onlyAdvisor.idDescription != "Agreeing"],
             labels = c("*M*|agr", "*M*|acc")))

```

### Bayesian ANOVA approach

The questions above can perhaps be more suitably answered using a heirachical Bayesian ANOVA analogue:

```{r bayesAnova}

# calculate woa for the advisor

tmp <- aggregate(onlyAdvisor.woa ~ 
                   pid + hasFeedback + onlyAdvisor.idDescription,
                 block2, mean, na.rm = T)
tmp$hasFeedback <- factor(tmp$hasFeedback)

r <- BANOVA.Normal(l1_formula = onlyAdvisor.woa ~ onlyAdvisor.idDescription,
                   l2_formula = ~ hasFeedback, 
                   data = tmp,
                   id = tmp$pid)

r

```

## Update modelling

We can model some of the changes investigated above as a function of the properties of each of the advisors on the trial. These models can be thought of as constituting more specifc hypotheses about the mechanics underlying the updating of advice quality on the basis of experience.

As a sanity check, we can run the established models of advisor updating as a consequence of feedback in the feedback condition. In this model, advisors' advice is adjusted as a consequence of the feedback received by an amount equal to a free learning rate parameter:
$$\omega_a^{t+1} = \omega_a^t + \lambda f(e_{a}^t, v^t)$$
where $\omega_a^t$ is the credibility of advisor $a$ on trial $t$, $\lambda$ is a learning rate parameter, $f(x,y)$ is a fitness function for an estimate $x$ and target value $y$, $e_a^t$ is the estimate provided by advisor $a$ on trial $t$, and $v^t$ the target value on trial $t$.

In the simplest case, $f(x, y)$ takes values of -1 or +1 for incorrect and correct estimates, respectively, while in other cases it may be continuous, e.g. the reciprocal of the error.

## Credits 

### Acknowledgements

Thanks as always to Nick Yeung and the other folks at the [ACC Lab](https://www.psy.ox.ac.uk/research/attention-cognitive-control-lab).

### R Packages

```{r results = 'asis'}
# list packages
packageNames <- (.packages())
# don't include very core package
packageNames <- packageNames[!(packageNames %in% 
                                 rownames(installed.packages(
                                   priority = "base")))]
# but do include the base package
packageNames <- c("base", packageNames)
out <- NULL
for (p in packageNames) {
  out <- rbind(out, data.frame('Package' = p, 
                               'Citations' = paste(format(citation(p), 
                                                          style = 'textVersion'), 
                                                   collapse = '<br/><br/>')))
}

kable(out)
```

### Funding

Matt Jaquiery is funded by a studentship from the [Medical Research Council](https://mrc.ukri.org/) (reference 1943590) and the University of Oxford [Department of Experimental Psychology](https://www.psy.ox.ac.uk/) (reference 17/18_MSD_661552).

### Technical details  

```{r results = 'hold'}
cat(paste('Time stamp:', Sys.time(), '\n\n'))
cat('Runtime \n')
proc.time()
cat('\n')
sessionInfo()
```