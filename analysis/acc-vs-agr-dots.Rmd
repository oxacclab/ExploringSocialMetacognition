---
title: "Accurate vs Agreeing advice (Dots task)"
author: Matt Jaquiery (matt.jaquiery@psy.ox.ac.uk)
output: 
  html_document:
    toc: false
    toc_depth: 3
    includes:
      after_body: src/toc_menu.html
editor_options:
  chunk_output_type: inline
---
June 2020  
[Script run `r Sys.time()`]

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F)
startTime <- Sys.time()
source('src/ESM_core.R')

library(ggridges)
library(dplyr)
library(broom)

theme_set(theme_light() +
            theme(
              text = element_text(size = 16),
              legend.position = 'top',
              panel.grid = element_blank()
            ))
```

# Descriptives 

## Introduction 

Investigation using the Dates task indicated that people showed a strong preference for picking accurate over agreeing advisors when feedback was provided. When feedback was withheld, no systematic preference was seen. Here we investigate whether the same pattern of results is observed when the Dots (perceptual decision-making) task is used instead. 

### Preregistration 

This study was preregistered on [the OSF](https://osf.io/5z2fp).

## Load data  

```{r}
folderName <- "G:\\Documents\\University\\Google Drive\\Temp\\data\\processed"
# folderName <- "G:/Documents/University/Google Drive/Project Documents/AdvisorChoice/results/Accuracy/2018-09-17 120 practice trials/processed"
results <- loadFilesFromFolder(folderName)
results <- removeParticipantIds(results)

# folderName <- 'ESM_sim.R'
# source('src/ESM_sim.R')
# results <- simulateAdvisorChoice(5, advisorClasses = c("Advisor"))

# unpack results
for (i in 1:length(results))
  assign(names(results)[i], results[i][[1]])

cat(paste('Loaded data from', folderName))
```

```{r}
trials <- cbind(trials, trialUtilityVariables(results))
all.trials <- trials
trials <- trials[trials$practice == F, ]
cat('Generated utility variables')
```

## Describe data 

### Metadata

Responses (`r paste('*N* =', length(unique(participants$pid)))`) were collected between `r as.POSIXct(min(unlist(participants$timeStart))/1000, tz = '', origin = '1970-01-01')` and `r as.POSIXct(max(unlist(participants$timeEnd))/1000, tz = '', origin = '1970-01-01')`.

Demographic data are not collected and therefore not analysed. Participants must be over 18 years old to use the Prolific recruitment platform.

### Task performance

#### Type 1 performance

Participants initial performance was held at 71% by design. Participants' mean percentage correct on initial decisions was `r md.mean(aggregate(initialCorrect ~ pid, trials, mean)$initialCorrect, isProportion = T)`. We would expect final decisions to be more accurate due to the presence of advice: 

```{r results = 'asis'} 
tmp <- aggregate(cbind(initialCorrect, finalCorrect) ~ pid, trials, mean) 
cat(md.ttest(tmp$initialCorrect, tmp$finalCorrect, 
             labels = c('*M*|initial', '*M*|final'), 
             isProportion = T, paired = T))

tmp %>% gather("decision", "pCorrect", -pid) %>%
  mutate(decision = str_match(decision, "(\\w+)Correct")[, 2]) %>%
  ggplot(aes(x = decision, y = pCorrect)) +
  geom_hline(yintercept = .71, linetype = 'dashed') +
  geom_violin(fill = 'grey85', colour = NA) +
  geom_boxplot(width = .2, outlier.colour = NA, size = 1.15) +
  geom_line(aes(group = pid), alpha = .25) +
  scale_y_continuous(limits = c(0, 1))

# caption = "Correctness by decision. Task difficulty was adaptively altered to target an initial decision accuracy of .71 (dashed line). Each faint line shows a participant's mean, while boxplots and violins show the distribution of those means."
```

#### Type 2 performance

Type 2 (metacognitive) performance is characterised using Type 2 ROC.

```{r results = 'asis'}
df.type2 <- NULL
for(p in unique(trials$pid)) {
  for(d in c('initial', 'final')) {
    tmp <- trials[trials$pid == p, c(paste0(d, 'Correct'), paste0(d, 'Confidence'))]
    # remove NA values which appear in final judgements which are never made
    tmp <- tmp[!is.na(tmp[ ,1]), ]
    roc <- type2ROC(tmp[ ,1], tmp[ ,2], bins = 7)
    df.type2 <- rbind(df.type2, data.frame(pid = factor(p), decision = d, conf = roc$x, pCorrect = roc$y))
  }
}
tmp <- seq(0, 1, length.out = length(unique(df.type2$conf))+1)
tmp <- sapply(1:(length(tmp)-1), function(i) mean(c(tmp[i], tmp[i+1])))
df.type2$confProp <- sapply(df.type2$conf, function(x) tmp[which(levels(df.type2$conf) == x)])

tmp <- aggregate(. ~ pid + decision, df.type2, mean)
tmp <- tmp[order(tmp$pid),]
# print neatly with rounding
kable(prop2str(aggregate(pCorrect ~ decision, tmp, mean)))
```

Participants' ROC curves:

```{r}
ggplot(df.type2, aes(x = confProp, y = pCorrect, colour = pid)) +
  geom_abline(slope = 1, intercept = c(0,0), linetype = 'dashed', colour = 'black') +
  geom_point() +
  geom_line(aes(group = pid)) +
  facet_wrap(~decision, labeller = label_both) +
  scale_x_continuous(limits = c(0,1)) +
  scale_y_continuous(limits = c(0,1)) +
  coord_fixed() +
  style.long +
  theme(panel.spacing.x = unit(1, 'lines'))
```

### Exclusions

```{r}
participants$excluded <- sapply(participants$pid, function(pid){
  ts <- all.trials[all.trials$pid == pid,]
  # overall accuracy of initial decisions
  m <- mean(ts$initialCorrect, na.rm = T)
  if(m < .6 | m > .85) return('Accuracy')
  # varied use of confidence scale
  cCs <- aggregate(pid ~ confidenceCategory, data = ts, FUN = length)
  # All confidence categories must be used
  if(nrow(cCs) < 3) return ('Confident')
  # All confidence categories must have at least 5% of the number of trials
  if(any(cCs$pid < length(ts)*.05)) return('<5%')
  return(F)
  })
```

We exclude participants for:

* Proportion of correct initial judgements must be (.60 < cor1/n < .90) (*N* = `r sum(participants$excluded == 'Accuracy')`)

* Having fewer than 3 confidence categories (*N* = `r sum(participants$excluded == 'Confident')`)

* Having fewer than 5% of trials in each confidence category (*N* = `r sum(participants$excluded == '<5%')`)

*NB: **practice trials are included** in this since they are used in part for determining confidence calibration*

The number of participants analysed after exclusions (total *N* = `r sum(participants$excluded != F)`) have taken place is `r sum(participants$excluded == F)`.

```{r}
# Perform exclusions
participants <- participants[participants$excluded==F, ]
# Remove excluded participants' data from other data frames
all.trials <- all.trials[all.trials$pid %in% participants$pid, ]
trials <- trials[trials$pid %in% participants$pid, ]
advisors <- advisors[advisors$pid %in% participants$pid, ]
questionnaires <- questionnaires[questionnaires$pid %in% participants$pid, ]
genTrustQ <- genTrustQ[genTrustQ$pid %in% participants$pid, ]
```

### Advisor performance

#### Manipulation checks

The advisors need to differ appropriately. How they differ depends on the advisors being compared, but in each case we need to check the participants' experience of the advisors matched the specifications for the advisors.

```{r results = 'asis'}
trials <- trials %>% 
  left_join(participants %>% select(pid, feedbackCondition), by = 'pid') %>%
  mutate(
    feedbackCondition = if_else(unlist(feedbackCondition), 'feedback', 'no-feedback'),
    advisorCorrect = adviceSide == correctAnswer,
    adviceTypeName = getAdviceTypeName(adviceType)
  )

trials %>% 
  group_by(adviceTypeName, pid) %>%
  summarise(agreement = mean(advisorAgrees),
            accuracy = mean(advisorCorrect)) %>%
  nest(d = -adviceTypeName) %>%
  mutate(
    agr = map(d, ~ mean_cl_normal(.$agreement)),
    acc = map(d, ~ mean_cl_normal(.$accuracy))
  ) %>%
  unnest(cols = c(agr, acc), names_sep = ".") %>%
  select(-d) %>% 
  pivot_longer(-adviceTypeName, 
               names_to = c("property", ".value"), 
               names_pattern = '(\\w+)\\.(\\w+)') %>%
  prop2str() %>% 
  kable()
  
```

#### Initial decision comparability

The participants' initial decisions should be equivalent (BF < .333) between advisors. Comparing initial decision by advice profile we can see whether this was the case:

```{r results = 'asis'} 
tmp <- aggregate(initialCorrect ~ pid + adviceType, trials, mean) 
# sort through using reference advisors appearing in the trial lists
for(a in adviceTypes[unlist(adviceTypes) %% 2 == 1 & unlist(adviceTypes) %in% trials$adviceType]) {
  cat(md.ttestBF(tmp$initialCorrect[tmp$adviceType == a], 
                 tmp$initialCorrect[tmp$adviceType == a+1], 
                 labels = paste0('*M*|', getAdviceTypeName(c(a, a+1))),
                 isProportion = T, paired = T))
  cat('\n\n')
}
```

# Pick rate differences

The primary outcome is the relative frequencies of advisor picking on trials where there is a choice of advisor. *Choice* trials show participants two advisors and wait until the participant selects one.

## Choice trials

Analysis for choice trials is conducted by comparing the proportion of choices for a reference advisor to the null hypothesis of .50 using a one-sample t-test. 

```{r results = 'asis'}
df.choice <- trials %>% 
  dplyr::filter(hasChoice) %>%
  group_by(pid) %>%
  dplyr::summarize(pChooseAcc = mean(adviceTypeName == "avaAcc"),
                   feedbackCondition = unique(feedbackCondition))

print('P(choose accurate)')
df.choice %>%
  nest(d = -feedbackCondition) %>%
  mutate(m = map(d, ~ mean_cl_normal(.$pChooseAcc))) %>%
  unnest(cols = m) %>%
  select(-d) %>%
  prop2str() %>%
  kable()

cat(md.ttest(df.choice$pChooseAcc[df.choice$feedbackCondition == 'feedback'], 
             y = .5, labels = c('*M*|feedback')))
cat('\n\n')
cat(md.ttest(df.choice$pChooseAcc[df.choice$feedbackCondition == 'no-feedback'], 
             y = .5, labels = c('*M*|no-feedback')))
cat('\n\n')

cat(md.ttest(df.choice$pChooseAcc[df.choice$feedbackCondition == 'feedback'], 
             df.choice$pChooseAcc[df.choice$feedbackCondition == 'no-feedback'],
             labels = c('*M*|feedback', '*M*|no-feedback')))
cat('\n\n')

```

```{r}
# forest plot for choice trials
ggplot(df.choice, aes(x = pChooseAcc, y = feedbackCondition)) +
  geom_vline(xintercept = .5, linetype = 'dashed') +
  geom_density_ridges(aes(fill = feedbackCondition), colour = NA, alpha = .75) +
  geom_point(position = position_jitter(0, .05), alpha = .5,
             aes(y = as.numeric(factor(feedbackCondition)) - .07,
                 colour = feedbackCondition))
```

## Summary {.summary}

Participants do not seem to have learned about the differences in the advisors during the training phase, even with feedback. Despite efforts to make the advisors more salient by introducing them one at a time and by adding identicons, no clear preference emerged for either advisor, even in the Feedback condition.

# Influence differences  

Influence, defined as the extent to which a participant adjusts their answer in the direction of an advisor's advice, can be compared between advisors. Note that the experimental design is not optimised for measuring this outcome because influence may vary systematically with choices, and the trials without choices constitute a learning phase wherein preferences cannot be expected to have crystalised. 

Influence scores are compared for trials on which a choice is not presented. For forced trials analysis is a simple ANOVA of agreement \* advice profile:

```{r}

df.forced <- trials %>%
  filter(!hasChoice) %>%
  mutate(pid = factor(pid),
         adviceTypeName = factor(adviceTypeName),
         feedbackCondition = factor(feedbackCondition))

dw <- 1
df.forced %>%
  group_by(pid, adviceTypeName, feedbackCondition) %>%
  summarise(influence = mean(advisorInfluence)) %>%
  ggplot(aes(x = adviceTypeName, y = influence, colour = adviceTypeName)) +
  geom_hline(yintercept = 0, linetype = 'dashed') +
  geom_violin(fill = NA, size = 1.15,
              position = position_dodge(dw)) +
  geom_boxplot(width = .2, size = 1.15, outlier.shape = NA,
               position = position_dodge(dw)) +
  geom_line(alpha = .25, aes(group = paste0(pid), colour = NA)) +
  facet_wrap(~feedbackCondition)

a <- ez::ezANOVA(
  data = df.forced,
  dv = advisorInfluence,
  wid = pid,
  within = adviceTypeName,
  between = feedbackCondition
)

a

a.bf <- anovaBF(
  advisorInfluence ~ adviceTypeName + feedbackCondition + pid,
  df.forced,
  whichRandom = c('pid'), 
  progress = F
)

```

## Summary {.summary}

There was a systematic effect of advisor on influence, with the accurate advisor being slightly more influential than the agreeing advisor in both conditions. There was possibly an interaction between advisor and Feedback condition, with the difference between the advisors being more pronounced in the No feedback condition !TODO[check this - it's not obvious from the graph], although this was not significant.

# Pick rate x Influence

If picking and influence measure a single latent 'trust' estimation for an advisor, the ratings should be correlated. The analysis is by advisor ID rather than advice profile, because individual advisors should have pick rates and influences which are more tightly correlated than advice profiles which average over multiple advisors. 

Forced trials are used for the influence calculations, and choice trials for the pick rate calculations. 

```{r}

tmp <- trials %>% 
  group_by(pid, feedbackCondition) %>%
  dplyr::summarize(
    influenceAcc = mean(if_else(!hasChoice & adviceTypeName == "avaAcc", 
                                advisorInfluence, NA_real_), 
                        na.rm = T),
    influenceAgr = mean(if_else(!hasChoice & adviceTypeName == "avaAgr",
                                advisorInfluence, NA_real_),
                        na.rm = T),
    pPickAcc = mean(if_else(hasChoice, adviceTypeName == "avaAcc", NA), na.rm = T)
  ) %>%
  mutate(influenceDiff = mean(influenceAcc - influenceAgr))

ggplot(tmp, aes(x = pPickAcc, y = influenceDiff, 
                colour = feedbackCondition, fill = feedbackCondition)) +
  geom_hline(yintercept = 0, linetype = 'dashed') +
  geom_vline(xintercept = .5, linetype = 'dashed') +
  geom_smooth(method = 'lm', alpha = .15) +
  geom_point(alpha = .25)

tmp %>%
  nest(d = -feedbackCondition) %>%
  mutate(d = map(d, ~ lm(influenceDiff ~ pPickAcc, data = .)),
         t = map(d, tidy)) %>%
  unnest(cols = t)

```

# Exploration 

## Questionnaires

```{r}

questionnaires <- trials %>% 
  select(pid, advisorId, adviceTypeName) %>% 
  unique() %>% 
  left_join(questionnaires)

qq <- questionnaires %>% 
  select(pid, adviceTypeName, likeability, benevolence, ability) %>%
  pivot_longer(c(-pid, -adviceTypeName), names_to = 'question', values_to = 'rating') 

qq %>%
  ggplot(aes(x = adviceTypeName, y = rating)) +
  geom_violin(fill = NA) +
  geom_boxplot(size = 1.25, width = .2, outlier.colour = NA) +
  geom_line(aes(group = pid), alpha = .25) +
  scale_y_continuous(limits = c(0, 100)) +
  facet_wrap(~ question)

```
### Correlation with pick rates

```{r}

tmp <- qq %>%
  pivot_wider(names_from = adviceTypeName, values_from = rating) %>%
  mutate(ratingDifference = avaAcc - avaAgr) %>%
  left_join(df.choice, by = 'pid')

r <- tmp %>% 
  nest(d = c(-feedbackCondition, -question)) %>%
  mutate(
    d = map(d, ~ cor.test(~ ratingDifference + pChooseAcc, data = .)),
    d = map(d, tidy)
  ) %>%
  unnest(cols = d)

ggplot(tmp, aes(x = pChooseAcc, y = ratingDifference)) +
  geom_hline(yintercept = 0, linetype = 'dashed') +
  geom_vline(xintercept = .5, linetype = 'dashed') +
  geom_smooth(method = 'lm', fill = 'blue', alpha = .2) +
  geom_point(alpha = 1/3) +
  scale_x_continuous(limits = c(0, 1)) +
  scale_y_continuous(limits = c(-100, 100)) +
  geom_label(data = r, x = .15, y = 100, hjust = .5, vjust = 1,
             aes(label = paste0(
               'r ', lteq(prop2str(estimate)), 
               ' [95%CI ', prop2str(conf.low),
               ', ', prop2str(conf.high), ']\n',
               'p ', lteq(prop2str(p.value))
             ))) +
  facet_grid(question ~ feedbackCondition)

tmp %>% 
  group_by(question, feedbackCondition) %>% 
  summarise(
    ratingDifference_mean = mean(ratingDifference),
    rd_sd = sd(ratingDifference),
    pChooseAcc_mean = mean(pChooseAcc),
    pCA_sd = sd(pChooseAcc)
    )

```

# Credits 

## Acknowledgements

Thanks as always to Nick Yeung and the other folks at the [ACC Lab](https://www.psy.ox.ac.uk/research/attention-cognitive-control-lab).

## R Packages

```{r results = 'asis'}
# list packages
packageNames <- (.packages())
# don't include very core package
packageNames <- packageNames[!(packageNames %in% rownames(installed.packages(priority = "base")))]
# but do include the base package
packageNames <- c("base", packageNames)
out <- NULL
for (p in packageNames) {
  out <- rbind(out, data.frame('Package' = p, 
                               'Citations' = paste(format(citation(p), style = 'textVersion'), 
                                                   collapse = '<br/><br/>')))
}

kable(out)
```

## Funding

Matt Jaquiery is funded by a studentship from the [Medical Research Council](https://mrc.ukri.org/) (reference 1943590) and the University of Oxford [Department of Experimental Psychology](https://www.psy.ox.ac.uk/) (reference 17/18_MSD_661552).

## Technical details  

```{r results = 'hold'}
cat(paste('Time stamp:', Sys.time(), '\n\n'))
cat('Runtime \n')
proc.time()
cat('\n')
sessionInfo()
```